<!DOCTYPE html> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <style>main.svelte-1pnr8ds{position:relative;box-sizing:border-box;min-height:100vh;width:100%;padding-top:3.5rem;padding-bottom:2.75rem}main.homepage.svelte-1pnr8ds{padding-right:2rem;padding-left:2rem}.bg.contentPage.svelte-1pnr8ds{padding-left:1rem;padding-right:1rem}@media screen and (min-width:775px){.bg.contentPage.svelte-1pnr8ds{padding-left:calc(75px + 1rem);padding-right:4rem}}.bg.svelte-1pnr8ds{background-size:auto,0;background-repeat:auto,no-repeat}@media screen and (min-width:775px){.bg.svelte-1pnr8ds{background-position:right,left;background-attachment:fixed,fixed;background-repeat:auto,repeat-y;background-size:cover,120px}}a{font-weight:700}nav.svelte-2cyc0n{z-index:20;position:fixed;top:0;left:0;right:0;background-color:#fff;font-weight:300;padding:0 1em 1px;transition:box-shadow .2s;margin:0;display:flex;flex-flow:row nowrap}nav.elevated.svelte-2cyc0n{box-shadow:0 0 .5rem rgba(0,0,0,.3),0 1px #eee}.brand-logo.svelte-2cyc0n{height:1em;transform:scale(1) translateX(0);transition:transform .6s ease-in-out}ul.svelte-2cyc0n{margin:0;padding:0;flex-shrink:0}ul.svelte-2cyc0n:first-child{flex-grow:1}ul.svelte-2cyc0n::after{content:'';display:block;clear:both}li.svelte-2cyc0n{display:block;float:left}.selected.svelte-2cyc0n{position:relative;display:inline-block;font-weight:700}.selected.svelte-2cyc0n::after{position:absolute;content:'';width:calc(100% - 1em);height:2px;background-color:#0460a9;display:block;bottom:-1px}a.svelte-2cyc0n{text-decoration:none;padding:1em .5em;display:block}.page-content.svelte-ja5ske{padding-left:1rem;padding-right:1rem}@media screen and (min-width:775px){.page-content.svelte-ja5ske{padding-left:calc(75px + 1rem)}}.neg-margin.svelte-ja5ske>*{margin:1rem -2rem 0}@keyframes svelte-13ybuhq-fadein{from{transform:translate(1rem);opacity:0}to{transform:translate(0);opacity:1}}.wrapper.svelte-13ybuhq{display:grid;grid-template-rows:20vh 1fr 10vh;grid-template-columns:minmax(0,20rem) 1fr;margin-left:-2rem;margin-right:-2rem}@media screen and (max-width:774.5px){.wrapper.svelte-13ybuhq{grid-template-rows:2em 1fr 2em}}.callout.svelte-13ybuhq{background-color:#0460a9;color:#fff;z-index:10;padding:1rem;max-width:100%;grid-row:2/3;grid-column:1/2;animation:svelte-13ybuhq-fadein .4s}.bg.svelte-13ybuhq{z-index:1;grid-row:1/4;grid-column:1/3;background-size:cover;background-position:center}a.svelte-13ybuhq{font-weight:700;white-space:nowrap;text-decoration:none;text-decoration-skip:none}a.svelte-13ybuhq:hover{text-decoration:underline}a.svelte-13ybuhq::after{content:' →'}h2.svelte-13ybuhq{margin-top:0}.wrapper.svelte-57am63{display:grid;grid-template-columns:repeat(auto-fit,minmax(20ch,1fr));grid-column-gap:1rem}@media screen and (min-width:900px){.wrapper.svelte-57am63{grid-template-columns:repeat(auto-fit,minmax(30%,1fr))}}.project.svelte-99he1c{border-top:3px solid #000;padding:1rem 0;margin-bottom:2rem}.project-title.svelte-99he1c{margin-top:0}.project-title.svelte-99he1c a.svelte-99he1c{color:#000;text-decoration:none}.cta.svelte-99he1c{font-weight:700;white-space:nowrap;text-decoration:none;text-decoration-skip:none}.cta.svelte-99he1c:hover{text-decoration:underline}.cta.svelte-99he1c::after{content:' →'}.ico-wrap.svelte-99he1c{margin:0 1em;text-decoration:none}.icon-display.svelte-99he1c{padding-top:100%;background-position:center;background-size:contain;background-repeat:no-repeat;transform:scale(1);transition:transform .3s ease-in-out;box-shadow:inset 0 0 5px #fff}.ico-wrap.svelte-99he1c:hover .icon-display.svelte-99he1c{transform:scale(1.05)}.callout.svelte-bf2rc0{background-color:#0460a9;color:#fff;padding:2rem 1rem}@media screen and (min-width:775px){.callout.svelte-bf2rc0{padding-left:calc(75px + 3rem)}}.callout-content.svelte-bf2rc0{max-width:50rem}.btn.svelte-zemmdc,a[role=button]{border:1px solid #000;background-color:#fff;color:#000;transition:color .2s,background-color .2s;font-weight:700;display:inline-flex;align-items:center;justify-content:center;text-decoration:none;padding:.25em .75em;white-space:nowrap;cursor:pointer}.btn.svelte-zemmdc:hover,a[role=button]:hover{background-color:#000;color:#fff}.btn.primary.svelte-zemmdc{border-color:#fff;background-color:#0460a9;color:#fff}.btn.primary.svelte-zemmdc:hover{background-color:#fff;color:#0460a9}footer.svelte-1o8yfks{display:flex;flex-flow:row nowrap;font-size:.75rem;padding:.5rem .75rem;align-items:center;border-top:1px solid #eee;position:fixed;bottom:0;left:0;right:0;background-color:#fff}@media screen and (max-width:499px){footer.svelte-1o8yfks{text-align:center;flex-flow:column nowrap}}.footer-left.svelte-1o8yfks{flex-grow:1}.footer-right.svelte-1o8yfks{flex-grow:0}a.svelte-1o8yfks{color:rgba(0,0,0,.5);text-decoration:none;font-weight:400}a.svelte-1o8yfks:hover{text-decoration:underline}.social.svelte-1o8yfks{font-size:.825rem}.social.svelte-1o8yfks a.svelte-1o8yfks{border:1px solid #000;background-color:#fff;color:#000;transition:color .2s,background-color .2s;width:1.5rem;height:1.5rem;display:inline-flex;align-items:center;justify-content:center;text-decoration:none;margin:0 .125rem}.social.svelte-1o8yfks a.svelte-1o8yfks:hover{background-color:#000;color:#fff}.sep.svelte-1o8yfks::before{content:'|'}</style> <noscript id=sapper-head-start></noscript><link href=/favicon.ico rel=icon type=image/ico><link href=/manifest.json rel=manifest><title>NIBR Open Source</title><meta content="NIBR Open Source" property=og:title><meta content="We believe in the power of open-sourced, global collaboration for the greater good." property=og:description><meta content=https://opensource.nibr.com/social-preview.png property=og:image><meta content=https://opensource.nibr.com/ property=og:url><meta content=summary_large_image name=twitter:card><meta content="NIBR Open Source" property=og:site_name><noscript id=sapper-head-end></noscript> <link href=/client/1ef9048e76cf74561a9e/main.js rel=preload as=script><link href=/client/1ef9048e76cf74561a9e/index.1.js rel=preload as=script></head> <body> <div id=sapper> <nav class="svelte-2cyc0n accent"> <ul class=svelte-2cyc0n> <li class=svelte-2cyc0n> <a class="svelte-2cyc0n selected" href="">NIBR Open Source</a> </li> </ul> <ul class=svelte-2cyc0n> <li class=svelte-2cyc0n> <a class=svelte-2cyc0n href=https://www.novartis.com rel="nofollow noreferrer" target=_blank> <img alt="Novartis logo" class="svelte-2cyc0n brand-logo" src=/novartis-logo.png> </a> </li> </ul> </nav> <div class="bg svelte-1pnr8ds" style="background-image:linear-gradient(to right,rgba(255,255,255,0),rgba(255,255,255,0) 75px,rgba(255,255,255,255) 75px,rgba(255,255,255,255)),url(/blue-carbon.png)"> <main class="svelte-1pnr8ds homepage"> <div class="svelte-13ybuhq wrapper"> <div class="svelte-13ybuhq callout"> <h2 class=svelte-13ybuhq>NIBR Open Source</h2> <p> The Novartis Institutes for BioMedical Research (NIBR) is pioneering new informatics tools for drug discovery. We believe in the power of open-sourced, global collaboration for the greater good. Join us to help patients worldwide. </p> <a class=svelte-13ybuhq href=https://novartis.com/ rel="nofollow noreferrer" target=_blank> Read about our work </a> </div> <div class="svelte-13ybuhq bg" style=background-image:url(/novartis-robot.jpg)></div> </div> <div class="svelte-ja5ske page-content"> <h1>Projects</h1> <div class="wrapper svelte-57am63"> <div class="svelte-99he1c project"> <h2 class="svelte-99he1c project-title"> <a class=svelte-99he1c href=/projects/jpkl title=JPkl>JPkl</a> </h2> <p> Fast, compressed JPEG storage for NumPy applications. </p> <a class="svelte-99he1c cta" href=/projects/jpkl title="View JPkl project page">Read more</a> </div><div class="svelte-99he1c project"> <h2 class="svelte-99he1c project-title"> <a class=svelte-99he1c href=/projects/peax title=Peax>Peax</a> </h2> <p> Interactive concept learning and visual exploration of epigenomic patterns. </p> <a class="svelte-99he1c cta" href=/projects/peax title="View Peax project page">Read more</a> </div><div class="svelte-99he1c project"> <h2 class="svelte-99he1c project-title"> <a class=svelte-99he1c href=/projects/jenkins-lsci title=Jenkins-LSCI>Jenkins-LSCI</a> </h2> <p> Life Science Continuous Integration -- Workflows and data pipelines for life science research. </p> <a class="svelte-99he1c cta" href=/projects/jenkins-lsci title="View Jenkins-LSCI project page">Read more</a> </div><div class="svelte-99he1c project"> <h2 class="svelte-99he1c project-title"> <a class=svelte-99he1c href=/projects/habitat title=Habitat>Habitat</a> </h2> <p> Habitat - Where files live. A simple and yet powerful self-contained object storage management system. </p> <a class="svelte-99he1c cta" href=/projects/habitat title="View Habitat project page">Read more</a> </div><div class="svelte-99he1c project"> <h2 class="svelte-99he1c project-title"> <a class=svelte-99he1c href=/projects/yada title=YADA>YADA</a> </h2> <p> A universal remote control for data. </p> <a class="svelte-99he1c cta" href=/projects/yada title="View YADA project page">Read more</a> </div><div class="svelte-99he1c project"> <h2 class="svelte-99he1c project-title"> <a class=svelte-99he1c href=/projects/ontobrowser title=OntoBrowser>OntoBrowser</a> </h2> <p> A web-based application for managing ontologies and assigning synonyms to ontology terms. </p> <a class="svelte-99he1c cta" href=/projects/ontobrowser title="View OntoBrowser project page">Read more</a> </div><div class="svelte-99he1c project"> <h2 class="svelte-99he1c project-title"> <a class=svelte-99he1c href=/projects/railroadtracks title=Railroadtracks>Railroadtracks</a> </h2> <p> A toolkit for DNA and RNA-Seq processing steps. </p> <a class="svelte-99he1c cta" href=/projects/railroadtracks title="View Railroadtracks project page">Read more</a> </div><div class="svelte-99he1c project"> <h2 class="svelte-99he1c project-title"> <a class=svelte-99he1c href=/projects/yap title="Yet Another Pipeline">Yet Another Pipeline</a> </h2> <p> YAP allows researchers to quickly build high throughput big data pipelines without extensive knowledge of parallel programming. </p> <a class="svelte-99he1c cta" href=/projects/yap title="View Yet Another Pipeline project page">Read more</a> </div><div class="svelte-99he1c project"> <h2 class="svelte-99he1c project-title"> <a class=svelte-99he1c href=/projects/gridvar title=GridVar>GridVar</a> </h2> <p> GridVar is a jQuery plugin that visualizes multi-dimensional datasets as layers organized in a row-column format. </p> <a class="svelte-99he1c cta" href=/projects/gridvar title="View GridVar project page">Read more</a> </div> </div> </div> <div class="svelte-ja5ske neg-margin"> <div class="callout svelte-bf2rc0"> <div class="svelte-bf2rc0 callout-content"> <h2>Interested in NIBR Engineering?</h2> <p>At NIBR, you'll be at the forefront of technology — helping to shape it, develop it, and make it impactful. Partnering with scientists, our engineers create cutting-edge, state-of-the-art solutions that accelerate drug discovery and ultimately improve patients’ lives.</p> <a class="primary svelte-zemmdc" class="primary btn" href=https://www.novartis.com/careers/careers-research rel="nofollow noopener" target=_blank variant=primary> Learn more </a> </div> </div> </div> </main> </div> <footer class=svelte-1o8yfks> <div class="svelte-1o8yfks footer-left"> © 2019 <a class=svelte-1o8yfks href=https://www.novartis.com rel="nofollow noreferrer" target=_blank>Novartis AG</a> <span class="svelte-1o8yfks sep"></span> <a class=svelte-1o8yfks href=https://www.novartis.com/terms-use rel="nofollow noreferrer" target=_blank> Terms of use </a> <span class="svelte-1o8yfks sep"></span> <a class=svelte-1o8yfks href=https://www.novartis.com/privacy-policy rel="nofollow noreferrer" target=_blank> Privacy policy </a> <span class="svelte-1o8yfks sep"></span> <a class=svelte-1o8yfks href=mailto:open.source@novartis.com> open.source@novartis.com </a> </div> <div class="svelte-1o8yfks footer-right social"> <a class=svelte-1o8yfks href=https://www.github.com/Novartis title=GitHub rel="nofollow noreferrer" target=_blank> <i class="fab fa-github"></i> </a> <a class=svelte-1o8yfks href=https://www.facebook.com/novartis title=Facebook rel="nofollow noreferrer" target=_blank> <i class="fab fa-facebook-f"></i> </a> <a class=svelte-1o8yfks href=https://twitter.com/NovartisScience title=Twitter rel="nofollow noreferrer" target=_blank> <i class="fab fa-twitter"></i> </a> <a class=svelte-1o8yfks href=https://www.linkedin.com/company/novartis-institutes-for-biomedical-research title=LinkedIn rel="nofollow noreferrer" target=_blank> <i class="fab fa-linkedin-in"></i> </a> </div> </footer></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,(function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q){return {projects:[{slug:b,metadata:{name:i,abbreviation:b,projectName:b,launched:"2018-09-18T00:00:00.000Z",summary:"Fast, compressed JPEG storage for NumPy applications.",description:"\u003Cp\u003ESerialize high memory \u002F disk consumption NumPy multidimensional arrays of images into JPEG-Pickle files for low storage cost and decently fast, random access with NumPy-like indexing.\u003C\u002Fp\u003E\n",dateString:"Tue Sep 18 2018",title:i,styles:[],scripts:[]},html:"\u003Cp\u003ESerialize high memory \u002F disk consumption NumPy multidimensional arrays of images into JPEG-Pickle files for low storage cost and decently fast, random access with NumPy-like indexing.\u003C\u002Fp\u003E\n\u003Cp\u003ECode at a glance:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-python\"\u003Eimport numpy as np\nfrom jpkl import JPkl\n\n# Make some arbitrary data. Say we have 512x512 images with time, z depth, and multiple channels.\nimages = np.zeros((512, 512, 100, 3, 2))\n\n# Automagically encode all 512x512 images into JPEGs.\njpkl_images = JPkl(images)\n\n# Index \u002F slice encoded JPEGs as you would a NumPy array.\n# JPEG-encoded slices are decoded and all concatenated into an array on-the-fly.\n\ndecoded_images = pkl_images[128:256:2, :, 4, ...]  # decoded_images is a NumPy array\n\ndecoded_images.shape  # (64, 512, 14, 15) [array is .squeeze()&#39;d as in NumPy indexing]\n\n# JPkl objects support 3 simple NumPy array properties.\n# This means you can sometimes get away with passing a JPkl instead of an array to functions.\n\njpkl_images.shape    # (512, 512, 100, 3, 2)\njpkl_images.ndim     # 5\njpkl_images.size     # 5505024000 [product of pkl_images.shape elements]\n\n# Pickle your JPkl and save to disk:\njpkl_images.save(&#39;pickled_images.jpkl&#39;)\n\n# Load a saved JPkl file:\nsaved_jpkl_images = JPkl.load(&#39;pickled_images.jpkl&#39;)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003ENote that JPkl data is immutable from the high-level interface.\u003C\u002Fp\u003E\n\u003Cp\u003ESee the Jupyter Notebook demo for a more in-depth walkthrough (as well as a cool HoloViews visualization demo of scrubbing through these multidimensional arrays!).\u003C\u002Fp\u003E\n\u003Ch2 id=\"motivation\"\u003EMotivation\u003C\u002Fh2\u003E\n\u003Cp\u003EData from scientific imaging applications (such as microscopy) is often stored as raw, uncompressed data. This is important in cases where accurate quantification on pixel values is necessary. Due to this need, however, these image files (and their corresponding memory footprints) are often large, sometimes approaching 10s (or more) of gigabytes in extreme examples. While memory-mapping these files to access them without loading everything into memory is often a viable option, large I\u002FO speed demands due to the sizes of individual frames makes latency during visualization a possible issue. Additionally, this mapping still does not solve the issue of huge disk space consumption.\u003C\u002Fp\u003E\n\u003Cp\u003EAs such, to facilitate speedy visualization of the raw \u002F completely processed images as well as intermediate stage images (which you may not need to keep the uncompressed pixel values for anyway), it makes sense to lossily compress the images for non-computional, solely visualization purposes. JPkl handles this need by compressing all the individual image slices into JPEGs in memory which can then be serialized out to disk. Depending on image content and JPEG quality level, you can see massive (5-10x +) decreases in file size. In addition, JPkl supports on-the-fly decoding of abitrary slices of the pseudo-NumPy array into true NumPy arrays, meaning when you unpickle the JPkl file, your memory consumption remains as low as the file usage on disk unless you want to convert the whole JPkl into an array at once.\u003C\u002Fp\u003E\n\u003Cp\u003ENote that JPkl is not limited to using Pickle serialization or JPEG encoding. Extending the \u003Ccode\u003EJPkl\u003C\u002Fcode\u003E class and changing the implementations of \u003Ccode\u003Ejpkl_obj.save()\u003C\u002Fcode\u003E \u002F \u003Ccode\u003EJPkl.load()\u003C\u002Fcode\u003E or \u003Ccode\u003Ejpkl_obj.encode_slice()\u003C\u002Fcode\u003E \u002F \u003Ccode\u003Ejpkl_obj.decode_slice()\u003C\u002Fcode\u003E will allow you to use any serialization or encoding you wish as appropriate for your application, respectively.\u003C\u002Fp\u003E\n\u003Cp\u003EJPkl is most useful in the case where you have:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003ELarge raw image datasets which you would like to visualize (such as in a Jupyter Notebook)\u003C\u002Fli\u003E\n\u003Cli\u003EDatasets that are highly multi-dimensional (x,y spatial + arbitrary number of time \u002F channel axes)\u003C\u002Fli\u003E\n\u003Cli\u003EThe need to visualize intermediate steps of image processing (which adds a multiplier on disk space consumption if\nintermediate steps are stored as raw images)\u003C\u002Fli\u003E\n\u003Cli\u003EThe desire to keep all images from a dataset \u002F NumPy array in a single file, rather than all messily dumped as separate .jpgs into the filesystem\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EJPkl uses the Pillow (PIL fork) library for encoding \u002F decoding JPEG images.\u003C\u002Fp\u003E\n\u003Ch2 id=\"attribution\"\u003EAttribution\u003C\u002Fh2\u003E\n\u003Cp\u003EThis is part of the Open Source at Novartis Institutes for BioMedical Research (NIBR) initiative:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Fopensource.nibr.com\u002F\"\u003EOpen Source @ NIBR home\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002FNovartis\u002F\"\u003EGitHub organization\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EIt is licensed under Apache License, Version 2.0.\u003C\u002Fp\u003E\n\u003Cp\u003EMaintainer: @zbarry\u003C\u002Fp\u003E\n\u003Ch2 id=\"jpkl-class-file-format-specification-\"\u003EJPkl class \u002F file format specification:\u003C\u002Fh2\u003E\n\u003Cp\u003EJPkls on disk are simply a dictionary of:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003E&#39;header&#39;: Tuple of (&#39;JPkl&#39;, &#39;version&#39;), version in set{&#39;1&#39;}\n    Used for sanity checking if file on disk is JPkl file and is a loadable version.\n\n&#39;color&#39;: bool: if True, this came from a set of images with a color channel.\n    Therefore, the third axis [index 2] of the returned stack will be of length 3 for RGB.\n    Otherwise, there is no color axis.\n\n&#39;jpeg_quality&#39;: int: level of JPEG compression (0-100).\n\n&#39;images&#39;: Dictionary of byte streams from Pillow `Image` objects encoding images to `bytes` using `io.BytesIO` memory streams\n    Each key of the dictionary is a tuple of (channel 1, channel 2, ...) indices which correspond to a single image\n    slice. All images must be of the same width and height (since they are derived from \u002F decoded into NumPy arrays).\n    jpkl_obj.images[0, 5, 2], for example, returns a byte stream of the JPEG-encoded image slice which would have\n    been accessed in the original NumPy array as `image_array[:, :, 0, 5, 2]`. If the images were RGB, the color\n    axis is not a key in the dictionary.\n\n&#39;dim_names&#39;: List of strings of axis identities that come after height, width. Does not include color name. Not necessary for JPkl, but included for user&#39;s own documentation, if desired.\n\n&#39;dim_sizes&#39;: Tuple of the lengths of each channel axis. Does not include [height, width, RGB] axes.\n\n&#39;metadata&#39;: Dictionary of arbitrary data for the user&#39;s usage. Completely ignored by JPkl.\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Ch2 id=\"installation\"\u003EInstallation\u003C\u002Fh2\u003E\n\u003Cp\u003E\u003Cstrong\u003EDependencies (installation is through Anaconda system):\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Ch3 id=\"jpkl-\"\u003EJPkl:\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli\u003EPython &gt;= 3.6\u003C\u002Fli\u003E\n\u003Cli\u003ENumPy\u003C\u002Fli\u003E\n\u003Cli\u003EPillow (Python Imaging Library fork)\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 id=\"jupyter-notebook-demo-\"\u003EJupyter Notebook demo:\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli\u003EJupyter Notebook \u002F Lab\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003EHoloViews and associated libraries:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EParam\u003C\u002Fli\u003E\n\u003Cli\u003EParamBokeh\u003C\u002Fli\u003E\n\u003Cli\u003EBokeh\u003C\u002Fli\u003E\n\u003Cli\u003EImaGen\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 id=\"installation-procedure-\"\u003EInstallation procedure:\u003C\u002Fh3\u003E\n\u003Cp\u003EDefault installation including libraries enabling the notebook demo:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Egit clone &lt;&lt;repo&gt;&gt;\ncd jpkl\nsource activate YOURENVNAME\nconda env update -f environment.yml\npip install .\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003ESubstitute \u003Ccode\u003Epip install -e .\u003C\u002Fcode\u003E above if you wish to make edits to the source.\u003C\u002Fp\u003E\n\u003Cp\u003EWhile using a Conda environment is highly recommended in general, you can technically leave out the the \u003Ccode\u003Esource activate YOURENVNAME\u003C\u002Fcode\u003E line to install to the default package folder. If you&#39;ve not used Conda environments before, you can create one with \u003Ccode\u003Econda create -n YOURENVNAME\u003C\u002Fcode\u003E and then activate it with \u003Ccode\u003Esource activate YOURENVNAME\u003C\u002Fcode\u003E as in above. This will allow you to keep your installed packages separate between projects.\u003C\u002Fp\u003E\n\u003Cp\u003ELightweight install with solely JPkl functionality without visualization:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Egit clone &lt;&lt;repo&gt;&gt;\ncd jpkl\nsource activate YOURENVNAME\nconda env update -f environment-novis.yml\npip install .\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Ch2 id=\"contributing\"\u003EContributing\u003C\u002Fh2\u003E\n\u003Cp\u003EI am more than happy to field pull requests! If interested, please post an issue for what you&#39;re thinking about working on so I can make sure it aligns with the vision for the project. The current guiding principle here is to keep it as lightweight \u002F minimalistic as possible, though this is by no means set in stone.\u003C\u002Fp\u003E\n\u003Cp\u003EAreas of immediate interest for PRs:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EMultiprocessing for encoding \u002F decoding (chunks of \u002F whole) huge NumPy arrays quickly. Initial array encoding isn&#39;t incredibly fast, though random access for visualization is very usable. Multiprocessing might be a huge boon for the initial construction of the JPkls from arrays.\u003C\u002Fli\u003E\n\u003Cli\u003EClever ways of caching frequently-accessed images might be useful in some cases.\u003C\u002Fli\u003E\n\u003Cli\u003EOther encoding \u002F compression methods which may be superior to JPEG \u002F Pickle files.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n"},{slug:j,metadata:{name:c,abbreviation:c,projectName:j,launched:"2018-09-15T00:00:00.000Z",icon:"\u002Fprojects\u002Fpeax\u002Fpeax-icon.png",summary:"Interactive concept learning and visual exploration of epigenomic patterns.",description:"\u003Cp\u003EPeax is a tool for interactive concept learning and exploration of epigenomic patterns based on unsupervised machine learning with autoencoders.\u003C\u002Fp\u003E\n",dateString:"Sat Sep 15 2018",title:c,styles:[],scripts:[]},html:"\u003Cblockquote\u003E\n\u003Cp\u003EA pattern explorer for epigenomic data.\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003E\u003Cimg src=\"\u002Fprojects\u002Fpeax\u002Fteaser.png\" alt=\"Peax&#39;s UI\"\u003E\u003C\u002Fp\u003E\n\u003Cblockquote\u003E\n\u003Cp\u003EEpigenomic data expresses a rich body of diverse patterns that help to identify\nregulatory elements like promoter, enhancers, etc. But finding these patterns reliably\ngenome wide is challenging. Peax is a tool for interactive visual pattern search and\nexploration of epigenomic patterns based on unsupervised representation learning with\nconvolutional autoencoders. The visual search is driven by manually labeled genomic\nregions for actively learning a classifier to reflect your notion of interestingness.\nMore at \u003Ca href=\"http:\u002F\u002Fpeax.lekschas.de\"\u003Epeax.lekschas.de\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Ch2 id=\"installation\"\u003EInstallation\u003C\u002Fh2\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-bash\"\u003Egit clone https:\u002F\u002Fgithub.com\u002FNovartis\u002Fpeax peax &amp;&amp; cd peax\nmake install\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Cem\u003EDo not fear, \u003Ccode\u003Emake install\u003C\u002Fcode\u003E is just a convenience function for setting up conda and installing npm packages.\u003C\u002Fem\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ENotes:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EIf you&#39;re a macOS user you might need to \u003Ca href=\"https:\u002F\u002Fbrew.sh\"\u003Ebrew\u003C\u002Fa\u003E install \u003Ccode\u003Elibpng\u003C\u002Fcode\u003E and \u003Ccode\u003Eopenssl\u003C\u002Fcode\u003E for the \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fnvictus\u002Fpybbi\"\u003Epybbi\u003C\u002Fa\u003E package (see \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fnvictus\u002Fpybbi\u002Fissues\u002F2\"\u003Ehere\u003C\u002Fa\u003E) and \u003Ccode\u003Exz\u003C\u002Fcode\u003E for pysam (if you see an error related to \u003Ccode\u003Elzma.h\u003C\u002Fcode\u003E).\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 id=\"overview\"\u003EOverview\u003C\u002Fh2\u003E\n\u003Cp\u003EPeax consists of three main parts:\u003C\u002Fp\u003E\n\u003Col\u003E\n\u003Cli\u003EA server application for serving genomic and autoencoded data on the web. [\u003Ca href=\"https:\u002F\u002Fgithub.com\u002FNovartis\u002Fpeax\u002Ftree\u002Fdevelop\u002Fserver\"\u003E\u002Fserver\u003C\u002Fa\u003E].\u003C\u002Fli\u003E\n\u003Cli\u003EA user interface for exploring, visualizing, and interactively labeling genomic regions. [\u003Ca href=\"https:\u002F\u002Fgithub.com\u002FNovartis\u002Fpeax\u002Ftree\u002Fdevelop\u002Fui\"\u003E\u002Fui\u003C\u002Fa\u003E].\u003C\u002Fli\u003E\n\u003Cli\u003EA set of examples showing how to configure Peax and build your own. [\u003Ca href=\"https:\u002F\u002Fgithub.com\u002FNovartis\u002Fpeax\u002Ftree\u002Fdevelop\u002Fexamples\"\u003E\u002Fexamples\u003C\u002Fa\u003E]\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003Ch2 id=\"data\"\u003EData\u003C\u002Fh2\u003E\n\u003Cp\u003EWe provide 6 autoencoders trained on 3 kb, 12 kb, and 120 kb window sizes (with 25,\n100, and 1000 bp binning) on DNase-seq and histone mark ChIP-seq data.\u003C\u002Fp\u003E\n\u003Cp\u003EYou can find the autoencoder at \u003Ca href=\"https:\u002F\u002Fzenodo.org\u002Frecord\u002F2609763\"\u003Ezenodo.org\u002Frecord\u002F2609763\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Ch2 id=\"preprint\"\u003EPreprint\u003C\u002Fh2\u003E\n\u003Cp\u003ELekschas et al., \u003Ca href=\"https:\u002F\u002Fwww.biorxiv.org\u002Fcontent\u002F10.1101\u002F597518v1\"\u003EPeax: Interactive Visual Pattern Search in Sequential Data Using Unsupervised Deep Representation Learning\u003C\u002Fa\u003E,\n\u003Cem\u003EbioRxiv\u003C\u002Fem\u003E, 2019, doi: \u003Ca href=\"10.1101\u002F597518\"\u003E10.1101\u002F597518\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Ch2 id=\"quick-start\"\u003EQuick start\u003C\u002Fh2\u003E\n\u003Cp\u003EPeax comes with \u003Ca href=\"#autoencoders\"\u003E6 autoencoders\u003C\u002Fa\u003E for DNase-seq and histone mark\nChIP-seq data and several example configurations for which we provide\nconvenience scripts to get you started as quickly as possible.\u003C\u002Fp\u003E\n\u003Cp\u003EFor instance, run one of the following commands to start Peax with a DNase-seq\ntrack for 3 kb, 12 kb, and 120 kb genomic windows.\u003C\u002Fp\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003ECommand\u003C\u002Fth\u003E\n\u003Cth\u003EWindow Size\u003C\u002Fth\u003E\n\u003Cth\u003EStep Freq.\u003C\u002Fth\u003E\n\u003Cth\u003EChromosomes\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Emake example-3kb\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003E3 kb\u003C\u002Ftd\u003E\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\n\u003Ctd\u003E21\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Emake example-12kb\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003E12 kb\u003C\u002Ftd\u003E\n\u003Ctd\u003E3\u003C\u002Ftd\u003E\n\u003Ctd\u003E20-21\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\u003Ccode\u003Emake example-120kb\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003E120 kb\u003C\u002Ftd\u003E\n\u003Ctd\u003E6\u003C\u002Ftd\u003E\n\u003Ctd\u003E17-21\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Cp\u003E\u003Cstrong\u003ENote:\u003C\u002Fstrong\u003E The first time Peax is started it will precompute the datasets for\nexploration. This can take a few minutes depending on your hardware. Also, these demos\nwill only prepare the above mentioned chromosomes, so don&#39;t try to search for patterns\non another chromosome. It won&#39;t work! For your own data you can freely configure this\nof course.\u003C\u002Fp\u003E\n\u003Cp\u003EThe scripts will download test ENCODE tracks and use the matching\nconfiguration to start the server. More examples are described in \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FNovartis\u002Fpeax\u002Ftree\u002Fdevelop\u002Fexamples\"\u003E\u003Ccode\u003E\u002Fexamples\u003C\u002Fcode\u003E\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Ch2 id=\"get-started\"\u003EGet Started\u003C\u002Fh2\u003E\n\u003Cp\u003EIn the following we describe how you can configure Peax for your own data.\u003C\u002Fp\u003E\n\u003Ch4 id=\"configure-peax-with-your-data\"\u003EConfigure Peax with your data\u003C\u002Fh4\u003E\n\u003Cp\u003ENext, you need to configure Peax with your data to tell it which tracks you want to visualize in HiGlass and which of those tracks are encodable using an (auto)encoder.\u003C\u002Fp\u003E\n\u003Cp\u003EThe fastest way to get started is to copy the example config:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode\u003Ecp config.json.sample config.json\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EThe config file has 6 top level properties:\u003C\u002Fp\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EField\u003C\u002Fth\u003E\n\u003Cth\u003EDescription\u003C\u002Fth\u003E\n\u003Cth\u003EDtype\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003Eencoders\u003C\u002Ftd\u003E\n\u003Ctd\u003EList of encoders.\u003C\u002Ftd\u003E\n\u003Ctd\u003Elist\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Edatasets\u003C\u002Ftd\u003E\n\u003Ctd\u003EList of tracks.\u003C\u002Ftd\u003E\n\u003Ctd\u003Elist\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Ecoords\u003C\u002Ftd\u003E\n\u003Ctd\u003EGenome coordinates. Peax currently supports hg19, hg28, mm9, and mm10\u003C\u002Ftd\u003E\n\u003Ctd\u003Estr\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Echroms\u003C\u002Ftd\u003E\n\u003Ctd\u003EChromosomes to to be searched. If omitted all chromosomes will be prepared for searching.\u003C\u002Ftd\u003E\n\u003Ctd\u003Elist\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Estep_freq\u003C\u002Ftd\u003E\n\u003Ctd\u003EStep frequency of the sliding window approach. E.g., given an encoder with window size 12 kb, a step frequency of 6 means that every 2 kb a 12 kb window will be extracted from the bigWig.\u003C\u002Ftd\u003E\n\u003Ctd\u003Eint\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Edb_path\u003C\u002Ftd\u003E\n\u003Ctd\u003ERelative path to the sqlite db for storing searches.\u003C\u002Ftd\u003E\n\u003Ctd\u003Estr\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Cp\u003EThe main parts to adjust are \u003Ccode\u003Eencoders\u003C\u002Fcode\u003E and \u003Ccode\u003Edatasets\u003C\u002Fcode\u003E. \u003Ccode\u003Eencoders\u003C\u002Fcode\u003E is a list of\n(auto)encoder definitions for different datatypes.T here are two ways to\nconfigure an (auto)encoder: (a) point to a pre-defined autoencoder or (b)\nconfigure from scratch.\u003C\u002Fp\u003E\n\u003Cp\u003EAssuming you want to use a predefined autoencoder all you have to do is\u003C\u002Fp\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EField\u003C\u002Fth\u003E\n\u003Cth\u003EDescription\u003C\u002Fth\u003E\n\u003Cth\u003EDtype\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003Econtent_type\u003C\u002Ftd\u003E\n\u003Ctd\u003EUnique string identifying the autoencoder in the configuration file\u003C\u002Ftd\u003E\n\u003Ctd\u003Estr\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Efrom_file\u003C\u002Ftd\u003E\n\u003Ctd\u003ERelative path to the encoder configuration file.\u003C\u002Ftd\u003E\n\u003Ctd\u003Estr\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Cp\u003E\u003Cstrong\u003EExample:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-json\"\u003E{\n  &quot;content_type&quot;: &quot;histone-mark-chip-seq-3kb&quot;,\n  &quot;from_file&quot;: &quot;examples\u002Fencoders.json&quot;\n}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EThe encoder configuration file is a dictionary with the top level keys acting\nas the identifier and need to match \u003Ccode\u003Econtent_type\u003C\u002Fcode\u003E above. Given the example\nfrom above the file could look like this:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-json\"\u003E{\n  &quot;histone-mark-chip-seq-3kb&quot;: {},\n  &quot;dnase-seq-3kb&quot;: {}\n}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003ESee \u003Ccode\u003E[encoders.json](encoders.json)\u003C\u002Fcode\u003E for an example. The specific definition if an\nautoencoder is the same as described in the following.\u003C\u002Fp\u003E\n\u003Cp\u003ETo configure an autoencoder from scratch you need to provide a dictionary with\nthe following required format:\u003C\u002Fp\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EField\u003C\u002Fth\u003E\n\u003Cth\u003EDescription\u003C\u002Fth\u003E\n\u003Cth\u003EDefaults\u003C\u002Fth\u003E\n\u003Cth\u003EDtype\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003Eautoencoder\u003C\u002Ftd\u003E\n\u003Ctd\u003ERelative path to your pickled autoencoder model. (hdf5 file)\u003C\u002Ftd\u003E\n\u003Ctd\u003E&nbsp;\u003C\u002Ftd\u003E\n\u003Ctd\u003Estr\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Eencoder\u003C\u002Ftd\u003E\n\u003Ctd\u003ERelative path to your pickled encoder model. (hdf5 file)\u003C\u002Ftd\u003E\n\u003Ctd\u003E&nbsp;\u003C\u002Ftd\u003E\n\u003Ctd\u003Estr\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Edecoder\u003C\u002Ftd\u003E\n\u003Ctd\u003ERelative path to your pickled decoder model. (hdf5 file)\u003C\u002Ftd\u003E\n\u003Ctd\u003E&nbsp;\u003C\u002Ftd\u003E\n\u003Ctd\u003Estr\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Econtent_type\u003C\u002Ftd\u003E\n\u003Ctd\u003EUnique string describing the content this autoencoder can handle. Data tracks with the same content type will be encoded by this autoencoder.\u003C\u002Ftd\u003E\n\u003Ctd\u003E&nbsp;\u003C\u002Ftd\u003E\n\u003Ctd\u003Estr\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Ewindow_size\u003C\u002Ftd\u003E\n\u003Ctd\u003EWindow size in base pairs used for training the autoencoder.\u003C\u002Ftd\u003E\n\u003Ctd\u003E&nbsp;\u003C\u002Ftd\u003E\n\u003Ctd\u003Eint\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Eresolution\u003C\u002Ftd\u003E\n\u003Ctd\u003EResolution or bin size of the window in base pairs.\u003C\u002Ftd\u003E\n\u003Ctd\u003E&nbsp;\u003C\u002Ftd\u003E\n\u003Ctd\u003Eint\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Elatent_dim\u003C\u002Ftd\u003E\n\u003Ctd\u003ENumber of latent dimensions of the encoded windows.\u003C\u002Ftd\u003E\n\u003Ctd\u003E&nbsp;\u003C\u002Ftd\u003E\n\u003Ctd\u003Eint\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Einput_dim\u003C\u002Ftd\u003E\n\u003Ctd\u003ENumber of input dimensions for Keras. For 1D data these are 3: samples, data length (which is \u003Ccode\u003Ewindow_size\u003C\u002Fcode\u003E \u002F \u003Ccode\u003Eresolution\u003C\u002Fcode\u003E), channels.\u003C\u002Ftd\u003E\n\u003Ctd\u003E3\u003C\u002Ftd\u003E\n\u003Ctd\u003Eint\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Echannels\u003C\u002Ftd\u003E\n\u003Ctd\u003ENumber of channels of the input data. This is normally 1.\u003C\u002Ftd\u003E\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\n\u003Ctd\u003Eint\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Cp\u003E\u003Cem\u003ENote that if you have specified an \u003Ccode\u003Eautoencoder\u003C\u002Fcode\u003E you do not need to provide\nseparate \u003Ccode\u003Eencoder\u003C\u002Fcode\u003E and \u003Ccode\u003Edecoder\u003C\u002Fcode\u003E models.\u003C\u002Fem\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EExample:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-json\"\u003E{\n  &quot;encoder&quot;: &quot;path\u002Fto\u002Fmy-12kb-chip-seq-encoder.h5&quot;,\n  &quot;decoder&quot;: &quot;path\u002Fto\u002Fmy-12kb-chip-seq-decoder.h5&quot;,\n  &quot;content_type&quot;: &quot;histone-mark-chip-seq&quot;,\n  &quot;window_size&quot;: 12000,\n  &quot;resolution&quot;: 100,\n  &quot;channels&quot;: 1,\n  &quot;input_dim&quot;: 3,\n  &quot;latent_dim&quot;: 12\n}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EDatasets require the following format:\u003C\u002Fp\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EField\u003C\u002Fth\u003E\n\u003Cth\u003EDescription\u003C\u002Fth\u003E\n\u003Cth\u003EDtype\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003Efilepath\u003C\u002Ftd\u003E\n\u003Ctd\u003ERelative path to the data file (bigWig or bigBed).\u003C\u002Ftd\u003E\n\u003Ctd\u003Estr\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Econtent_type\u003C\u002Ftd\u003E\n\u003Ctd\u003EUnique string describing the content this dataset. If you want to search for patterns in this track you need to have an autoencoder with a matching content type.\u003C\u002Ftd\u003E\n\u003Ctd\u003Estr\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Eid\u003C\u002Ftd\u003E\n\u003Ctd\u003EA unique string identifying your track. (Optional)\u003C\u002Ftd\u003E\n\u003Ctd\u003Estr\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Ename\u003C\u002Ftd\u003E\n\u003Ctd\u003EA human readable name to be shown in HiGlass. (Optional)\u003C\u002Ftd\u003E\n\u003Ctd\u003Estr\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Cp\u003E\u003Cstrong\u003EExample:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-json\"\u003E{\n  &quot;filepath&quot;: &quot;data\u002Fchip-seq\u002Fmy-fancy-gm12878-chip-seq-h3k27ac-fc-signal.bigWig&quot;,\n  &quot;content_type&quot;: &quot;histone-mark-chip-seq&quot;,\n  &quot;uuid&quot;: &quot;my-fancy-gm12878-chip-seq-h3k27c-track&quot;,\n  &quot;name&quot;: &quot;My Fancy GM12878 ChIP-Seq H3k27c Track&quot;\n}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch4 id=\"start-peax\"\u003EStart Peax\u003C\u002Fh4\u003E\n\u003Cp\u003EFirst, start the Peax server to serve your data.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ENote:\u003C\u002Fstrong\u003E The first time you run Peax on a new dataset all the data will be prepared!\nDepending on your machine this can take some time. If you want to track the progress\nactivate the debugging mode using \u003Ccode\u003E-d\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-bash\"\u003Epython start.py\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003ENow go to \u003Ca href=\"http:\u002F\u002Flocalhost:5000\"\u003Ehttp:\u002F\u002Flocalhost:5000\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003ETo \u003Ccode\u003Estart.py\u003C\u002Fcode\u003E script supports the following options:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-bash\"\u003Eusage: start.py [-h] [-c CONFIG] [--clear] [--clear-cache]\n                [--clear-cache-at-exit] [--clear-db] [-d] [--host HOST]\n                [--port PORT] [-v]\n\nPeak Explorer CLI\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -c CONFIG, --config CONFIG\n                        path to your JSON config file\n  --clear               clears the cache and database on startup\n  --clear-cache         clears the cache on startup\n  --clear-cache-at-exit\n                        clear the cache on shutdown\n  --clear-db            clears the database on startup\n  -d, --debug           turn on debug mode\n  --host HOST           customize the hostname\n  --port PORT           customize the port\n  -v, --verbose         turn verbose logging on\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EThe \u003Ccode\u003Ehostname\u003C\u002Fcode\u003E defaults to \u003Ccode\u003Elocalhost\u003C\u002Fcode\u003E and the \u003Ccode\u003Eport\u003C\u002Fcode\u003E of the backend server defaults\nto \u003Ccode\u003E5000\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003EIn order to speed up subsequend user interaction, Peax initially prepapres all\nthe data and caches that data under \u003Ccode\u003E\u002Fcache\u003C\u002Fcode\u003E. You can always remove this\ndirectory manually or clear the cache on startup or at exist using the \u003Ccode\u003E--clear\u003C\u002Fcode\u003E\nas specified above.\u003C\u002Fp\u003E\n\u003Chr\u003E\n\u003Ch2 id=\"development\"\u003EDevelopment\u003C\u002Fh2\u003E\n\u003Cp\u003EHandy commands to keep in mind:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Ccode\u003Emake install\u003C\u002Fcode\u003E installs the conda environment and npm packages and builds the UI\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ccode\u003Emake update\u003C\u002Fcode\u003E updates the conda environment and npm packages and rebuilds the UI\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ccode\u003Emake build\u003C\u002Fcode\u003E builds the UI\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ccode\u003E.\u002Fstart.py\u003C\u002Fcode\u003E starts the Flask server application for serving data\u003C\u002Fli\u003E\n\u003Cli\u003E[\u002Fui]: \u003Ccode\u003Enpm install\u003C\u002Fcode\u003E installs and updates all the needed packages for the frontend\u003C\u002Fli\u003E\n\u003Cli\u003E[\u002Fui]: \u003Ccode\u003Enpm build\u003C\u002Fcode\u003E creates the production built of the frontend\u003C\u002Fli\u003E\n\u003Cli\u003E[\u002Fui]: \u003Ccode\u003Enpm start\u003C\u002Fcode\u003E starts a dev server with hot reloading for the frontend\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003ETo start developing on the server and the ui in parallel, first start the backend server\napplication using \u003Ccode\u003E.\u002Fstart.py\u003C\u002Fcode\u003E and then start the frontend server application from\n\u003Ccode\u003E.\u002Fui\u003C\u002Fcode\u003E using \u003Ccode\u003Enpm start\u003C\u002Fcode\u003E. Both server&#39;s watch the source code, so whenever you make\nchanges to the source code the servers will reload.\u003C\u002Fp\u003E\n\u003Ch3 id=\"configuration\"\u003EConfiguration\u003C\u002Fh3\u003E\n\u003Cp\u003EThere are 2 types of configuration files. The \u003Ca href=\"#configure-peax-with-your-data\"\u003Ebackend server configuration\u003C\u002Fa\u003E\ndefines the datasets to explore and is described in detail \u003Ca href=\"#configure-peax-with-your-data\"\u003Eabove\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003EAdditionally, the frontend application can be configured to talk to a different backend\nserver and port if needed. Get started by copying the example configuration:\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-bash\"\u003Ecd ui &amp;&amp; cp config.json.sample config.json\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EBy default the \u003Ccode\u003Eserver\u003C\u002Fcode\u003E is dynamically set to the hostname of the server running the\nfrontend application. I.e., it is assumed that the backend server application is\nrunning on the same host as the frontend application. The \u003Ccode\u003Eport\u003C\u002Fcode\u003E of the server\ndefaults to \u003Ccode\u003E5000\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\n\u003Ch3 id=\"start-the-backend-and-frontend-apps\"\u003EStart the backend and frontend apps\u003C\u002Fh3\u003E\n\u003Cp\u003EFor development the backend and frontend applications run as seperate server\napplications.\u003C\u002Fp\u003E\n\u003Cpre\u003E\u003Ccode class=\"language-bash\"\u003E# Backend server\n.\u002Fstart.py --debug --config path\u002Fto\u002Fyour\u002Fconfig.json\n\n# Frontend server\ncd ui &amp;&amp; npm start\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n"},{slug:"jenkins-lsci",metadata:{name:a,abbreviation:a,projectName:a,launched:"2016-08-30T00:00:00.000Z",icon:"\u002Fprojects\u002Fjenkins-lsci\u002FJenkins_LifeSci.png",summary:"Life Science Continuous Integration -- Workflows and data pipelines for life science research.",description:"\u003Cp\u003EJenkins-LSCI enables research scientists to build workflows and data pipelines on the same robust framework and plugin ecosystem as Jenkins-CI the widely used continuous integration server that supports building, deploying and automating any software project.\u003C\u002Fp\u003E\n",dateString:"Tue Aug 30 2016",title:a,styles:[],scripts:[]},html:"\u003Ch2 id=\"what-can-jenkins-lsci-do-\"\u003EWhat can Jenkins-LSCI do?\u003C\u002Fh2\u003E\n\u003Cp\u003EJenkins-LSCI enables research scientists to build workflows and data pipelines on the same robust framework and plugin ecosystem as \u003Ca href=\"https:\u002F\u002Fjenkins.io\u002F\"\u003EJenkins-CI\u003C\u002Fa\u003E, the widely used continuous integration server that supports building, deploying and automating any software project.\u003C\u002Fp\u003E\n\u003Ch2 id=\"high-content-image-analysis-with-jenkins-lsci-and-cellprofiler\"\u003EHigh Content Image Analysis with Jenkins-LSCI and CellProfiler\u003C\u002Fh2\u003E\n\u003Cp\u003ETo demonstrate the utility of Jenkins-LSCI as an integration platform for life-sciences research applications, we provide a set of Jenkins-LSCI jobs that can enhance the usability of \u003Ca href=\"http:\u002F\u002Fcellprofiler.org\"\u003ECellProfiler\u003C\u002Fa\u003E in a high performance workflow for high content screening image analysis. In addition, these jobs can form the basis for managing and sharing imaging pipelines and data in ways that can enhance scientific collaboration and reproducibility.\u003C\u002Fp\u003E\n\u003Cp\u003EThese related Jenkins-LSCI jobs enable users to:\u003C\u002Fp\u003E\n\u003Col\u003E\n\u003Cli\u003ECreate a library of re-usable, annotated CellProfiler pipelines.\u003C\u002Fli\u003E\n\u003Cli\u003ECreate a library of re-usable, annotated CellProfiler image lists\u003C\u002Fli\u003E\n\u003Cli\u003EPrepare task arrays for CellProfiler high performance image analysis (CellProfiler batch mode) on a Linux cluster\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003Ch2 id=\"what-are-the-advantages-\"\u003EWhat are the advantages?\u003C\u002Fh2\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cp\u003EImage analysis pipelines, image lists, and generated measurements are all managed by the Jenkins-LSCI server and can be reviewed, shared and reused in multiple ways thus building a \u003Cstrong\u003Ecollaborative foundation for reproducible research\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003ECellProfiler can be parallelized to execute on a \u003Cstrong\u003Ehigh performance\u003C\u002Fstrong\u003E compute cluster.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003EHigh performance image analysis becomes \u003Cstrong\u003Eaccesible to laboratory scientists\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cp\u003EAll functionality is \u003Cstrong\u003Eweb-accessible\u003C\u002Fstrong\u003E. Users do not need to install CellProfiler locally, or run CellProfiler with the potentially limited compute resources of their local workstation\u002Flaptop.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003EThis basic set of jobs can be \u003Cstrong\u003Eeasily extended\u003C\u002Fstrong\u003E with jobs for data and imaging utilities as the computational needs of the laboratory demand.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 id=\"the-jenkins-lsci-cellprofiler-jobs\"\u003EThe Jenkins-LSCI-CellProfiler Jobs\u003C\u002Fh2\u003E\n\u003Ch3 id=\"os_contribute_pipeline\"\u003EOS_CONTRIBUTE_PIPELINE\u003C\u002Fh3\u003E\n\u003Cp\u003EThe \u003Ccode\u003EOS_CONTRIBUTE_PIPELINE\u003C\u002Fcode\u003E job allows users to annotate and upload a CellProfiler pipeline to the Jenkins server. The build generates a pipeline summary report combining user and internal pipeline annotations. The pipeline is stored on the Jenkins server and can be used by CellProfiler running on a remote cluster or workstation, or on the user&#39;s desktop.\u003C\u002Fp\u003E\n\u003Ch3 id=\"os_contribute_imagelist\"\u003EOS_CONTRIBUTE_IMAGELIST\u003C\u002Fh3\u003E\n\u003Cp\u003EThe \u003Ccode\u003EOS_CONTRIBUTE_IMAGELIST\u003C\u002Fcode\u003E job facilitates the creation and management of correctly formatted image lists that can be used for processing large number of images through a CellProfiler pipeline. Image processing is highly dependent on image acquisition and experimental metadata (wavelengths, plates, wells, fields, time-points, Z-stacks etc.) We support the direct parsing of metadata generated during the image acquisition phase from the \u003Cstrong\u003EInCell\u003C\u002Fstrong\u003E and \u003Cstrong\u003EYokogawa\u003C\u002Fstrong\u003E scientific imagers.\u003C\u002Fp\u003E\n\u003Cp\u003EIn the absence of instrument metadata, image lists can be created using the CellProfiler desktop client. These image lists can then be easily adapted using the \u003Ccode\u003EOS_CONTRIBUTE_IMAGELIST\u003C\u002Fcode\u003E job to formats compatible with high performance cluster analysis.\u003C\u002Fp\u003E\n\u003Ch3 id=\"os_cellprofiler_batch\"\u003EOS_CELLPROFILER_BATCH\u003C\u002Fh3\u003E\n\u003Cp\u003EThe \u003Ccode\u003EOS_CELLPROFILER_BATCH\u003C\u002Fcode\u003E job allows users to generate correctly formatted CellProfiler task array scripts that can be submitted to a Linux cluster for parallel processing. The job builds take as input a reference to an \u003Ccode\u003EOS_CONTRIBUTE_PIPELINE\u003C\u002Fcode\u003E build and a reference to an \u003Ccode\u003EOS_CONTRIBUTE_IMAGELIST\u003C\u002Fcode\u003E build. Theses builds define the two inputs for a typical CellProfiler run, the image processing pipeline and the image list.\u003C\u002Fp\u003E\n\u003Cp\u003EThe task array script is currently formatted for the \u003Ca href=\"http:\u002F\u002Fwww.univa.com\u002Fproducts\u002F\"\u003EUNIVA grid engine\u003C\u002Fa\u003E scheduler and it should require little effort to customize for other cluster schedulers. However, due to the non-standardized, local, Linux cluster environment and installed software dependencies, the task array submission process is left up to the user. In addition, CellProfiler and its library dependencies must be installed and correctly working on the destination Linux cluster.\u003C\u002Fp\u003E\n\u003Ch3 id=\"os_cellprofiler_jclustbatch\"\u003EOS_CELLPROFILER_JCLUSTBATCH\u003C\u002Fh3\u003E\n\u003Cp\u003EThe \u003Ccode\u003EOS_CELLPROFILER_JCLUSTBATCH\u003C\u002Fcode\u003E job is an example CellProfiler on cluster job that includes the steps for, creating task arrays, submitting them to the UNIVA grid engine, monitoring the task array execution, and finally merging the individual CellProfiler task output generated on the Linux cluster.\u003C\u002Fp\u003E\n\u003Cp\u003EDue to its high dependency on the Novartis cluster infrastructure this job is unlikely to run unmodified on other Linux clusters, but it provides a useful and complete exemplar on how Jenkins-LSCI is used to integrate a complex workflow and monitor an external job.\u003C\u002Fp\u003E\n\u003Ch2 id=\"test-drive-jenkins\"\u003ETest Drive Jenkins\u003C\u002Fh2\u003E\n\u003Cp\u003EIf you would like to quickly \u003Ca href=\"https:\u002F\u002Fwiki.jenkins-ci.org\u002Fdisplay\u002FJENKINS\u002FMeet+Jenkins\"\u003Etest drive Jenkins-CI\u003C\u002Fa\u003E you can \u003Ca href=\"http:\u002F\u002Fmirrors.jenkins-ci.org\u002Fwar\u002Flatest\u002Fjenkins.war\"\u003Edownload jenkins.war\u003C\u002Fa\u003E directly and launch it by executing \u003Ccode\u003Ejava -jar jenkins.war\u003C\u002Fcode\u003E. Once it launches, visit \u003Ccode\u003Ehttp:\u002F\u002Flocalhost:8080\u002F\u003C\u002Fcode\u003E in your browser to get to the dashboard. On Windows, you can even choose to install \u003Ca href=\"https:\u002F\u002Fwiki.jenkins-ci.org\u002Fdisplay\u002FJENKINS\u002FInstalling+Jenkins+as+a+Windows+service\"\u003EJenkins as a service\u003C\u002Fa\u003E afterwards.\u003C\u002Fp\u003E\n\u003Ch2 id=\"jenkins-lsci-installation-configuration\"\u003EJenkins-LSCI Installation &amp; Configuration\u003C\u002Fh2\u003E\n\u003Cp\u003EA fully functional Jenkins-LSCI server supporting CellProfiler image analysis requires the installation and configuraton of the following software components:\u003C\u002Fp\u003E\n\u003Col\u003E\n\u003Cli\u003EA mirror of the Jenkins-LSCI project code (from git)\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Fjenkins.io\u002F\"\u003EJenkins-CI\u003C\u002Fa\u003E and required Jenkins plugins\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"http:\u002F\u002Fcellprofiler.org\"\u003ECellProfiler\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003Cp\u003EPlease, refer to the \u003Ca href=\".\u002FuserContent\u002Fdocs\u002Finstallation_and_use.html\"\u003EJenkins-LSCI Installation and Usage\u003C\u002Fa\u003E for more details\u003C\u002Fp\u003E\n\u003Ch2 id=\"getting-help\"\u003EGetting Help\u003C\u002Fh2\u003E\n\u003Cp\u003EFor general assistance with Jenkins-CI you can consult the \u003Ca href=\"https:\u002F\u002Fgroups.google.com\u002Fforum\u002F#!forum\u002Fjenkinsci-users\"\u003EJenkins Google user group\u003C\u002Fa\u003E and the extensive, community-maintained \u003Ca href=\"https:\u002F\u002Fwiki.jenkins-ci.org\u002Fdisplay\u002FJENKINS\u002FUse+Jenkins\"\u003EJenkins-CI wiki\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EFor general assistance with CellProfiler you can consult the \u003Ca href=\"http:\u002F\u002Fforum.cellprofiler.org\u002F\"\u003ECellProfiler User Group\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EThe \u003Ca href=\"http:\u002F\u002Fbiouno.org\"\u003EBioUno\u003C\u002Fa\u003E open source project and \u003Ca href=\"https:\u002F\u002Fgroups.google.com\u002Fforum\u002F#!forum\u002Fbiouno-users\"\u003EBioUno Google user group\u003C\u002Fa\u003E can provide additional guidance and assistance in setting up Jenkins for bioinformatics and data science applications\u003C\u002Fp\u003E\n\u003Ch2 id=\"license\"\u003ELicense\u003C\u002Fh2\u003E\n\u003Cp\u003EThe Jenkins-LSCI code is provided under the \u003Ca href=\"http:\u002F\u002Fwww.apache.org\u002Flicenses\u002FLICENSE-2.0.txt\"\u003EApache 2.0\u003C\u002Fa\u003E license\u003C\u002Fp\u003E\n"},{slug:k,metadata:{name:d,abbreviation:d,projectName:k,launched:"2016-04-29T00:00:00.000Z",icon:"\u002Fprojects\u002Fhabitat\u002Fhabitat-icon.png",summary:"Habitat - Where files live. A simple and yet powerful self-contained object storage management system.",description:"\u003Cp\u003EHabitat is a simple and yet powerful self-contained object storage management system.\nBased on Amazon Web Services, it is capable of virtually unlimited storage.\nInstead of a large centralized management system, Habitat can be used as a local repository\nfor a single application or it can be shared and used with many clients.\u003C\u002Fp\u003E\n\u003Cp\u003EHabitat is best used for situations where the client producers and consumers of the files\ndo not require a file system protocol interface and can use http(s) to access the store.\u003C\u002Fp\u003E\n",dateString:"Fri Apr 29 2016",title:d,styles:[],scripts:[]},html:"\u003Ch2 id=\"key-features\"\u003EKey features\u003C\u002Fh2\u003E\n\u003Cul\u003E\n\u003Cli\u003EUpload\u002Fdownload via http(s) from any client that can issue HTTP POST requests\u003C\u002Fli\u003E\n\u003Cli\u003EUpload\u002Fdownload via any tools that support writing\u002Freading from S3. For example:\u003Cul\u003E\n\u003Cli\u003EWeb browser (via a Java applet \u003Ca href=\"http:\u002F\u002Fdocs.aws.amazon.com\u002FAmazonS3\u002Flatest\u002FUG\u002Fenhanced-uploader.htm\"\u003Ehttp:\u002F\u002Fdocs.aws.amazon.com\u002FAmazonS3\u002Flatest\u002FUG\u002Fenhanced-uploader.htm\u003C\u002Fa\u003E)\u003C\u002Fli\u003E\n\u003Cli\u003EWindows client: \u003Ca href=\"http:\u002F\u002Fs3browser.com\"\u003Ehttp:\u002F\u002Fs3browser.com\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003EWindows client: \u003Ca href=\"http:\u002F\u002Fwww.cloudberrylab.com\u002Ffree-amazon-s3-explorer-cloudfront-IAM.aspx#close\"\u003Ehttp:\u002F\u002Fwww.cloudberrylab.com\u002Ffree-amazon-s3-explorer-cloudfront-IAM.aspx#close\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003EFuture: Web browser: we are building a web client that we will likely open source\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003EFuture: upload via file interface where a file is stored by dropping in a temp directory\u003C\u002Fli\u003E\n\u003Cli\u003EFuture: checkout a collection of files into a local file system (setting user\u002Fgroup permissions as desired)\u003C\u002Fli\u003E\n\u003Cli\u003EImmutable reference key for each stored file\u003C\u002Fli\u003E\n\u003Cli\u003EStores metadata about the object from a variety of sources:\u003Cul\u003E\n\u003Cli\u003EParsed from the file name (customized parsing via a regular expression)\u003C\u002Fli\u003E\n\u003Cli\u003EExtracted from the file contents (via a custom plugin)\u003C\u002Fli\u003E\n\u003Cli\u003EExracted from a companion metadata file (uploaded either before or after the data file)\u003C\u002Fli\u003E\n\u003Cli\u003EExtracted from the write action event itself\u003C\u002Fli\u003E\n\u003Cli\u003EExtracted from the S3 metadata attribute on the object\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003ELife cycle management to reduce to lower cost storage or to delete after defined time periods\u003C\u002Fli\u003E\n\u003Cli\u003EGet file or file list based on metadata search\u003C\u002Fli\u003E\n\u003Cli\u003ESearch index using a discrete or shared Elastic Search instance\u003C\u002Fli\u003E\n\u003Cli\u003EFuture: Supports object versioning\u003C\u002Fli\u003E\n\u003Cli\u003EEasily customized to tailor to unique requirements (config file driven, with custom plugins)\u003C\u002Fli\u003E\n\u003Cli\u003ESupports saving a shadow copy of metadata into the S3 object metadata in addition to the Elastic Search index\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n"},{slug:e,metadata:{name:l,abbreviation:e,projectName:e,launched:"2015-10-25T00:00:00.000Z",summary:"A universal remote control for data.",description:"\u003Cp\u003EEnable efficient, non-redundant development of data-dependent applications and utilities, data source querying, data analysis, processing pipelines, extract, transform, and load (ETL) processes. YADA does all this while preserving total decoupling between data access and other aspects of application architecture such as user interface.\u003C\u002Fp\u003E\n",dateString:"Sun Oct 25 2015",title:l,styles:[],scripts:[]},html:"\u003Ch2 id=\"why-yada-\"\u003EWhy YADA?\u003C\u002Fh2\u003E\n\u003Cimg src=\"https:\u002F\u002Fgithub.com\u002FNovartis\u002FYADA\u002Fraw\u002Fmaster\u002Fsrc\u002Fsite\u002Fresources\u002Fimages\u002Fblox250.png\"\u002F\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E is like a \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FUniversal_remote\"\u003E\u003Cem\u003EUniversal Remote Control\u003C\u002Fem\u003E\u003C\u002Fa\u003E for data.\u003C\u002Fp\u003E\n\u003Cp\u003EFor example, what if you could access\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cem\u003Eany\u003C\u002Fem\u003E data set\u003C\u002Fli\u003E\n\u003Cli\u003Eat \u003Cem\u003Eany\u003C\u002Fem\u003E data source\u003C\u002Fli\u003E\n\u003Cli\u003Ein \u003Cem\u003Eany\u003C\u002Fem\u003E format\u003C\u002Fli\u003E\n\u003Cli\u003Efrom \u003Cem\u003Eany\u003C\u002Fem\u003E environment\u003C\u002Fli\u003E\n\u003Cli\u003Eusing \u003Cem\u003Ejust\u003C\u002Fem\u003E a URL\u003C\u002Fli\u003E\n\u003Cli\u003Ewith just \u003Cem\u003Eone-time\u003C\u002Fem\u003E configuration?\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EYou can with \u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003EOr, what if you could get data\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003Efrom \u003Cem\u003Emultiple\u003C\u002Fem\u003E sources\u003C\u002Fli\u003E\n\u003Cli\u003Ein \u003Cem\u003Edifferent\u003C\u002Fem\u003E formats,\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cem\u003Emerging\u003C\u002Fem\u003E the results\u003C\u002Fli\u003E\n\u003Cli\u003Einto a \u003Cem\u003Esingle\u003C\u002Fem\u003E set\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cem\u003Eon-the-fly\u003C\u002Fem\u003E\u003C\u002Fli\u003E\n\u003Cli\u003Ewith \u003Cem\u003Euniform\u003C\u002Fem\u003E column names\u003C\u002Fli\u003E\n\u003Cli\u003Eusing \u003Cem\u003Ejust one\u003C\u002Fem\u003E URL?\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EYou can with \u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\n\u003Ch2 id=\"what-is-yada-\"\u003EWhat is YADA?\u003C\u002Fh2\u003E\n\u003Cp\u003E\u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E exists to simplify data access and eliminate work.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E is secure.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E is a lightweight framework for data retrieval, searching, storage, and manipulation.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E is an instant web service for your data.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E is a tool to enable efficient development of interfaces and data-processing pipelines.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E is as an implementation of \u003Ca href=\"http:\u002F\u002Fbit.ly\u002F1dhuiRY\"\u003EThin Server Architecture\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E is anti-middleware.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E is an acronym for &quot;Yet Another Data Abstraction.&quot;\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E is an open source software framework distributed by \u003Ca href=\"http:\u002F\u002Fopensource.nibr.com\"\u003ENovartis Institutes for BioMedical Research\u003C\u002Fa\u003E under the \u003Ca href=\"http:\u002F\u002Fwww.apache.org\u002Flicenses\u002FLICENSE-2.0.html\"\u003EApache 2.0 license\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003EIts \u003Ca href=\"http:\u002F\u002Fbit.ly\u002F1SHuiAd\"\u003Eraisons d&#39;être\u003C\u002Fa\u003E are to enable efficient, non-redundent development of data-dependent applications and utilities, data source querying, data analysis, processing pipelines, extract, transform, and load (ETL) processes, etc. \u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E does all this while preserving total decoupling between data access and other aspects of application architecture such as user interface.\u003C\u002Fp\u003E\n\u003Ch2 id=\"still-like-huh-\"\u003EStill like &quot;Huh?&quot;\u003C\u002Fh2\u003E\n\u003Cp\u003E\u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E is a software framework, which means it is a collection of software tools forming a basic structure underlying a system, for developers and data analysts to use to create new tools and solutions in a new way.\u003C\u002Fp\u003E\n\u003Cp\u003EThe novelty and utility of YADA lies in its centralization of management of data source access configuration. It simplifies these aspects of software development by eliminating many steps, thereby enabling rapid development, standardization of access methods, and the code in which these methods are implemented. Further it strongly encourages reuse of existing configurations (once configured.)\u003C\u002Fp\u003E\n\u003Cp\u003EAs a result of these configuration facilities, YADA enables the aggregation or integration of data from multiple data sources using a standard method, agnostic with regard to any vendor or technology-specific details of disparate data source implementations.\u003C\u002Fp\u003E\n\u003Cp\u003EFor example, the conventional method to access, or furthermore, combine data from say, an Oracle® database, and a web service, is to write code which connects to each database or service independently using different methods and libraries, write code to execute embedded queries independently, also using different methods and libraries, and write code to parse and aggregate the separately acquired data sets. Then the data is typically fed to an analysis tool.\u003C\u002Fp\u003E\n\u003Cp\u003EWith YADA, the data source connections and application-specific queries are stored securely and centrally, the queries are executed using identical methods (despite the different sources,) and the data can be integrated or aggregated on-the-fly.\u003C\u002Fp\u003E\n\u003Cp\u003EFor software developers and data analysts alike, these features offer potentially tremendous time savings, faster time-to-delivery, and a larger percentage of time focused not on the tedium of configuration, but on the specific context of a software solution or data analysis.\u003C\u002Fp\u003E\n\u003Ch2 id=\"what-s-in-this-document-\"\u003EWhat&#39;s in this document?\u003C\u002Fh2\u003E\n\u003Cp\u003EThis document contains an overview of the framework and features. Check out the \u003Ca href=\"src\u002Fsite\u002Fmarkdown\u002Fdeployment.md\"\u003EQuickstart\u002FDeployment Guide\u003C\u002Fa\u003E for details on getting started.\u003C\u002Fp\u003E\n\u003Ch3 id=\"table-of-contents\"\u003ETable of Contents\u003C\u002Fh3\u003E\n\u003Col\u003E\n\u003Cli\u003E\u003Ca href=\"#other\"\u003EOther Documentation\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"#mindset\"\u003EGetting into the YADA Mindset\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"#features\"\u003EFeatures\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"#arch\"\u003EArchitecture\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"#history\"\u003EHistory\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"#apps\"\u003EYADA Apps and Uses\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"#sources\"\u003EData Sources\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"#plugins\"\u003EPlugins\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"src\u002Fsite\u002Fmarkdown\u002Fdeployment.md\"\u003EInstallation\u003C\u002Fa\u003E (links directly to the \u003Ca href=\"src\u002Fsite\u002Fmarkdown\u002Fdeployment.md\"\u003EQuickstart\u002FDeployment Guide\u003C\u002Fa\u003E)\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"#license\"\u003ELicense\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"#issues\"\u003EKnown Issues\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\u003Cp\u003E\u003Ca name=\"other\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Ch2 id=\"other-documentation\"\u003EOther Documentation\u003C\u002Fh2\u003E\n\u003Ch3 id=\"getting-started\"\u003EGetting Started\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Ca href=\"src\u002Fsite\u002Fmarkdown\u002Fdeployment.md\"\u003EQuickstart\u002FDeployment Guide\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 id=\"advanced-documentation\"\u003EAdvanced Documentation\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Ca href=\"src\u002Fsite\u002Fmarkdown\u002Fadmin.md\"\u003EYADA Admin Guide\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"src\u002Fsite\u002Fmarkdown\u002Fguide.md\"\u003EUser Guide\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"src\u002Fsite\u002Fmarkdown\u002Fsecurity.md\"\u003ESecurity Guide\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"src\u002Fsite\u002Fmarkdown\u002Fpluginguide.md\"\u003EPlugin Use and Development Guide\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"src\u002Fsite\u002Fmarkdown\u002Ftesting.md\"\u003ETesting Notes\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 id=\"specifications\"\u003ESpecifications\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Ca href=\"http:\u002F\u002Fopensource.nibr.com\u002Fyada\u002Fyada-api\u002Fapidocs\u002Findex.html\"\u003EJavadoc\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"src\u002Fsite\u002Fmarkdown\u002Fparams.md\"\u003EYADA Parameter Specification\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"src\u002Fsite\u002Fmarkdown\u002Fjsonparams.md\"\u003EJSONParams Specification\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"src\u002Fsite\u002Fmarkdown\u002Fharmony.md\"\u003EHarmonizer Guide and Specification\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"src\u002Fsite\u002Fmarkdown\u002Fuml.md\"\u003EJava® Visual Reference\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"src\u002Fsite\u002Fmarkdown\u002Ffilters.md\"\u003EFilters Specification\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"src\u002Fsite\u002Fmarkdown\u002Fmail.md\"\u003EMail Specification\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E\u003Ca name=\"mindset\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Ch2 id=\"getting-into-the-yada-mindset\"\u003EGetting into the YADA Mindset\u003C\u002Fh2\u003E\n\u003Cp\u003EYADA exists to simplify data access and eliminate work.\u003C\u002Fp\u003E\n\u003Cp\u003EYADA may be exactly what you&#39;ve been looking for, or it may be a solution to a problem you didn&#39;t know you had. YADA is the perfect tool for many use-cases. Here are a few examples. Suppose you are a\u003C\u002Fp\u003E\n\u003Ch3 id=\"scientist-or-data-analyst-\"\u003EScientist or Data Analyst...\u003C\u002Fh3\u003E\n\u003Cp\u003EThe new numbers are in from the lab, or from last night&#39;s feed, and uploaded to your database or data warehouse. You want to create a new visualization in your favorite statistical analysis package, but you&#39;re not sure how to connect to the database. Your data gal helped you set it up in Python once, but since then, you just run the script to get the data. Now you need it in a different environment.\u003C\u002Fp\u003E\n\u003Cp\u003EIf your data gal had set up a \u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E query for your datasource, you could simply run the query in your web browser to download the data, or use any module that will retrieve data from a url. You can reuse the \u003Cem\u003Every same query\u003C\u002Fem\u003E that was already configured for your other tasks.\u003C\u002Fp\u003E\n\u003Ch3 id=\"datasource-owner-curator-\"\u003EDatasource Owner\u002FCurator...\u003C\u002Fh3\u003E\n\u003Cp\u003EYour constituents want their data, and they call you. Everyone wants basically the same set of columns but with a different &quot;WHERE&quot; clause, i.e., they each want a different subset of rows. Some can handle connection strings, but most can&#39;t.\u003C\u002Fp\u003E\n\u003Cp\u003ESo you configure your datasource in the \u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E server, store a query, and send the same url to every one, explaining to them where to plug in the values in the query parameter string so they get only the data they want. They might see some columns they don&#39;t want but they can easily ignore them. If someone complains, heck, you just store another similar query with a different name, and voilà.\u003C\u002Fp\u003E\n\u003Ch3 id=\"software-user-interface-developer-\"\u003ESoftware User Interface Developer...\u003C\u002Fh3\u003E\n\u003Cp\u003EYou hate middleware. Every time you want to extend the data model, you have to change your Resource layer, your DAO layer, your DAOImpls, your DTOs, your Model classes, your UI code, etc. You might have to touch 20 files to add one field.\u003C\u002Fp\u003E\n\u003Cp\u003ENot so with \u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003EWith \u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E, you change your stored query, and you change the code that executes the stored query, whether it&#39;s a javascript-based ajax call, or a perl LWP request, or a curl call from a shell script. As long as your client speaks HTTP, \u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E will deliver your data.\u003C\u002Fp\u003E\n\u003Ch3 id=\"software-middleware-developer-\"\u003ESoftware Middleware Developer...\u003C\u002Fh3\u003E\n\u003Cp\u003EEven you, middleware guy, can benefit from \u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003EMaybe you have to provide a RESTful interface to an existing application, and need to deliver in such a short window, or have only a handful of users, so a fully-specified REST service is not practical. Maybe you need to access an existing REST interface and can&#39;t use your own proxy script; or, you have to grant access to a unix filesystem without mapping it in Apache or changing privileges. Perhaps the business wants to integrate some existing perl-based pipeline processes into a user interface or your Javascript UI team is already using \u003Cstrong\u003EYADA\u003C\u002Fstrong\u003E, and needs a Java® plugin to post-process data it&#39;s retrieving from a third party.\u003C\u002Fp\u003E\n\u003Ch2 id=\"features\"\u003EFeatures\u003C\u002Fh2\u003E\n\u003Cul\u003E\n\u003Cli\u003EData vendor- and technology-agnostic\u003C\u002Fli\u003E\n\u003Cli\u003EAccesses any JDBC, SOAP, or REST, and some Filesystem datasources\u003C\u002Fli\u003E\n\u003Cli\u003EDelivers data as JSON (default), XML, or CSV, TSV, Pipe, or custom-delimited, natively, and in any other format via custom Response and Converter classes, or Plugins\u003C\u002Fli\u003E\n\u003Cli\u003E4-layer security model including url and token validation, query-execution authorization, and dynamic-predicate-based, pre-execution, row-level filtering\u003C\u002Fli\u003E\n\u003Cli\u003EDynamic datasource configuration\u003C\u002Fli\u003E\n\u003Cli\u003EExecutes multiple queries in a single HTTP request\u003C\u002Fli\u003E\n\u003Cli\u003EOn-the-fly inner and outer joins across disparate data sources\u003C\u002Fli\u003E\n\u003Cli\u003EAd hoc Harmonization (i.e., single http request to multiple data sources with harmonized results)\u003C\u002Fli\u003E\n\u003Cli\u003EUtilizes JDBC transactions (e.g., multiple inserts in a single HTTP request, with a single commit\u002Frollback)\u003C\u002Fli\u003E\n\u003Cli\u003ECommits a single query or an entire request\u003C\u002Fli\u003E\n\u003Cli\u003EProcesses file uploads\u003C\u002Fli\u003E\n\u003Cli\u003ECompatible with any client that speaks HTTP (e.g., web browser, python, curl, javascript, spotfire, curl, web service, mobile app, etc)\u003C\u002Fli\u003E\n\u003Cli\u003EFlexible Java® and Script plugin API to preprocess request parameters, post-process results, or override normal processing altogether\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca href=\"http:\u002F\u002Fwww.ehcache.org\u002F\"\u003EEhCache\u003C\u002Fa\u003E query-index caching\u003C\u002Fli\u003E\n\u003Cli\u003ESecurity (via Cookie forwarding and\u002For Default Plugins)\u003C\u002Fli\u003E\n\u003Cli\u003ESupport for Oracle®, MySQL®, Vertica®, PostgreSQL®, HyperSQL®, SQLite®\u003C\u002Fli\u003E\n\u003Cli\u003ETomcat 8 and JDK 1.8-compatible (YADA 8)\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cem\u003EComing Later\u003C\u002Fem\u003E: ElasticSearch® support\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cem\u003EComing Later\u003C\u002Fem\u003E: ElasticSearch®-based result and aggregate-result caching\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cem\u003EComing Later\u003C\u002Fem\u003E: Dynamic memory management and caching to facilitate large-scale request queuing and high volume result transformation in high frequency environments\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cem\u003EComing Later\u003C\u002Fem\u003E: SQL DDL\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cem\u003EComing Later\u003C\u002Fem\u003E: MongoDB® and other NoSQL support\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cem\u003EComing Later\u003C\u002Fem\u003E: SQL Server® support\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cem\u003EComing Later\u003C\u002Fem\u003E: Node.js® port (maybe?)\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cem\u003EComing Later\u003C\u002Fem\u003E: Scala?\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cem\u003EComing Later\u003C\u002Fem\u003E: Standalone java application\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cem\u003EComing Later\u003C\u002Fem\u003E: Spark-based result post-processor\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E\u003Ca name=\"arch\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Ch2 id=\"architecture\"\u003EArchitecture\u003C\u002Fh2\u003E\n\u003Cp\u003EA quick overview of the architecture\u003C\u002Fp\u003E\n\u003Ch3 id=\"generic\"\u003EGeneric\u003C\u002Fh3\u003E\n\u003Cp\u003EAbout as basic as it can be...\n\u003Cimg src=\"https:\u002F\u002Fgithub.com\u002FNovartis\u002FYADA\u002Fraw\u002Fmaster\u002Fsrc\u002Fsite\u002Fresources\u002Fimages\u002Fgeneric-arch.png\" alt=\"generic architecture\" title=\"Generic Architecture\"\u003E\u003C\u002Fp\u003E\n\u003Ch3 id=\"specific\"\u003ESpecific\u003C\u002Fh3\u003E\n\u003Cp\u003E...and a little bit more specific:\u003C\u002Fp\u003E\n\u003Cblockquote\u003E\n\u003Cp\u003ENote the image indicates Tomcat 6. It should be Tomcat \u003Cstrong\u003E7\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fgithub.com\u002FNovartis\u002FYADA\u002Fraw\u002Fmaster\u002Fsrc\u002Fsite\u002Fresources\u002Fimages\u002Fspecific-arch.png\" alt=\"specific architecture\" title=\"Specific Architecture\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ca name=\"history\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Ch2 id=\"history\"\u003EHistory\u003C\u002Fh2\u003E\n\u003Cp\u003EYADA grew organically from a reverse-engineering effort.\u003C\u002Fp\u003E\n\u003Cp\u003EOver the course of a few years, a scientist had developed an array of Perl CGI applications with thousands of lines of embedded SQL and Javascript.\u003C\u002Fp\u003E\n\u003Cp\u003EThen he abruptly left the company.\u003C\u002Fp\u003E\n\u003Cp\u003EHe was not a trained, nor experienced software developer, he made little use of third party libraries, and violated a lot of conventions.\u003C\u002Fp\u003E\n\u003Cp\u003ETo gain an understanding of his code in order to maintain, extend, or replace it, SQL queries were extracted from the code, stored in a database, and given unique names.\u003C\u002Fp\u003E\n\u003Cp\u003EA &quot;finder&quot; function was written in Perl to retrieve the SQL by name.\u003C\u002Fp\u003E\n\u003Cp\u003EThis &quot;finder&quot; was extended to support the passing of parameters.\u003C\u002Fp\u003E\n\u003Cp\u003ESoon thereafter, this perl utility was ported to Java®. The burgeoning framework was extended further to support multiple data types, INSERT, UPDATE, and DELETE statements in addition to SELECT statements, JDBC transactions, SOAP queries, plugins, I\u002FO, and so on.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ca name=\"apps\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Ch2 id=\"yada-apps-and-uses\"\u003EYADA &quot;Apps&quot; and uses\u003C\u002Fh2\u003E\n\u003Cp\u003EMost YADA &quot;Apps&quot; are \u003Ca href=\"http:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FSingle-page_application\"\u003Esingle-page Javascript applications\u003C\u002Fa\u003E running in web browsers. YADA is also heavily utilized by data analysts and bioinformaticians who need parameterized, delimited data subsets imported into their analysis tools such as R and Spotfire, or to be used by Perl or Python-based data processing pipelines.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ca name=\"sources\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Ch2 id=\"data-sources\"\u003EData Sources\u003C\u002Fh2\u003E\n\u003Cp\u003EYADA ships with scripts for using, as the YADA Index:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EMySQL®\u003C\u002Fli\u003E\n\u003Cli\u003EPostgreSQL®\u003C\u002Fli\u003E\n\u003Cli\u003EHyperSQL®\u003C\u002Fli\u003E\n\u003Cli\u003ESQLite®\u003C\u002Fli\u003E\n\u003Cli\u003EOracle®\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003ESoon the index will be stored in ElasticSearch®, but ultimately, it is vendor-agnostic. Other supported data sources currently include\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EVertica®\u003C\u002Fli\u003E\n\u003Cli\u003ESOAP\u003C\u002Fli\u003E\n\u003Cli\u003EREST\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EMongoDB®, SQL Server®, and other datasource compatibility will be added soon.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ca name=\"plugins\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Ch2 id=\"plugins\"\u003EPlugins\u003C\u002Fh2\u003E\n\u003Cp\u003EFor detailed information about plugin use and development, see the \u003Ca href=\"src\u002Fsite\u002Fmarkdown\u002Fpluginguide.md\"\u003EPlugin Use and Development Guide\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003EThe plugin API is versatile. Plugins can be written in java, or in any scripting language supported on the YADA server. Plugins can be applied at the request level, affecting the entire request, or it&#39;s output, or at the query level, affecting just a single query in a request. The conceptual, or implementation hierarchy of the plugin API (not to be confused with the actual package hierarchy) is reflected in the diagram below, from two different perspectives.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fgithub.com\u002FNovartis\u002FYADA\u002Fraw\u002Fmaster\u002Fsrc\u002Fsite\u002Fresources\u002Fimages\u002Fplugin-concept.png\" alt=\"Plugin Concept\" title=\"Plugin Concept\"\u003E\u003C\u002Fp\u003E\n\u003Ch3 id=\"plugin-types\"\u003EPlugin Types\u003C\u002Fh3\u003E\n\u003Ch4 id=\"pre-processors\"\u003EPre-Processors\u003C\u002Fh4\u003E\n\u003Cp\u003EThese are intended to manipulate URL parameters, either by removing, appending, or modifying them.\u003C\u002Fp\u003E\n\u003Ch4 id=\"post-processors\"\u003EPost-Processors\u003C\u002Fh4\u003E\n\u003Cp\u003EThese are intended to modify results returned by queries. For example, an XSL Post-Processor might accept XML-formatted results and transform them before returning the to the client. Uploaded file processors, i.e., batch handlers, are post-processors.\u003C\u002Fp\u003E\n\u003Ch4 id=\"bypassers\"\u003EBypassers\u003C\u002Fh4\u003E\n\u003Cp\u003EThese circumvent conventional YADA query processing. Effectively, anything is possible in a Bypass. Bypass plugins are popular ETL tools and bulk data loaders.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ca name=\"license\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Ch2 id=\"license\"\u003ELicense\u003C\u002Fh2\u003E\n\u003Cp\u003ECopyright &copy; 2016 \u003Ca href=\"http:\u002F\u002Fopensource.nibr.com\"\u003ENovartis Institutes for Biomedical Research\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Cp\u003ELicensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at \u003Ca href=\"http:\u002F\u002Fwww.apache.org\u002Flicenses\u002FLICENSE-2.0\"\u003Ehttp:\u002F\u002Fwww.apache.org\u002Flicenses\u002FLICENSE-2.0\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Ca name=\"issues\"\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Ch3 id=\"known-issues-last-updated-25-oct-2015-\"\u003EKnown Issues (last updated 25-OCT-2015)\u003C\u002Fh3\u003E\n\u003Cul\u003E\n\u003Cli\u003EDate and time value syntax, just like in the real world, are database-vendor specific. Use vendor-specific literals and functions. Check the test queries for guidance.\u003C\u002Fli\u003E\n\u003Cli\u003ESpeaking of dates and times, right now the TestNG tests which validate date and time values pass only on machines in the &quot;America\u002FNewYork&quot; timezone. This is likely because the insert statements used to put the test data into the test table is not specific.\u003C\u002Fli\u003E\n\u003Cli\u003EThere are two drivers for SQL Server®. The one I picked has problems, and I haven&#39;t made time to work with the other.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n"},{slug:m,metadata:{name:f,abbreviation:f,projectName:m,launched:"2015-03-23T00:00:00.000Z",summary:"A web-based application for managing ontologies and assigning synonyms to ontology terms.\n",description:"\u003Cp\u003EThe OntoBrowser tool was developed to manage ontologies and code lists. The primary goal of the tool is to provide an online collaborative solution for expert curators to map code list terms (sourced from multiple systems\u002Fdatabases) to preferred ontology terms. Other key features include visualisation of ontologies in hierarchical\u002Fgraph format, advanced search capabilities, peer review\u002Fapproval workflow and web service access to data.\u003C\u002Fp\u003E\n",icon:"\u002Fprojects\u002Fontobrowser\u002Fontobrowser-logo.png",dateString:"Mon Mar 23 2015",title:f,styles:[],scripts:[]},html:"\n\u003Ch2\u003EKey Features\u003C\u002Fh2\u003E\n\u003Cul\u003E\n  \u003Cli\u003EWeb based collaborative ontology curation\u003C\u002Fli\u003E\n  \u003Cli\u003EInteractive hierarchical\u002Fgraph visualisation\u003C\u002Fli\u003E\n  \u003Cli\u003ECross ontology searching\u003C\u002Fli\u003E\n  \u003Cli\u003ESynonym mapping\u003C\u002Fli\u003E\n  \u003Cli\u003EAutomated mapping of similar matching synonyms\u003C\u002Fli\u003E\n  \u003Cli\u003ECentral database for all ontologies\u003C\u002Fli\u003E\n  \u003Cli\u003EVersion tracking\u003C\u002Fli\u003E\n  \u003Cli\u003EReview\u002FApprove workflow\u003C\u002Fli\u003E\n  \u003Cli\u003EEmail notification\u003C\u002Fli\u003E\n  \u003Cli\u003EFull curator audit trail\u002Fhistory\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\n\u003Ch2\u003EDocumentation\u003C\u002Fh2\u003E\n\u003Cul\u003E\n  \u003Cli\u003E\n    \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FNovartis\u002Fontobrowser\u002Fblob\u002Fmaster\u002Fdoc\u002FINSTALL.md\"\n      \u003EBuild and Deploy\u003C\u002Fa\n    \u003E\n  \u003C\u002Fli\u003E\n  \u003Cli\u003E\n    \u003Ca\n      href=\"https:\u002F\u002Fgithub.com\u002FNovartis\u002Fontobrowser\u002Fblob\u002Fmaster\u002Fdoc\u002Fapproval_workflow.pdf\"\n      \u003EApproval Workflow\u003C\u002Fa\n    \u003E\n  \u003C\u002Fli\u003E\n  \u003Cli\u003E\n    \u003Ca\n      href=\"https:\u002F\u002Fgithub.com\u002FNovartis\u002Fontobrowser\u002Fblob\u002Fmaster\u002Fdoc\u002Fweb_services.md\"\n      \u003EWeb Services\u003C\u002Fa\n    \u003E\n  \u003C\u002Fli\u003E\n  \u003Cli\u003E\n    \u003Ca\n      href=\"https:\u002F\u002Fgithub.com\u002FNovartis\u002Fontobrowser\u002Fblob\u002Fmaster\u002Fdoc\u002Fdatabase_design.pdf\"\n      \u003EDatabase Design\u003C\u002Fa\n    \u003E\n  \u003C\u002Fli\u003E\n  \u003Cli\u003E\n    \u003Ca\n      href=\"https:\u002F\u002Fgithub.com\u002FNovartis\u002Fontobrowser\u002Fblob\u002Fmaster\u002Fdoc\u002Fsecurity_review.md\"\n      \u003ESecurity Review\u003C\u002Fa\n    \u003E\n  \u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\n\u003Ch2\u003EAcknowledgements\u003C\u002Fh2\u003E\n\u003Cp\u003E\n  The OntoBrowser was initially developed in frame of the eTOX consortium. It\n  has received support from the Innovative Medicines Initiative Joint\n  Undertaking under grant agreement 115002, resources of which are composed of\n  financial contribution from the European Union's Seventh Framework Programme\n  (FP7\u002F2007-2013) and EFPIA companies' in kind contribution. We would like to\n  formally acknowledge the contribution to the eTOX project of all scientists\n  and other staff involved.\n\u003C\u002Fp\u003E\n\u003Ccenter\u003E\n  \u003Ca href=\"http:\u002F\u002Fwww.etoxproject.eu\"\n    \u003E\u003Cimg\n      style=\"padding-right:20px;border-style:none\"\n      src=\"\u002Fprojects\u002Fontobrowser\u002Fetox-logo.png\"\n      class=\"scale-image\"\n  \u002F\u003E\u003C\u002Fa\u003E\n  \u003Ca href=\"http:\u002F\u002Fwww.imi.europa.eu\"\n    \u003E\u003Cimg\n      style=\"padding-left:20px;border-style:none\"\n      src=\"\u002Fprojects\u002Fontobrowser\u002Fimi-logo.png\"\n      class=\"scale-image\"\n  \u002F\u003E\u003C\u002Fa\u003E\n\u003C\u002Fcenter\u003E\n"},{slug:n,metadata:{name:g,abbreviation:g,projectName:n,icon:"\u002Fprojects\u002Frailroadtracks\u002Frailroadtracks-logo.png",launched:"2014-11-07T00:00:00.000Z",buttons:[{link:"railroadtracks.pdf",icon:"file",text:"Documentation (PDF)"}],summary:"A toolkit for DNA and RNA-Seq processing steps.\n",description:"\u003Cp\u003E\u003Ccode\u003Erailroadtracks\u003C\u002Fcode\u003E is a Python toolkit to handle graphs of dependent tasks such as the ones found in bioinformatics pipelines.\u003C\u002Fp\u003E\n\u003Cp\u003EIt was created for comparing RNA-Seq pipelines and found its use is other situations, such as writing a flexible system \nfor the QC of NGS data.\u003C\u002Fp\u003E\n",dateString:"Fri Nov 07 2014",title:g,styles:[],scripts:[]},html:"\n\u003Cp\u003E\u003Ccode\u003Erailroadtracks\u003C\u002Fcode\u003E provides the following main features:\u003C\u002Fp\u003E\n\n\u003Cul\u003E\n  \u003Cli\u003E\n    \u003Ci\u003Ead hoc\u003C\u002Fi\u003E creation of pipelines, interactive use in mind and\n    \u003Ca href=\"http:\u002F\u002Fipython.org\"\u003Eipython\u003C\u002Fa\u003E-specify display of objects\n  \u003C\u002Fli\u003E\n\n  \u003Cli\u003Eseparation of the declaration of tasks from their execution\u003C\u002Fli\u003E\n\n  \u003Cli\u003E\n    simple abstractions to perform parallel computing allowing computations to\n    be moved easily to different models for parallel and distributed computing\n  \u003C\u002Fli\u003E\n\n  \u003Cli\u003E\n    a fully-extendable and editable model layer unifying popular tools in DNA\n    and RNA-sequencing data processing under one common interface.\n  \u003C\u002Fli\u003E\n\n  \u003Cli\u003E\n    It can be installed as a regular Python package, for example using\n    \u003Ccode\u003Epip install\u003C\u002Fcode\u003E. A tutorial as an ipython notebook is avaible as\n    part of the documentation for the package.\n  \u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\n\u003Cdiv\n  id=\"carousel-example-generic\"\n  class=\"carousel slide\"\n  data-ride=\"carousel\"\n  data-interval=\"3000\"\n\u003E\n  \u003Cdiv class=\"carousel-inner\"\u003E\n    \u003Cdiv class=\"item active\"\u003E\n      \u003Cdiv class=\"carousel-caption\" style=\"background: rgba(255, 255, 255, .6)\"\u003E\n        \u003Ch3\u003EUse in the ipython notebook\u003C\u002Fh3\u003E\n      \u003C\u002Fdiv\u003E\n      \u003Cimg\n        src=\"\u002Fprojects\u002Frailroadtracks\u002Fipython.png\"\n        alt=\"First slide\"\n        height=\"400px\"\n      \u002F\u003E\n    \u003C\u002Fdiv\u003E\n    \u003Cdiv class=\"item\"\u003E\n      \u003Cdiv class=\"carousel-caption\" style=\"background: rgba(255, 255, 255, .6)\"\u003E\n        \u003Ch3\u003EModel for RNA-Seq\u003C\u002Fh3\u003E\n      \u003C\u002Fdiv\u003E\n      \u003Cimg\n        src=\"\u002Fprojects\u002Frailroadtracks\u002Fmodel.svg\"\n        alt=\"Second slide\"\n        height=\"400px\"\n      \u002F\u003E\n    \u003C\u002Fdiv\u003E\n  \u003C\u002Fdiv\u003E\n  \u003C!-- Controls --\u003E\n  \u003Ca\n    class=\"left carousel-control\"\n    href=\"#carousel-example-generic\"\n    role=\"button\"\n    data-slide=\"prev\"\n  \u003E\n    \u003Cspan class=\"glyphicon glyphicon-chevron-left\"\u003E\u003C\u002Fspan\u003E\n  \u003C\u002Fa\u003E\n  \u003Ca\n    class=\"right carousel-control\"\n    href=\"#carousel-example-generic\"\n    role=\"button\"\n    data-slide=\"next\"\n  \u003E\n    \u003Cspan class=\"glyphicon glyphicon-chevron-right\"\u003E\u003C\u002Fspan\u003E\n  \u003C\u002Fa\u003E\n\u003C\u002Fdiv\u003E\n"},{slug:o,metadata:{name:p,abbreviation:"YAP",projectName:o,launched:"2014-11-06T00:00:00.000Z",icon:"yap_logo.png",summary:"YAP allows researchers to quickly build high throughput big data pipelines without extensive\nknowledge of parallel programming.\n",description:"\u003Cp\u003EYAP is an extensible parallel framework, written in Python using \u003Ca href=\"http:\u002F\u002Fwww.open-mpi.org\u002F\"\u003EOpenMPI\u003C\u002Fa\u003E libraries. It\nallows researchers to quickly build high throughput big data pipelines without extensive knowledge of parallel\nprogramming. The user interacts with the framework through simple configuration files to capture analysis parameters\nand user directed metadata, enabling reproducible research. Using YAP, analysts have been able to achieve a\nsignificant speed up of up to 36× in \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FRNA-Seq\"\u003ERNASeq\u003C\u002Fa\u003E workflow execution time.\u003C\u002Fp\u003E\n\u003Cp\u003EYAP has been designed to be scalable and flexible. We have implemented YAP with a focus on next-generation sequencing\n(NGS), to meet the large data processing challenges at NIBR. However, the framework can be easily adapted for any kind\nof analysis. It can be executed on your local Linux workstations or large HPC cluster systems. The framework achieves\nefficiency by implementing optimal data handling mechanisms such as, parallel data distribution, avoiding file I\u002FO\nusing data streams and named pipes.\u003C\u002Fp\u003E\n",dateString:"Thu Nov 06 2014",title:p,styles:[],scripts:[]},html:"\n\u003Ch2\u003E YAP compared to analysts' scripts \u003C\u002Fh2\u003E\n\u003Ctable\u003E\n  \u003Cthead\u003E\n    \u003Ctr\u003E\n      \u003Cth\u003EAnalysis\u003C\u002Fth\u003E\n      \u003Cth\u003EData size\u003C\u002Fth\u003E\n      \u003Cth\u003ENumber of cores\u003C\u002Fth\u003E\n      \u003Cth\u003EAnalyst methods (hrs)\u003C\u002Fth\u003E\n      \u003Cth\u003EYAP (hrs)\u003C\u002Fth\u003E\n      \u003Cth\u003ESpeed-up\u003C\u002Fth\u003E\n    \u003C\u002Ftr\u003E\n  \u003C\u002Fthead\u003E\n  \u003Ctbody\u003E\n    \u003Ctr\u003E\n      \u003Cth\u003ERNASeq QC and Counts\u003C\u002Fth\u003E\n      \u003Ctd\u003E3 billion reads (150 samples)\u003C\u002Ftd\u003E\n      \u003Ctd\u003E500\u003C\u002Ftd\u003E\n      \u003Ctd\u003E325.6\u003C\u002Ftd\u003E\n      \u003Ctd\u003E9\u003C\u002Ftd\u003E\n      \u003Ctd\u003E\n        36&times;\n      \u003C\u002Ftd\u003E\n    \u003C\u002Ftr\u003E\n    \u003Ctr\u003E\n      \u003Cth\u003EBacterial studies using Mothur\u003C\u002Fth\u003E\n      \u003Ctd\u003E230,000 reads\u003C\u002Ftd\u003E\n      \u003Ctd\u003E72\u003C\u002Ftd\u003E\n      \u003Ctd\u003E90\u003C\u002Ftd\u003E\n      \u003Ctd\u003E12\u003C\u002Ftd\u003E\n      \u003Ctd\u003E8&times;\u003C\u002Ftd\u003E\n    \u003C\u002Ftr\u003E\n    \u003Ctr\u003E\n      \u003Cth\u003EChIPSeq Peak Calls\u003C\u002Fth\u003E\n      \u003Ctd\u003E190 million reads (6 samples)\u003C\u002Ftd\u003E\n      \u003Ctd\u003E12\u003C\u002Ftd\u003E\n      \u003Ctd\u003E9.3\u003C\u002Ftd\u003E\n      \u003Ctd\u003E4.5\u003C\u002Ftd\u003E\n      \u003Ctd\u003E2&times;\u003C\u002Ftd\u003E\n    \u003C\u002Ftr\u003E\n    \u003Ctr\u003E\n      \u003Cth\u003EEQP\u003C\u002Fth\u003E\n      \u003Ctd\u003E400 million reads (5 samples)\u003C\u002Ftd\u003E\n      \u003Ctd\u003E60\u003C\u002Ftd\u003E\n      \u003Ctd\u003E45\u003C\u002Ftd\u003E\n      \u003Ctd\u003E12\u003C\u002Ftd\u003E\n      \u003Ctd\u003E4&times;\u003C\u002Ftd\u003E\n    \u003C\u002Ftr\u003E\n  \u003C\u002Ftbody\u003E\n\u003C\u002Ftable\u003E\n\n\u003Cdiv class=\"container\" style=\"text-align:center;\"\u003E\n  \u003Cdiv class=\"row\"\u003E\n    \u003Cdiv class=\"col-sm-3\"\u003E\u003C\u002Fdiv\u003E\n    \u003Cdiv class=\"col-sm-6\"\u003E\n      \u003Ctable class=\"table table-striped table-bordered\"\u003E\n        \u003Cthead\u003E\n          \u003Ctr\u003E\n            \u003Cth\u003E\u003C\u002Fth\u003E\n            \u003Cth\u003ETraditional method\u003C\u002Fth\u003E\n            \u003Cth\u003EYAP\u003C\u002Fth\u003E\n          \u003C\u002Ftr\u003E\n        \u003C\u002Fthead\u003E\n        \u003Ctbody\u003E\n          \u003Ctr\u003E\n            \u003Cth\u003EI\u002FO steps\u003C\u002Fth\u003E\n            \u003Ctd\u003E1400 file reads\u003Cbr\u003E1200 file writes\u003C\u002Ftd\u003E\n            \u003Ctd\u003E200 file reads\u003Cbr\u003E800 file writes\u003C\u002Ftd\u003E\n          \u003C\u002Ftr\u003E\n          \u003Ctr\u003E\n            \u003Cth\u003EJobs spawned\u003C\u002Fth\u003E\n            \u003Ctd\u003E1500\u003C\u002Ftd\u003E\n            \u003Ctd\u003E1 MPI job\u003C\u002Ftd\u003E\n          \u003C\u002Ftr\u003E\n          \u003Ctr\u003E\n            \u003Ctd colspan=3\u003E\n              File-based reads reduced by \u003Cspan class=\"label label-success\"\u003E70%\u003C\u002Fspan\u003E\n              \u003Cbr\u003E\n              File-based writes reduced by \u003Cspan class=\"label label-success\"\u003E30%\u003C\u002Fspan\u003E\n            \u003C\u002Ftd\u003E\n          \u003C\u002Ftr\u003E\n        \u003C\u002Ftbody\u003E\n      \u003C\u002Ftable\u003E\n    \u003C\u002Fdiv\u003E\n  \u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Ch2\u003E Example Analysis Output \u003C\u002Fh2\u003E\n\u003Cp\u003E\n\n  The following images are the results of various applications run within the\n  YAP framework, such as FastQC, FastQScreen, PicardTools, etc.\n\n\u003C\u002Fp\u003E\n\u003Cimg src=\"\u002Fprojects\u002Fyap\u002Fimages\u002Fsample_output_images.png\" class=\"scale-image\"\u003E\n\n\u003Cp\u003E\n\n  \u003Cstrong\u003EYAP consolidates results from across the samples for the various\n      packages, such as gene counts from HTSeq and normalized counts from Cufflinks.\u003C\u002Fstrong\u003E\n\n\u003C\u002Fp\u003E\n\n\u003Cdiv class=\"row\"\u003E\n  \u003Cdiv class=\"col-md-6\"\u003E\n    \u003Ch5\u003EHTSeq gene counts\u003C\u002Fh5\u003E\n    \u003Ctable class='table table-striped table-bordered'\u003E\n        \u003Cthead\u003E\n          \u003Ctr\u003E\n            \u003Cth\u003E\n              SAMPLE\n            \u003C\u002Fth\u003E\n            \u003Cth\u003E\n              CR560274_1\n            \u003C\u002Fth\u003E\n            \u003Cth\u003E\n              CR560457_1\n            \u003C\u002Fth\u003E\n            \u003Cth\u003E\n              CR560502_1\n            \u003C\u002Fth\u003E\n            \u003Cth\u003E\n              CR560562_1\n            \u003C\u002Fth\u003E\n            \u003Cth\u003E...\u003C\u002Fth\u003E\n          \u003C\u002Ftr\u003E\n        \u003C\u002Fthead\u003E\n        \u003Ctbody\u003E\n          \u003Ctr\u003E\n            \u003Ctd\u003E\n              NM_000014\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              34\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              13\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              35\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              34\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\u003C\u002Ftd\u003E\n          \u003C\u002Ftr\u003E\n          \u003Ctr\u003E\n            \u003Ctd\u003E\n              NM_000015\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              2\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              1\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              1\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              1\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\u003C\u002Ftd\u003E\n          \u003C\u002Ftr\u003E\n          \u003Ctr\u003E\n            \u003Ctd\u003E\n              NM_000016\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              0\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              0\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              0\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              0\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\u003C\u002Ftd\u003E\n          \u003C\u002Ftr\u003E\n          \u003Ctr\u003E\n            \u003Ctd\u003E\n              NM_000017\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              27\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              11\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              9\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              3\n            \u003C\u002Ftd\u003E\u003Ctd\u003E\u003C\u002Ftd\u003E\n          \u003C\u002Ftr\u003E\n\n          \u003Ctr\u003E\u003Ctd\u003ENM_000018\u003C\u002Ftd\u003E\u003Ctd\u003E0\u003C\u002Ftd\u003E\u003Ctd\u003E0\u003C\u002Ftd\u003E\u003Ctd\u003E0\u003C\u002Ftd\u003E\u003Ctd\u003E0\u003C\u002Ftd\u003E\u003Ctd\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\n          \u003Ctr\u003E\u003Ctd\u003ENM_000019\u003C\u002Ftd\u003E\u003Ctd\u003E18\u003C\u002Ftd\u003E\u003Ctd\u003E48\u003C\u002Ftd\u003E\u003Ctd\u003E17\u003C\u002Ftd\u003E\u003Ctd\u003E14\u003C\u002Ftd\u003E\u003Ctd\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\n\u003Ctr\u003E\u003Ctd\u003ENM_000020\u003C\u002Ftd\u003E\u003Ctd\u003E0\u003C\u002Ftd\u003E\u003Ctd\u003E0\u003C\u002Ftd\u003E\u003Ctd\u003E0\u003C\u002Ftd\u003E\u003Ctd\u003E0\u003C\u002Ftd\u003E\u003Ctd\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\n\u003Ctr\u003E\u003Ctd\u003ENM_000021\u003C\u002Ftd\u003E\u003Ctd\u003E0\u003C\u002Ftd\u003E\u003Ctd\u003E0\u003C\u002Ftd\u003E\u003Ctd\u003E0\u003C\u002Ftd\u003E\u003Ctd\u003E0\u003C\u002Ftd\u003E\u003Ctd\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\n\n          \u003Ctr\u003E\n            \u003Ctd\u003E&#x22ee;\u003C\u002Ftd\u003E\u003Ctd\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E&#x22f1;\u003C\u002Ftd\u003E\n          \u003C\u002Ftr\u003E\n        \u003C\u002Ftbody\u003E\n      \u003C\u002Ftable\u003E\n  \u003C\u002Fdiv\u003E\n  \u003Cdiv class=\"col-md-6\"\u003E\n    \u003Ch5\u003E Normalized counts from Cufflinks \u003C\u002Fh5\u003E\n    \u003Ctable class='table table-striped table-bordered'\u003E\n      \u003Ctbody\u003E\n          \u003Ctr style=\"font-weight: bold;\"\u003E\n            \u003Ctd\u003E\n              SAMPLE\n            \u003C\u002Ftd\u003E\n            \u003Ctd colspan=2\u003E\n              CR560274_1\n            \u003C\u002Ftd\u003E\n            \u003Ctd colspan=2\u003E\n              CR560457_1\n            \u003C\u002Ftd\u003E\n            \u003Ctd colspan=2\u003E\n              CR560502_1\n            \u003C\u002Ftd\u003E\n          \u003Ctd\u003E...\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\n          \u003Ctr style=\"font-weight: bold;\"\u003E\n            \u003Ctd\u003E\n              TRACKING_ID\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              FPKM\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              FPKM_Status\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              FPKM\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              FPKM_Status\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              FPKM\n            \u003C\u002Ftd\u003E\n            \u003Ctd class=\"td10\"\u003E\n              FPKM_Status\n            \u003C\u002Ftd\u003E\n          \u003Ctd\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\n          \u003Ctr\u003E\n            \u003Ctd\u003E\n              NM_000014\u003Cbr\u003E|chr12:9220303-9268558|\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              0.187039\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              OK\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              0\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              OK\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              0.134608\n            \u003C\u002Ftd\u003E\n            \u003Ctd class=\"td10\"\u003E\n              OK\n            \u003C\u002Ftd\u003E\n          \u003Ctd\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\n          \u003Ctr\u003E\n            \u003Ctd\u003E\n              NM_000015\u003Cbr\u003E|chr8:18248754-18258723|\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              0.218917\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              OK\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              0.152739\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              OK\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              0.13776\n            \u003C\u002Ftd\u003E\n            \u003Ctd class=\"td10\"\u003E\n              OK\n            \u003C\u002Ftd\u003E\n          \u003Ctd\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\n          \u003Ctr\u003E\n            \u003Ctd\u003E\n              NM_000016\u003Cbr\u003E|chr1:76190042-76229355|\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              2.02618\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              OK\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              8.25528\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              OK\n            \u003C\u002Ftd\u003E\n            \u003Ctd\u003E\n              1.7346\n            \u003C\u002Ftd\u003E\n            \u003Ctd class=\"td10\"\u003E\n              OK\n            \u003C\u002Ftd\u003E\n          \u003Ctd\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\n          \u003Ctr\u003E\u003Ctd\u003E\n            NM_000017\u003Cbr\u003E|chr12:121163570-121177811|\u003C\u002Ftd\u003E\u003Ctd\u003E1.40608\u003C\u002Ftd\u003E\u003Ctd\u003EOK\u003C\u002Ftd\u003E\u003Ctd\u003E0.980779\u003C\u002Ftd\u003E\u003Ctd\u003EOK\u003C\u002Ftd\u003E\u003Ctd\u003E0.708088\u003C\u002Ftd\u003E\u003Ctd\u003EOK\n            \u003C\u002Ftd\u003E\u003Ctd\u003E\u003C\u002Ftd\u003E\u003C\u002Ftr\u003E\n              \u003Ctr\u003E\n              \u003Ctd\u003E⋮\u003C\u002Ftd\u003E\u003Ctd\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E\u003C\u002Ftd\u003E\u003Ctd\u003E⋱\u003C\u002Ftd\u003E\n            \u003C\u002Ftr\u003E\n            \u003C\u002Ftbody\u003E\n          \u003C\u002Ftable\u003E\n\n  \u003C\u002Fdiv\u003E\u003C!-- col-md-4 --\u003E\n\u003C\u002Fdiv\u003E\u003C!-- row --\u003E\n\n\u003Cdiv class=\"container-fluid\"\u003E\u003Cdiv class=\"row\"\u003E\u003Cdiv class=\"col-xs-4\"\u003E\n  \u003Ch2\u003E Reproducible research \u003C\u002Fh2\u003E\n\n  Here's an example of the metadata automatically collected during a YAP run. By\n  storing the commands and parameters used to run the job, YAP allows\n  scientists to reproduce their analysis results at later points.\n\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"col-xs-8\" style=\"max-height: 600px; overflow-y: scroll;\"\u003E\n  \u003Ccode\u003E\u003Cpre style=\"background-color: #111; color: #aaa;\"\u003E\n------------------------------ YAP ANALYSIS SUMMARY FOR WORKFLOW = yap2.3_test ------------------------------\n\u003Cspan style=\"color:white;\"\u003EOperating System Information= Linux yourhostname 2.6.18-371.9.1.el5#1 SMP Tue May 13 06:52:49 EDT 2014x86_64\nUSER= my_user\nYAP SOURCE= \u002FYAP\u002Fopensource\u002F\nPython Source= 2.7.5 (default, Sep 10 2013, 17:21:36)  [GCC 4.1.2 20080704 (Red Hat 4.1.2-54)]\nAnalysis Start Time For Workflow : yap2.3_test 2014\u002F09\u002F04 15:55:44\nYAP analysis general metadata:\n1.comment:120 chars\n2.analyst_name:120 chars\n3.organisation_name:NIBR\nInstrument Type= Illumina\nSpecimen Information= [tissue type]\nWorkflow type= rnaseq\nNumber of input files= 2\nNumber of processors= 6\nInput files path for the workflow= \u002Fexamples\u002Fsample_input\nInput file provided:\n1.RN0000108D_1 =\u003E \u002Fexamples\u002Fsample_input\u002FRN0000108D_1.fq\n\t\t  \u002Fexamples\u002Fsample_input\u002FRN0000108D_2.fq\n\nOutput file path for the workflow= \u002Ftest_output\u002Fyap2.3_test\nSequence data type= paired end\nInput file format= fastq\nMaximum read length= 150\nFile chunk size (in megabytes)= 1024\nData distribution method=chunk_based\nOutput file path= \u002Ftest_output\u002F\n-------------------------------------------------------------------------------------------------------------\nAnalysis stages :\nPreprocess analysis= yes\nReference Sequence Alignment=yes\nPostprocess Analysis= yes\n-------------------------------------------------------------------------------------------------------------\nPreprocess Analysis commands:\nBarcodes information:\nno_barcode_specified :\n1. command name= fastq_screen,command line= \u002Fpackages\u002FFastQScreen\u002Fv0.4.1\u002Ffastq_screen --subset 500000 --paired --outdir output_directory --conf fastq_screen_v0.4.1.conf --aligner bowtie\n2. command name= fastqc,command line= \u002Fpackages\u002Ffastqc\u002F0.10.1\u002Ffastqc --outdir output_directory --extract --threads 12\n-------------------------------------------------------------------------------------------------------------\nAligner commands:\n1. command name= bowtie,command line= \u002Fpackages\u002Fbowtie\u002F1.0.0\u002Fbowtie  \u002Faccessory_files\u002Findexes\u002Fbowtie\u002Fhg19 -q -v 2 -k 10 -m 10 --best -S -p 8 -1 pipe1 -2 pipe2  \u003Eoutput_file.sam\nAlignment output data sort order= both\n-------------------------------------------------------------------------------------------------------------\nSamples re-grouped in this workflow:\nNone.\n-------------------------------------------------------------------------------------------------------------\nPotprocess analysis commands:\n1. command type= :begin\n\tcommand input : ['input_file_type *junctions.bed*', 'input_directory aligner_output']\n\t1. command name= yap_junction_count,command line= yap_junction_count -exon_coordinates_file \u002Faccessory_files\u002Fhuman-ucsc-final_exon_coord.bed -exon_CoordToNumber_file \u002Faccessory_files\u002Fhuman-ucsc-final_exon_coord_number.bed -i - -o output_file\n2. command type= :begin\n\tcommand input : ['input_file_type *queryname*.sam', 'input_directory aligner_output']\n\t1. command name= htseq-count,command line= \u002Fpackages\u002Fpython\u002F2.6.5_gnu\u002Fbin\u002Fhtseq-count -s no -q  file_based_input  \u002Faccessory_files\u002Fhuman-ucsc-refGene.gtf  \u003Eoutput_file.out\n3. command type= :begin_tee\n\tcommand input : ['input_directory aligner_output', 'input_file_type *coordinate*']\n\t1. command name= yap_exon_count,command line= yap_exon_count -f 1.0 -exon_coordinates_file \u002Faccessory_files\u002Fhuman-ucsc-final_exon_coord.bed -exon_CoordToNumber_file \u002Faccessory_files\u002Fhuman-ucsc-final_exon_coord_number.bed -i - -o output_file\n\t2. command name= CollectAlignmentSummaryMetrics,command line= java -Xmx1g -jar \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FCollectAlignmentSummaryMetrics.jar VALIDATION_STRINGENCY= SILENT I= \u002Fdev\u002Fstdin O= output_file.txt IS_BISULFITE_SEQUENCED= true ASSUME_SORTED= True REFERENCE_SEQUENCE= \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Ffasta\u002Fhg19.fa QUIET= True\n\t3. command name= QualityScoreDistribution,command line= java -Xmx1g -jar \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FQualityScoreDistribution.jar VALIDATION_STRINGENCY= SILENT I= \u002Fdev\u002Fstdin O= output_file.txt ASSUME_SORTED= true REFERENCE_SEQUENCE= \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Ffasta\u002Fhg19.fa CHART= output_file.pdf ALIGNED_READS_ONLY= true\n\t4. command name= MeanQualityByCycle,command line= java -Xmx1g -jar \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FMeanQualityByCycle.jar VALIDATION_STRINGENCY= SILENT ASSUME_SORTED= true REFERENCE_SEQUENCE= \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Ffasta\u002Fhg19.fa I= \u002Fdev\u002Fstdin O= output_file.txt CHART= output_file.pdf ALIGNED_READS_ONLY= true\n\t5. command name= CollectGcBiasMetrics,command line= java -Xmx1g -jar \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FCollectGcBiasMetrics.jar VALIDATION_STRINGENCY= SILENT I= \u002Fdev\u002Fstdin O= output_file.txt SUMMARY_OUTPUT= output_file_summary.txt CHART= output_file.pdf ASSUME_SORTED= true REFERENCE_SEQUENCE= \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Ffasta\u002Fhg19.fa\n\t6. command name= CollectRnaSeqMetrics,command line= java -Xmx1g -jar  \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FCollectRnaSeqMetrics.jar VALIDATION_STRINGENCY= SILENT ASSUME_SORTED= true REF_FLAT= \u002Fdb\u002Fyap\u002Fucsc\u002Fmay_02_2013\u002Fgtf\u002Fhg19\u002Fhuman_refflat_for_picard.gff RIBOSOMAL_INTERVALS= \u002Fdb\u002Fyap\u002Fucsc\u002Fmay_02_2013\u002Fgtf\u002Fhg19\u002FHomo_sapiens_assembly19.rRNA.interval_list STRAND_SPECIFICITY= NONE I= \u002Fdev\u002Fstdin O= output_file.txt CHART_OUTPUT= output_file.pdf REFERENCE_SEQUENCE= \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Ffasta\u002Fhg19.fa\n\t7. command name= CalculateHsMetrics,command line= \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FCalculateHsMetrics.jar VALIDATION_STRINGENCY= SILENT BAIT_INTERVALS= \u002Faccessory_files\u002FTruSeq_exome_targeted_regions_for_picard.bed TARGET_INTERVALS= \u002Faccessory_files\u002FTruSeq_exome_targeted_regions_for_picard.bed INPUT= \u002Fdev\u002Fstdin OUTPUT= output_file.txt REFERENCE_SEQUENCE= \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Ffasta\u002Fhg19.fa\n\t8. command name= CollectTargetedPcrMetrics,command line= java -Xmx1g -jar \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FCollectTargetedPcrMetrics.jar VALIDATION_STRINGENCY= SILENT AMPLICON_INTERVALS= \u002Faccessory_files\u002FTruSeq_exome_targeted_regions_for_picard.bed TARGET_INTERVALS= \u002Faccessory_files\u002FTruSeq_exome_targeted_regions_for_picard.bed INPUT= \u002Fdev\u002Fstdin OUTPUT= output_file.txt REFERENCE_SEQUENCE= \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Ffasta\u002Fhg19.fa\n4. command type= :begin\n\tcommand input : ['input_file_type *coordinate*', 'input_directory aligner_output']\n\t1. command name= CollectInsertSizeMetrics,command line= java -Xmx1g -jar  \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FCollectInsertSizeMetrics.jar VALIDATION_STRINGENCY= SILENT ASSUME_SORTED= true I= file_based_input O= output_file.txt H= output_file.pdf TMP_DIR= \u002Fscratch\u002F$USER HISTOGRAM_WIDTH= 500\n5. command type= :begin\n\tcommand input : ['input_file_type *coordinate*', 'input_directory aligner_output']\n\t1. command name= MarkDuplicates,command line= java -Xmx1g -jar  \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FMarkDuplicates.jar VALIDATION_STRINGENCY= SILENT TMP_DIR= \u002Fscratch\u002F$USER MAX_RECORDS_IN_RAM= 1000000 MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP= 2000 ASSUME_SORTED= true I= file_based_input O= output_file.bam METRICS_FILE= output_file.txt\n6. command type= :begin\n\tcommand input : ['input_file_type *coordinate*', 'input_directory aligner_output']\n\t1. command name= cufflinks,command line= \u002Fpackages\u002Fcufflinks\u002F2.1.1\u002Fcufflinks  file_based_input -o output_directory -p 12 -G \u002Faccessory_files\u002Fhuman-ucsc-refGene.gtf\n-------------------------------------------------------------------------------------------------------------\n\n******************* YAP CHECK SUMMARY *******************\n* --Syntax check          : Passed                      *\n* --Compatibility check   : Passed                      *\n* --File paths check      : Passed With Warnings        *\n*********************************************************\n* YAP Configuration overall check status: Passed With Warnings\u003C\u002Fspan\u003E\n\n--------------------------------------- YAP Check Error\u002FWarning Info ---------------------------------------\n-------------------------------------------------------------------------------------------------------------\n--YAP Configuration File paths check status: Passed With Warnings\nWarning: At Line: 21 in file: bowtie_1.0.0_configuration.cfg. Files were found using basename in \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Findexes\u002Fbowtie\u002Fhg19. Please make sure that command: bowtie can work with basenames.\n\n\n-------------------------------------------------------------------------------------------------------------\n-------------------------- YAP configurations check end for Workflow = yap2.3_test --------------------------\n-------------------- PROVENANCE --------------------\n\nPREPROCESS:\n\n\t\u002Fpackages\u002FFastQScreen\u002Fv0.4.1\u002Ffastq_screen --subset 500000 --paired --outdir \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpreprocess_output --conf fastq_screen_v0.4.1.conf --aligner bowtie \u002Fdb\u002Fyap\u002Fbenchmark\u002Frobin_50_samples\u002FRN0000108D_1.fq \u002Fdb\u002Fyap\u002Fbenchmark\u002Frobin_50_samples\u002FRN0000108D_2.fq\n\n\t\u002Fpackages\u002Ffastqc\u002F0.10.1\u002Ffastqc --outdir \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpreprocess_output --extract --threads 12 \u002Fdb\u002Fyap\u002Fbenchmark\u002Frobin_50_samples\u002FRN0000108D_1.fq \u002Fdb\u002Fyap\u002Fbenchmark\u002Frobin_50_samples\u002FRN0000108D_2.fq\n\nALIGNMENT :\n\n\tINPUT: \u002Fdb\u002Fyap\u002Fbenchmark\u002Frobin_50_samples\u002FRN0000108D_1.fq and \u002Fdb\u002Fyap\u002Fbenchmark\u002Frobin_50_samples\u002FRN0000108D_2.fq chunk number= 0\n\n\t\u002Fpackages\u002Fbowtie\u002F1.0.0\u002Fbowtie  \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Findexes\u002Fbowtie\u002Fhg19 -q -v 2 -k 10 -m 10 --best -S -p 8 -1  \u002Fscratch\u002Fkulkatr1\u002F\u002Fkulkatr1\u002Fyap_temp\u002Faligner_RN0000108D_1_0000003008281_0.797636572311_pipe_0_0_1  -2  \u002Fscratch\u002Fkulkatr1\u002F\u002Fkulkatr1\u002Fyap_temp\u002Faligner_RN0000108D_1_0000003008281_0.797636572311_pipe_0_0_2   \u003E\u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000000.sam\n\n\tsamtools view -bhS - samtools sort -on -m 100000000 - \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000000_queryname | samtools view -h - -o \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000000_queryname.sam\n\n\tsamtools view -bhS - samtools sort -o -m 100000000 - \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000000_coordinate | samtools view -h - -o \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000000_coordinate.sam\n\n\tINPUT: \u002Fdb\u002Fyap\u002Fbenchmark\u002Frobin_50_samples\u002FRN0000108D_1.fq and \u002Fdb\u002Fyap\u002Fbenchmark\u002Frobin_50_samples\u002FRN0000108D_2.fq chunk number= 1\n\n\t\u002Fpackages\u002Fbowtie\u002F1.0.0\u002Fbowtie  \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Findexes\u002Fbowtie\u002Fhg19 -q -v 2 -k 10 -m 10 --best -S -p 8 -1  \u002Fscratch\u002Fkulkatr1\u002F\u002Fkulkatr1\u002Fyap_temp\u002Faligner_RN0000108D_1_0000013008281_0.797636572311_pipe_1_0_1  -2  \u002Fscratch\u002Fkulkatr1\u002F\u002Fkulkatr1\u002Fyap_temp\u002Faligner_RN0000108D_1_0000013008281_0.797636572311_pipe_1_0_2   \u003E\u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000001.sam\n\n\tsamtools view -bhS - samtools sort -on -m 100000000 - \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000001_queryname | samtools view -h - -o \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000001_queryname.sam\n\n\tsamtools view -bhS - samtools sort -o -m 100000000 - \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000001_coordinate | samtools view -h - -o \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000001_coordinate.sam\n\n\tINPUT: \u002Fdb\u002Fyap\u002Fbenchmark\u002Frobin_50_samples\u002FRN0000108D_1.fq and \u002Fdb\u002Fyap\u002Fbenchmark\u002Frobin_50_samples\u002FRN0000108D_2.fq chunk number= 2\n\n\t\u002Fpackages\u002Fbowtie\u002F1.0.0\u002Fbowtie  \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Findexes\u002Fbowtie\u002Fhg19 -q -v 2 -k 10 -m 10 --best -S -p 8 -1  \u002Fscratch\u002Fkulkatr1\u002F\u002Fkulkatr1\u002Fyap_temp\u002Faligner_RN0000108D_1_0000023008281_0.797636572311_pipe_2_0_1  -2  \u002Fscratch\u002Fkulkatr1\u002F\u002Fkulkatr1\u002Fyap_temp\u002Faligner_RN0000108D_1_0000023008281_0.797636572311_pipe_2_0_2   \u003E\u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000002.sam\n\n\tsamtools view -bhS - samtools sort -on -m 100000000 - \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000002_queryname | samtools view -h - -o \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000002_queryname.sam\n\n\tsamtools view -bhS - samtools sort -o -m 100000000 - \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000002_coordinate | samtools view -h - -o \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000002_coordinate.sam\n\n\tINPUT: \u002Fdb\u002Fyap\u002Fbenchmark\u002Frobin_50_samples\u002FRN0000108D_1.fq and \u002Fdb\u002Fyap\u002Fbenchmark\u002Frobin_50_samples\u002FRN0000108D_2.fq chunk number= 3\n\n\t\u002Fpackages\u002Fbowtie\u002F1.0.0\u002Fbowtie  \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Findexes\u002Fbowtie\u002Fhg19 -q -v 2 -k 10 -m 10 --best -S -p 8 -1  \u002Fscratch\u002Fkulkatr1\u002F\u002Fkulkatr1\u002Fyap_temp\u002Faligner_RN0000108D_1_0000033008281_0.797636572311_pipe_3_0_1  -2  \u002Fscratch\u002Fkulkatr1\u002F\u002Fkulkatr1\u002Fyap_temp\u002Faligner_RN0000108D_1_0000033008281_0.797636572311_pipe_3_0_2   \u003E\u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000003.sam\n\n\tsamtools view -bhS - samtools sort -on -m 100000000 - \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000003_queryname | samtools view -h - -o \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000003_queryname.sam\n\n\tsamtools view -bhS - samtools sort -o -m 100000000 - \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000003_coordinate | samtools view -h - -o \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000003_coordinate.sam\n\n\tINPUT: \u002Fdb\u002Fyap\u002Fbenchmark\u002Frobin_50_samples\u002FRN0000108D_1.fq and \u002Fdb\u002Fyap\u002Fbenchmark\u002Frobin_50_samples\u002FRN0000108D_2.fq chunk number= 4\n\n\t\u002Fpackages\u002Fbowtie\u002F1.0.0\u002Fbowtie  \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Findexes\u002Fbowtie\u002Fhg19 -q -v 2 -k 10 -m 10 --best -S -p 8 -1  \u002Fscratch\u002Fkulkatr1\u002F\u002Fkulkatr1\u002Fyap_temp\u002Faligner_RN0000108D_1_0000043008281_0.797636572311_pipe_4_0_1  -2  \u002Fscratch\u002Fkulkatr1\u002F\u002Fkulkatr1\u002Fyap_temp\u002Faligner_RN0000108D_1_0000043008281_0.797636572311_pipe_4_0_2   \u003E\u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000004.sam\n\n\tsamtools view -bhS - samtools sort -on -m 100000000 - \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000004_queryname | samtools view -h - -o \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000004_queryname.sam\n\n\tsamtools view -bhS - samtools sort -o -m 100000000 - \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000004_coordinate | samtools view -h - -o \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000004_coordinate.sam\n\n\tINPUT: \u002Fdb\u002Fyap\u002Fbenchmark\u002Frobin_50_samples\u002FRN0000108D_1.fq and \u002Fdb\u002Fyap\u002Fbenchmark\u002Frobin_50_samples\u002FRN0000108D_2.fq chunk number= 5\n\n\t\u002Fpackages\u002Fbowtie\u002F1.0.0\u002Fbowtie  \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Findexes\u002Fbowtie\u002Fhg19 -q -v 2 -k 10 -m 10 --best -S -p 8 -1  \u002Fscratch\u002Fkulkatr1\u002F\u002Fkulkatr1\u002Fyap_temp\u002Faligner_RN0000108D_1_0000053008281_0.797636572311_pipe_5_0_1  -2  \u002Fscratch\u002Fkulkatr1\u002F\u002Fkulkatr1\u002Fyap_temp\u002Faligner_RN0000108D_1_0000053008281_0.797636572311_pipe_5_0_2   \u003E\u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000005.sam\n\n\tsamtools view -bhS - samtools sort -on -m 100000000 - \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000005_queryname | samtools view -h - -o \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000005_queryname.sam\n\n\tsamtools view -bhS - samtools sort -o -m 100000000 - \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000005_coordinate | samtools view -h - -o \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000005_coordinate.sam\n\nMERGE ALIGNMENT OUTPUT :\n\n\tsamtools merge -n  -  \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000002_queryname.bam \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000003_queryname.bam \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000005_queryname.bam \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000000_queryname.bam \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000001_queryname.bam \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000004_queryname.bam  | samtools view -h - -o \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002FRN0000108D_1_queryname.sam\n\n\tsamtools merge  -  \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000005_coordinate.bam \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000001_coordinate.bam \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000000_coordinate.bam \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000002_coordinate.bam \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000003_coordinate.bam \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002Faligner_RN0000108D_1_000004_coordinate.bam  | samtools view -h - -o \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002FRN0000108D_1_coordinate.sam\n\nPOSTPROCESS :\n\n\tINPUT: \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002FRN0000108D_1_queryname.sam\n\n\t\u002Fpackages\u002Fpython\u002F2.6.5_gnu\u002Fbin\u002Fhtseq-count -s no -q  \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002FRN0000108D_1_queryname.sam  \u002Faccessory_files\u002Fhuman-ucsc-refGene.gtf  \u003E\u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_htseq-count.out\n\n\tINPUT: \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002FRN0000108D_1_coordinate.sam\n\n\tyap_exon_count -f 1.0 -exon_coordinates_file \u002Faccessory_files\u002Fhuman-ucsc-final_exon_coord.bed -exon_CoordToNumber_file \u002Faccessory_files\u002Fhuman-ucsc-final_exon_coord_number.bed -i - -o \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_yap_exon_count\n\n\tjava -Xmx1g -jar \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FCollectAlignmentSummaryMetrics.jar VALIDATION_STRINGENCY= SILENT I= \u002Fdev\u002Fstdin O= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_CollectAlignmentSummaryMetrics.txt IS_BISULFITE_SEQUENCED= true ASSUME_SORTED= True REFERENCE_SEQUENCE= \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Ffasta\u002Fhg19.fa QUIET= True\n\n\tjava -Xmx1g -jar \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FQualityScoreDistribution.jar VALIDATION_STRINGENCY= SILENT I= \u002Fdev\u002Fstdin O= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_QualityScoreDistribution.txt ASSUME_SORTED= true REFERENCE_SEQUENCE= \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Ffasta\u002Fhg19.fa CHART= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_QualityScoreDistribution.pdf ALIGNED_READS_ONLY= true\n\n\tjava -Xmx1g -jar \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FMeanQualityByCycle.jar VALIDATION_STRINGENCY= SILENT ASSUME_SORTED= true REFERENCE_SEQUENCE= \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Ffasta\u002Fhg19.fa I= \u002Fdev\u002Fstdin O= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_MeanQualityByCycle.txt CHART= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_MeanQualityByCycle.pdf ALIGNED_READS_ONLY= true\n\n\tjava -Xmx1g -jar \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FCollectGcBiasMetrics.jar VALIDATION_STRINGENCY= SILENT I= \u002Fdev\u002Fstdin O= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_CollectGcBiasMetrics.txt SUMMARY_OUTPUT= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_CollectGcBiasMetrics_summary.txt CHART= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_CollectGcBiasMetrics.pdf ASSUME_SORTED= true REFERENCE_SEQUENCE= \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Ffasta\u002Fhg19.fa\n\n\tjava -Xmx1g -jar  \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FCollectRnaSeqMetrics.jar VALIDATION_STRINGENCY= SILENT ASSUME_SORTED= true REF_FLAT= \u002Fdb\u002Fyap\u002Fucsc\u002Fmay_02_2013\u002Fgtf\u002Fhg19\u002Fhuman_refflat_for_picard.gff RIBOSOMAL_INTERVALS= \u002Fdb\u002Fyap\u002Fucsc\u002Fmay_02_2013\u002Fgtf\u002Fhg19\u002FHomo_sapiens_assembly19.rRNA.interval_list STRAND_SPECIFICITY= NONE I= \u002Fdev\u002Fstdin O= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_CollectRnaSeqMetrics.txt CHART_OUTPUT= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_CollectRnaSeqMetrics.pdf REFERENCE_SEQUENCE= \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Ffasta\u002Fhg19.fa\n\n\t\u002Fpackages\u002Fpicard-tools\u002F1.89\u002FCalculateHsMetrics.jar VALIDATION_STRINGENCY= SILENT BAIT_INTERVALS= \u002Faccessory_files\u002FTruSeq_exome_targeted_regions_for_picard.bed TARGET_INTERVALS= \u002Faccessory_files\u002FTruSeq_exome_targeted_regions_for_picard.bed INPUT= \u002Fdev\u002Fstdin OUTPUT= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_CalculateHsMetrics.txt REFERENCE_SEQUENCE= \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Ffasta\u002Fhg19.fa\n\n\tjava -Xmx1g -jar \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FCollectTargetedPcrMetrics.jar VALIDATION_STRINGENCY= SILENT AMPLICON_INTERVALS= \u002Faccessory_files\u002FTruSeq_exome_targeted_regions_for_picard.bed TARGET_INTERVALS= \u002Faccessory_files\u002FTruSeq_exome_targeted_regions_for_picard.bed INPUT= \u002Fdev\u002Fstdin OUTPUT= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_CollectTargetedPcrMetrics.txt REFERENCE_SEQUENCE= \u002Fdb\u002Fnibrgenome\u002FNG00006.0\u002Ffasta\u002Fhg19.fa\n\n\tINPUT: \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002FRN0000108D_1_coordinate.sam\n\n\tjava -Xmx1g -jar  \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FCollectInsertSizeMetrics.jar VALIDATION_STRINGENCY= SILENT ASSUME_SORTED= true I= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002FRN0000108D_1_coordinate.sam O= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_CollectInsertSizeMetrics.txt H= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_CollectInsertSizeMetrics.pdf TMP_DIR= \u002Fscratch\u002F$USER HISTOGRAM_WIDTH= 500\n\n\tINPUT: \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002FRN0000108D_1_coordinate.sam\n\n\tjava -Xmx1g -jar  \u002Fpackages\u002Fpicard-tools\u002F1.89\u002FMarkDuplicates.jar VALIDATION_STRINGENCY= SILENT TMP_DIR= \u002Fscratch\u002F$USER MAX_RECORDS_IN_RAM= 1000000 MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP= 2000 ASSUME_SORTED= true I= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002FRN0000108D_1_coordinate.sam O= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_MarkDuplicates.bam METRICS_FILE= \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output\u002FRN0000108D_1_MarkDuplicates.txt\n\n\tINPUT: \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002FRN0000108D_1_coordinate.sam\n\n\t\u002Fpackages\u002Fcufflinks\u002F2.1.1\u002Fcufflinks  \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Faligner_output\u002FRN0000108D_1_coordinate.sam -o \u002Ftest_output\u002Fyap2.3_test\u002FRN0000108D_1\u002Fno_barcode_specified\u002Fpostprocess_output -p 12 -G \u002Faccessory_files\u002Fhuman-ucsc-refGene.gtf\n\n--------------------Analysis End Time For Workflow : yap2.3_test 2014\u002F09\u002F04 19:43:14--------------------\u003C\u002Fpre\u003E\n      \u003C\u002Fcode\u003E\n    \u003C\u002Fdiv\u003E\n  \u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Ch2\u003E\n\u003Ca name=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003E\u003Cspan class=\"octicon octicon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003ERequirements\u003C\u002Fh2\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EYAP only runs on Linux systems!\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\n\u003Ch3\u003EDependencies: \u003C\u002Fh3\u003E\n\n\u003Cp\u003EThe following dependencies have to be first installed in your environment. Once installed, make sure these dependencies are added to your path.\u003C\u002Fp\u003E\n\n\u003Cul class=\"task-list\"\u003E\n\u003Cli\u003ERecent versions of gcc (gcc 4.8.x is well tested)\u003C\u002Fli\u003E\n\u003Cli\u003EPython 2.7.7\u003C\u002Fli\u003E\n\u003Cli\u003EOpenmpi 1.6.5\u003C\u002Fli\u003E\n\u003Cli\u003EPython modules:\n\n\u003Cul class=\"task-list\"\u003E\n\u003Cli\u003EMPI4py - 1.3\u003C\u002Fli\u003E\n\u003Cli\u003EPyPdf - 1.13\u003C\u002Fli\u003E\n\u003Cli\u003ENumpy - 1.7.1\u003C\u002Fli\u003E\n\u003Cli\u003Enetsa-utils - 1.4.3\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003Ebedtools - 2.15.0\u003C\u002Fli\u003E\n\u003Cli\u003Esamtools - 0.1.18\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\n\u003Ch3\u003ESystem Configuration:\u003C\u002Fh3\u003E\n\n\u003Cp\u003E   YAP provides a framework to run external tools and data, so the tools used\nin the workflows drive the system requirements. It can be installed on multicore\nlinux workstation with a decent amount of memory for small data, or on large\ncluster systems to scale optimally for large data processing. The framework has\nbeen tested extensively for \u003Cabbr title=\"Next-Generation Sequencing\"\u003ENGS\u003C\u002Fabbr\u003E\ndata on clusters with minimum system configuration of 8-12 cores and 24-48 GB\nmemory.   \u003C\u002Fp\u003E\n\n\u003Ch2\u003EYAP Setup\u003C\u002Fh2\u003E\n\n\u003Cul class=\"task-list\"\u003E\n\u003Cli\u003EDownload the yap source from \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FNovartis\u002Fyap\u002Farchive\u002Fmaster.zip\"\u003Ehere\u003C\u002Fa\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003EUncompress the source directory \u003C\u002Fp\u003E\n\n\u003Cp\u003Efor example: uncompress the directory as \u003Ccode\u003E\u002Fhome\u002Fpackages\u002FYAP\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003ESet \u003Ccode\u003EYAP_HOME\u003C\u002Fcode\u003E environment variable to the source directory.\u003C\u002Fp\u003E\n\n\u003Cp\u003E\u003Cpre\u003E\u003Ccode\u003E$ export YAP_HOME=\u002Fhome\u002Fpackages\u002FYAP\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003EAdd bin directory to path\u003C\u002Fp\u003E\n\n\u003Cpre\u003E\u003Ccode\u003E$ export PATH=$PATH:$YAP_HOME\u002Fbin\u002F\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\n\u003Cp\u003ESet \u003Ccode\u003EYAP_LOCAL_TEMPDIR\u003C\u002Fcode\u003E\nenvironment variable for temporary computation. For optimum performance point\nthis directory to a location which is  local to the machine. \u003C\u002Fp\u003E\n\n\u003Cpre\u003E\u003Ccode\u003E$ export YAP_LOCAL_TEMPDIR=\u002Fscratch\u002Fusername\u002Fyap_temp\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\u003Cp\u003E\u003Cstrong\u003EVerification\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\n\u003Cpre\u003E\u003Ccode\u003E$ echo $YAP_HOME\n    \u002Fhome\u002Fpackages\u002FYAP\n\n$ echo $YAP_LOCAL_TEMPDIR\n    \u002Fscratch\u002Fusername\u002Fyap_temp\n\n$ which yap\n    \u002Fhome\u002Fpackages\u002FYAP\u002Fbin\u002Fyap\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\n\u003Ch1\u003E \u003Ca name=\"user-content-running-a-yap-job\" class=\"anchor\"\nhref=\"#running-a-yap-job\" aria-hidden=\"true\"\u003E\u003Cspan class=\"octicon\nocticon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003ERunning a YAP job\u003C\u002Fh1\u003E\n\n\u003Cp\u003EOnce you've set your environment, it is best to run a quick demo job to get\nthe feel of running YAP. The following section is meant to be interactive and\nhence you would need Linux account access and access to the cluster.\u003C\u002Fp\u003E\n\n\u003Cp\u003EAfter downloading the project, please see the demo configuration files in\n\u003Ccode\u003Eyap\u002Fcfg\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\n\n\u003Cp\u003EThere are 3 stages in YAP - Preprocess, Alignment and Postprocess. You can\nhave command level control of these three stages in the namesake configuration\nfiles and a workflow level control in the\n\u003Ccode\u003Eworkflow_configuration.\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\n\u003Ctable class=\"table table-striped\"\u003E\n\u003Cthead\u003E\u003Ctr\u003E\n\u003Cth\u003EConfiguration\u003C\u002Fth\u003E\n\u003Cth\u003EPurpose\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Ealigner_configuration\u003C\u002Ftd\u003E\n\u003Ctd\u003Ebwa, bowtie, bowtie2, tophat or insert your own aligner\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Epostprocess_configuration\u003C\u002Ftd\u003E\n\u003Ctd\u003Epostalignment packages, generate counts or metrics\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Epreprocess_configuration\u003C\u002Ftd\u003E\n\u003Ctd\u003Epre-alignment packages to massage your seqdata\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Eworkflow_configuration\u003C\u002Ftd\u003E\n\u003Ctd\u003Emanage metadata, specify input files, paths and output directories\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Eyap_sge\u003C\u002Ftd\u003E\n\u003Ctd\u003Esubmitting your job to the cluster\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\n\u003C\u002Ftable\u003E\u003Cp\u003EThe demo runs a RNASeq QC and counts workflow consisting of:\u003C\u002Fp\u003E\n\n\u003Cul class=\"task-list\"\u003E\n\u003Cli\u003EPreprocess: FastQC, Fastqscreen\u003C\u002Fli\u003E\n\u003Cli\u003EAlignment: Bowtie, both queryname and coordinate sorted\u003C\u002Fli\u003E\n\u003Cli\u003EPostprocess: yap junction and exon counts, Picard tools (PostQC), HTSeq (Raw counts) and Cufflinks (normalized counts)\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\u003Cp\u003EWe run this workflow on 2 nodes on the UGE cluster.\u003C\u002Fp\u003E\n\n\u003Cp\u003ETo run the yap_demo job, we next need to check to see if our configuration files are correct using the command.\u003C\u002Fp\u003E\n\n\u003Cpre\u003E\u003Ccode\u003Ecd &lt;your_working directory&gt;\nyap --check workflow_configuration.cfg\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\n\u003Cp\u003EThe \u003Ccode\u003Eyap --check\u003C\u002Fcode\u003E  command checks to see\u003C\u002Fp\u003E\n\n\u003Cul class=\"task-list\"\u003E\n\u003Cli\u003EIf all paths specified are valid\u003C\u002Fli\u003E\n\u003Cli\u003EIf YAP finds the appropriate input files\u003C\u002Fli\u003E\n\u003Cli\u003EChecks for syntax errors\u003C\u002Fli\u003E\n\u003Cli\u003ELists commands to be executed.\u003C\u002Fli\u003E\n\u003Cli\u003EGives section-wise error\u002Fwarning report.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\n\u003Ch2\u003ERunning the YAP job\u003C\u002Fh2\u003E\n\n\u003Cpre\u003E\u003Ccode\u003Empirun -n &lt;number_of_cores&gt; yap workflow_configuration.cfg\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\n\u003Cp\u003EIf you have a SGE environment, pass the number of slots into the \u003Ccode\u003E$NSLOTS\u003C\u002Fcode\u003E variable.\u003C\u002Fp\u003E\n\u003Cscript\u003E\n$('abbr').tooltip();\n\u003C\u002Fscript\u003E\n"},{slug:q,metadata:{name:h,abbreviation:h,projectName:q,launched:"2014-02-21T00:00:00.000Z",icon:"\u002Fprojects\u002Fgridvar\u002Fgridvar-gallery-01.png",summary:"GridVar is a jQuery plugin that visualizes multi-dimensional datasets as layers organized in a row-column format.\n",description:"\u003Cp\u003EGridVar is a jQuery plugin that visualizes multi-dimensional datasets as layers organized in a row-column format. At each cell (i.e., rectangle at the intersection of a row and column), GridVar displays your data as a background color (like a color\u002Fheat map) and\u002For a glyph (shape). This enables different characteristics of your dataset to be layered on top of each other. For more information on usage, required libraries, and other developer information, please see \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FNovartis\u002Fgridvar\"\u003Eour documentation on GitHub\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n",scripts:["https:\u002F\u002Fcode.jquery.com\u002Fjquery-1.10.1.min.js","https:\u002F\u002Fcdn.jsdelivr.net\u002Fqtip2\u002F2.2.0\u002Fjquery.qtip.min.js","https:\u002F\u002Fcode.jquery.com\u002Fui\u002F1.10.3\u002Fjquery-ui.js","https:\u002F\u002Fcdnjs.cloudflare.com\u002Fajax\u002Flibs\u002Funderscore.js\u002F1.5.2\u002Funderscore-min.js","https:\u002F\u002Fcdnjs.cloudflare.com\u002Fajax\u002Flibs\u002Fd3\u002F3.3.11\u002Fd3.min.js","\u002Fprojects\u002Fgridvar\u002Fjquery.nibrGridVar.min.js","\u002Fprojects\u002Fgridvar\u002Fgridvar-example.js"],styles:["https:\u002F\u002Fcdn.jsdelivr.net\u002Fqtip2\u002F2.2.0\u002Fjquery.qtip.min.css","\u002Fprojects\u002Fgridvar\u002Fgridvar.css"],onMount:"$(document).ready(function() {\n  onMainReady();\n});\n",onDestroy:"$('.qtip').remove();\n",dateString:"Fri Feb 21 2014",title:h},html:"\n\u003Ch3\u003E\u003Ca id=\"genomicsExample\"\u003E\u003C\u002Fa\u003EGenomics Example\u003C\u002Fh3\u003E\n\n\u003Cp\u003E\n  Our inspiration for developing GridVar was a figure from\n  \u003Ca\n    target=\"_blank\"\n    href=\"http:\u002F\u002Fwww.nature.com\u002Fnature\u002Fjournal\u002Fv486\u002Fn7403\u002Ffull\u002Fnature11154.html\"\n    \u003ESequence analysis of mutations and translocations across breast cancer\n    subtypes\u003C\u002Fa\n  \u003E\n  published in \u003Ci\u003ENature\u003C\u002Fi\u003E. Users can interact with the visualization by\n  hovering over a cell to view a popup with more details, clicking on labels,\n  and by reordering the rows and columns by selecting actions in the dropdowns.\n\u003C\u002Fp\u003E\n\n\u003Cdiv id=\"tissueSampleOrder\" class=\"btn-group\"\u003E\n  \u003Clabel\u003EOrder Tissue Samples (Columns) By...\u003C\u002Flabel\u003E\n  \u003Cul class=\"dropdown-menu\" role=\"menu\"\u003E\n    \u003Cli\u003E\n      \u003Ca role=\"button\" class=\"nibr-pointer\" id=\"tissueTopSort\"\u003E\n        Top gene set mutation frequency\n      \u003C\u002Fa\u003E\n    \u003C\u002Fli\u003E\n    \u003Cli\u003E\n      \u003Ca role=\"button\" class=\"nibr-pointer\" id=\"tissueGlobalSort\"\u003E\n        Global gene set mutation frequency\n      \u003C\u002Fa\u003E\n    \u003C\u002Fli\u003E\n  \u003C\u002Ful\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cdiv class=\"btn-group\"\u003E\n  \u003Clabel\u003EOrder Genes (Rows) By...\u003C\u002Flabel\u003E\n  \u003Cul class=\"dropdown-menu\" role=\"menu\"\u003E\n    \u003Cli\u003E\n      \u003Ca role=\"button\" class=\"nibr-pointer\" id=\"geneMissenseMutationSort\"\u003E\n        Missense mutations only\n      \u003C\u002Fa\u003E\n    \u003C\u002Fli\u003E\n    \u003Cli\u003E\n      \u003Ca role=\"button\" class=\"nibr-pointer\" id=\"geneNonSynonymousMutationSort\"\u003E\n        Other Non-synonymous mutations only\n      \u003C\u002Fa\u003E\n    \u003C\u002Fli\u003E\n    \u003Cli\u003E\n      \u003Ca role=\"button\" class=\"nibr-pointer\" id=\"geneAllMutationSort\"\u003E\n        All mutations\n      \u003C\u002Fa\u003E\n    \u003C\u002Fli\u003E\n  \u003C\u002Ful\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cdiv id=\"gridVarGenomics\"\u003E\u003C\u002Fdiv\u003E\n\n\u003Cp\u003E\n  We've taken data (\u003Ca\n    target=\"_blank\"\n    href=\"http:\u002F\u002Fwww.nature.com\u002Fnature\u002Fjournal\u002Fv486\u002Fn7403\u002Fextref\u002Fnature11154-s2.xls\"\n    \u003ESupplementary Table 4\u003C\u002Fa\n  \u003E\n  from the aforementioned Nature paper) to create the above example. Tissue\n  samples (columns) are initially ordered by total mutations per sample in\n  descending order across the entire set of genes. Similarly, genes (rows) are\n  ordered by the number of mutations across samples in descending order.\n\u003C\u002Fp\u003E\n\n\u003Cp\u003E\n  Alternate orderings for both tissue samples and genes enable visual\n  exploration of the view. By restricting the tissue samples mutation counts to\n  just the top 6 mutated genes, tissue samples can be reordered. This enables\n  our users to see the impact of mutations across the global set or restricted\n  to a subset. When selecting this option, users will see the samples with\n  mutations shift to the left.\n\u003C\u002Fp\u003E\n\n\u003Cp\u003E\n  To understand gain and\u002For loss of function, our users can adjust the gene\n  mutation ordering by switching the gene order based on the frequency of\n  mutation types: missense, other non-synonymous, or both. The gene rows will\n  reorder and the histogram will update based on this selection.\n\u003C\u002Fp\u003E\n\n\u003Ch3\u003EExample code for initializing GridVar\u003C\u002Fh3\u003E\n\n\u003Cpre\u003E\u003Ccode class=\"javascript\"\u003E$('#gridVarGenomics').gridVar({\n    cellHeight: 30,\n    cellWidth: 9,\n    rowOrder: ['TP53', 'PIK3CA', 'AKT1',\n        'CBFB', 'GATA3', 'MAP3K1'],\n        columnOrder: ['BR-M-191','BR-M-037',\n        'BR-V-043','BR-V-027','BR-V-067',\n        ...],\n    dataMapping: {\n        data: [\n            ['AKT1', 'BR-V-016', ['Missense']],\n            ['AKT1', 'BR-V-017', ['Missense']],\n            ['TP53', 'BR-M-123', ['other non-synonymous']],\n            ...],\n        dataIndex: {\n            rowKey: 0,\n            columnKey: 1,\n            mutation: 2\n        }\n    },\n    dataDisplayMapping: [{\n        dataType: 'mutation',\n        mappings: {\n            Missense: '#bb88bb',\n            'other non-synonymous': '#777777'\n        }\n    }],\n    histogramMapping: {\n        data: histogramData,\n        label: 'Mut. Frequency'\n    },\n    rowLabelClicked: function(event, data) {\n        alert(data + ' clicked!');\n    }\n});\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\n\u003Ch3\u003EExample code for reordering rows\u003C\u002Fh3\u003E\n\u003Cpre\u003E\u003Ccode class=\"javascript\"\u003E$('#gridVarGenomics').gridVar('option', 'rowOrder', ['TP53', 'PIK3CA', 'AKT1', 'CBFB', 'GATA3', 'MAP3K1']);\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\n\u003Ch3\u003EExample code for reordering columns\u003C\u002Fh3\u003E\n\u003Cpre\u003E\u003Ccode class=\"javascript\"\u003E$('#gridVarGenomics').gridVar('option', 'columnOrder', ['BR-M-191','BR-M-037','BR-V-043','BR-V-027','BR-V-067']);\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\n\u003Ch2\u003E\u003Ca id=\"weatherExample\"\u003E\u003C\u002Fa\u003EWeather Example\u003C\u002Fh2\u003E\n\n\u003Cp\u003E\n  Although GridVar was developed for a genomics use case, GridVar can visualize\n  all types of categorical data. In the example below, GridVar displays climate\n  data from the\n  \u003Ca\n    target=\"_blank\"\n    href=\"http:\u002F\u002Fwww1.ncdc.noaa.gov\u002Fpub\u002Fdata\u002Fccd-data\u002FCCD-2012.pdf\"\n    \u003EComparative Climate Data for the U.S. through 2012\u003C\u002Fa\n  \u003E\n  from the National Climate Data Center. Each row is one city per state,\n  including Washington D.C., Puerto Rico, and Pacific Islands (i.e. Guam), each\n  column is a month, and at the intersection the example displays the highest\n  recorded temperature per month and the days in those months that were or\n  weren't cloudy.\n\u003C\u002Fp\u003E\n\n\u003Cp\u003ETemperature - Highest of Record: Degrees F\u003C\u002Fp\u003E\n\n\u003Cdiv id=\"gridVarWeather\"\u003E\u003C\u002Fdiv\u003E\n\n\u003Ch3\u003EExample code for initializing GridVar with climate data\u003C\u002Fh3\u003E\n\n\u003Cpre\u003E\u003Ccode class=\"javascript\"\u003E$('#gridVarWeather').gridVar({\n    rowOrder: ['BIRMINGHAM AP, AL', 'ANCHORAGE, AK',\n        'FLAGSTAFF, AZ', 'FORT SMITH, AR',\n        'BAKERSFIELD, CA', ...\n    ],\n    columnOrder: ['January', 'February', 'March', ...],\n    dataMapping: data: [\n            ['BIRMINGHAM AP, AL', 'January', ['Neutral'],\n            [81],\n            ['cloudy'],\n            [7, 6, 18]\n        ],\n            ['BIRMINGHAM AP, AL', 'February', ['Neutral'],\n            [83],\n            ['cloudy'],\n            [7, 6, 15]\n        ],\n            ['BIRMINGHAM AP, AL', 'March', ['Hot'],\n            [89],\n            ['cloudy'],\n            [7, 8, 16]\n        ],\n        ...\n    ],\n    dataIndex: {\n        rowKey: 0,\n        columnKey: 1,\n        temperature: 2,\n        cloudy: 4\n    }\n    ],\n    dataDisplayMapping: [{\n      dataType: 'temperature',\n      mappings: {\n          Hot: '#E44C16',\n           Neutral: '#E3E3E3',\n            Cool: '#01A2BF'\n        },\n        labelMapping: {\n           Hot: 'Hot (greater than 88)',\n           Neutral: 'Neutral',\n           Cool: 'Cool (less than 55)'\n        }\n    }, {\n       dataType: 'cloudy',\n           mappings: {\n           clear: 'circleRenderer',\n           cloudy: 'minusRenderer'\n        }\n    }],\n    rowLabelClicked: function (event, data) {\n         alert(data + ' clicked!');\n    }\n});\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n"}]}}("Jenkins-LSCI","jpkl","Peax","Habitat","yada","OntoBrowser","Railroadtracks","GridVar","JPkl","peax","habitat","YADA","ontobrowser","railroadtracks","yap","Yet Another Pipeline","gridvar"))],session:{basepath:"",forArtifactory:false}};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');</script><script src=/client/1ef9048e76cf74561a9e/main.js></script> 