[{"slug":"jpkl","metadata":{"name":"JPkl","abbreviation":"jpkl","projectName":"jpkl","launched":"2018-09-18T00:00:00.000Z","summary":"Fast, compressed JPEG storage for NumPy applications.","description":"<p>Serialize high memory / disk consumption NumPy multidimensional arrays of images into JPEG-Pickle files for low storage cost and decently fast, random access with NumPy-like indexing.</p>\n","dateString":"Tue Sep 18 2018","title":"JPkl","styles":[],"scripts":[]},"html":"<p>Serialize high memory / disk consumption NumPy multidimensional arrays of images into JPEG-Pickle files for low storage cost and decently fast, random access with NumPy-like indexing.</p>\n<p>Code at a glance:</p>\n<pre><code class=\"language-python\">import numpy as np\nfrom jpkl import JPkl\n\n# Make some arbitrary data. Say we have 512x512 images with time, z depth, and multiple channels.\nimages = np.zeros((512, 512, 100, 3, 2))\n\n# Automagically encode all 512x512 images into JPEGs.\njpkl_images = JPkl(images)\n\n# Index / slice encoded JPEGs as you would a NumPy array.\n# JPEG-encoded slices are decoded and all concatenated into an array on-the-fly.\n\ndecoded_images = pkl_images[128:256:2, :, 4, ...]  # decoded_images is a NumPy array\n\ndecoded_images.shape  # (64, 512, 14, 15) [array is .squeeze()&#39;d as in NumPy indexing]\n\n# JPkl objects support 3 simple NumPy array properties.\n# This means you can sometimes get away with passing a JPkl instead of an array to functions.\n\njpkl_images.shape    # (512, 512, 100, 3, 2)\njpkl_images.ndim     # 5\njpkl_images.size     # 5505024000 [product of pkl_images.shape elements]\n\n# Pickle your JPkl and save to disk:\njpkl_images.save(&#39;pickled_images.jpkl&#39;)\n\n# Load a saved JPkl file:\nsaved_jpkl_images = JPkl.load(&#39;pickled_images.jpkl&#39;)</code></pre>\n<p>Note that JPkl data is immutable from the high-level interface.</p>\n<p>See the Jupyter Notebook demo for a more in-depth walkthrough (as well as a cool HoloViews visualization demo of scrubbing through these multidimensional arrays!).</p>\n<h2 id=\"motivation\">Motivation</h2>\n<p>Data from scientific imaging applications (such as microscopy) is often stored as raw, uncompressed data. This is important in cases where accurate quantification on pixel values is necessary. Due to this need, however, these image files (and their corresponding memory footprints) are often large, sometimes approaching 10s (or more) of gigabytes in extreme examples. While memory-mapping these files to access them without loading everything into memory is often a viable option, large I/O speed demands due to the sizes of individual frames makes latency during visualization a possible issue. Additionally, this mapping still does not solve the issue of huge disk space consumption.</p>\n<p>As such, to facilitate speedy visualization of the raw / completely processed images as well as intermediate stage images (which you may not need to keep the uncompressed pixel values for anyway), it makes sense to lossily compress the images for non-computional, solely visualization purposes. JPkl handles this need by compressing all the individual image slices into JPEGs in memory which can then be serialized out to disk. Depending on image content and JPEG quality level, you can see massive (5-10x +) decreases in file size. In addition, JPkl supports on-the-fly decoding of abitrary slices of the pseudo-NumPy array into true NumPy arrays, meaning when you unpickle the JPkl file, your memory consumption remains as low as the file usage on disk unless you want to convert the whole JPkl into an array at once.</p>\n<p>Note that JPkl is not limited to using Pickle serialization or JPEG encoding. Extending the <code>JPkl</code> class and changing the implementations of <code>jpkl_obj.save()</code> / <code>JPkl.load()</code> or <code>jpkl_obj.encode_slice()</code> / <code>jpkl_obj.decode_slice()</code> will allow you to use any serialization or encoding you wish as appropriate for your application, respectively.</p>\n<p>JPkl is most useful in the case where you have:</p>\n<ul>\n<li>Large raw image datasets which you would like to visualize (such as in a Jupyter Notebook)</li>\n<li>Datasets that are highly multi-dimensional (x,y spatial + arbitrary number of time / channel axes)</li>\n<li>The need to visualize intermediate steps of image processing (which adds a multiplier on disk space consumption if\nintermediate steps are stored as raw images)</li>\n<li>The desire to keep all images from a dataset / NumPy array in a single file, rather than all messily dumped as separate .jpgs into the filesystem</li>\n</ul>\n<p>JPkl uses the Pillow (PIL fork) library for encoding / decoding JPEG images.</p>\n<h2 id=\"attribution\">Attribution</h2>\n<p>This is part of the Open Source at Novartis Institutes for BioMedical Research (NIBR) initiative:</p>\n<ul>\n<li><a href=\"https://opensource.nibr.com/\">Open Source @ NIBR home</a></li>\n<li><a href=\"https://github.com/Novartis/\">GitHub organization</a></li>\n</ul>\n<p>It is licensed under Apache License, Version 2.0.</p>\n<p>Maintainer: @zbarry</p>\n<h2 id=\"jpkl-class-file-format-specification-\">JPkl class / file format specification:</h2>\n<p>JPkls on disk are simply a dictionary of:</p>\n<pre><code>&#39;header&#39;: Tuple of (&#39;JPkl&#39;, &#39;version&#39;), version in set{&#39;1&#39;}\n    Used for sanity checking if file on disk is JPkl file and is a loadable version.\n\n&#39;color&#39;: bool: if True, this came from a set of images with a color channel.\n    Therefore, the third axis [index 2] of the returned stack will be of length 3 for RGB.\n    Otherwise, there is no color axis.\n\n&#39;jpeg_quality&#39;: int: level of JPEG compression (0-100).\n\n&#39;images&#39;: Dictionary of byte streams from Pillow `Image` objects encoding images to `bytes` using `io.BytesIO` memory streams\n    Each key of the dictionary is a tuple of (channel 1, channel 2, ...) indices which correspond to a single image\n    slice. All images must be of the same width and height (since they are derived from / decoded into NumPy arrays).\n    jpkl_obj.images[0, 5, 2], for example, returns a byte stream of the JPEG-encoded image slice which would have\n    been accessed in the original NumPy array as `image_array[:, :, 0, 5, 2]`. If the images were RGB, the color\n    axis is not a key in the dictionary.\n\n&#39;dim_names&#39;: List of strings of axis identities that come after height, width. Does not include color name. Not necessary for JPkl, but included for user&#39;s own documentation, if desired.\n\n&#39;dim_sizes&#39;: Tuple of the lengths of each channel axis. Does not include [height, width, RGB] axes.\n\n&#39;metadata&#39;: Dictionary of arbitrary data for the user&#39;s usage. Completely ignored by JPkl.</code></pre><h2 id=\"installation\">Installation</h2>\n<p><strong>Dependencies (installation is through Anaconda system):</strong></p>\n<h3 id=\"jpkl-\">JPkl:</h3>\n<ul>\n<li>Python &gt;= 3.6</li>\n<li>NumPy</li>\n<li>Pillow (Python Imaging Library fork)</li>\n</ul>\n<h3 id=\"jupyter-notebook-demo-\">Jupyter Notebook demo:</h3>\n<ul>\n<li>Jupyter Notebook / Lab</li>\n<li><p>HoloViews and associated libraries:</p>\n<ul>\n<li>Param</li>\n<li>ParamBokeh</li>\n<li>Bokeh</li>\n<li>ImaGen</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"installation-procedure-\">Installation procedure:</h3>\n<p>Default installation including libraries enabling the notebook demo:</p>\n<pre><code>git clone &lt;&lt;repo&gt;&gt;\ncd jpkl\nsource activate YOURENVNAME\nconda env update -f environment.yml\npip install .</code></pre><p>Substitute <code>pip install -e .</code> above if you wish to make edits to the source.</p>\n<p>While using a Conda environment is highly recommended in general, you can technically leave out the the <code>source activate YOURENVNAME</code> line to install to the default package folder. If you&#39;ve not used Conda environments before, you can create one with <code>conda create -n YOURENVNAME</code> and then activate it with <code>source activate YOURENVNAME</code> as in above. This will allow you to keep your installed packages separate between projects.</p>\n<p>Lightweight install with solely JPkl functionality without visualization:</p>\n<pre><code>git clone &lt;&lt;repo&gt;&gt;\ncd jpkl\nsource activate YOURENVNAME\nconda env update -f environment-novis.yml\npip install .</code></pre><h2 id=\"contributing\">Contributing</h2>\n<p>I am more than happy to field pull requests! If interested, please post an issue for what you&#39;re thinking about working on so I can make sure it aligns with the vision for the project. The current guiding principle here is to keep it as lightweight / minimalistic as possible, though this is by no means set in stone.</p>\n<p>Areas of immediate interest for PRs:</p>\n<ul>\n<li>Multiprocessing for encoding / decoding (chunks of / whole) huge NumPy arrays quickly. Initial array encoding isn&#39;t incredibly fast, though random access for visualization is very usable. Multiprocessing might be a huge boon for the initial construction of the JPkls from arrays.</li>\n<li>Clever ways of caching frequently-accessed images might be useful in some cases.</li>\n<li>Other encoding / compression methods which may be superior to JPEG / Pickle files.</li>\n</ul>\n"},{"slug":"peax","metadata":{"name":"Peax","abbreviation":"Peax","projectName":"peax","launched":"2018-09-15T00:00:00.000Z","icon":"/projects/peax/peax-icon.png","summary":"Interactive concept learning and visual exploration of epigenomic patterns.","description":"<p>Peax is a tool for interactive concept learning and exploration of epigenomic patterns based on unsupervised machine learning with autoencoders.</p>\n","dateString":"Sat Sep 15 2018","title":"Peax","styles":[],"scripts":[]},"html":"<blockquote>\n<p>A pattern explorer for epigenomic data.</p>\n</blockquote>\n<p><img src=\"/projects/peax/teaser.png\" alt=\"Peax&#39;s UI\"></p>\n<blockquote>\n<p>Epigenomic data expresses a rich body of diverse patterns that help to identify\nregulatory elements like promoter, enhancers, etc. But finding these patterns reliably\ngenome wide is challenging. Peax is a tool for interactive visual pattern search and\nexploration of epigenomic patterns based on unsupervised representation learning with\nconvolutional autoencoders. The visual search is driven by manually labeled genomic\nregions for actively learning a classifier to reflect your notion of interestingness.\nMore at <a href=\"http://peax.lekschas.de\">peax.lekschas.de</a>.</p>\n</blockquote>\n<h2 id=\"installation\">Installation</h2>\n<pre><code class=\"language-bash\">git clone https://github.com/Novartis/peax peax &amp;&amp; cd peax\nmake install</code></pre>\n<p><em>Do not fear, <code>make install</code> is just a convenience function for setting up conda and installing npm packages.</em></p>\n<p><strong>Notes:</strong></p>\n<ul>\n<li>If you&#39;re a macOS user you might need to <a href=\"https://brew.sh\">brew</a> install <code>libpng</code> and <code>openssl</code> for the <a href=\"https://github.com/nvictus/pybbi\">pybbi</a> package (see <a href=\"https://github.com/nvictus/pybbi/issues/2\">here</a>) and <code>xz</code> for pysam (if you see an error related to <code>lzma.h</code>).</li>\n</ul>\n<h2 id=\"overview\">Overview</h2>\n<p>Peax consists of three main parts:</p>\n<ol>\n<li>A server application for serving genomic and autoencoded data on the web. [<a href=\"https://github.com/Novartis/peax/tree/develop/server\">/server</a>].</li>\n<li>A user interface for exploring, visualizing, and interactively labeling genomic regions. [<a href=\"https://github.com/Novartis/peax/tree/develop/ui\">/ui</a>].</li>\n<li>A set of examples showing how to configure Peax and build your own. [<a href=\"https://github.com/Novartis/peax/tree/develop/examples\">/examples</a>]</li>\n</ol>\n<h2 id=\"data\">Data</h2>\n<p>We provide 6 autoencoders trained on 3 kb, 12 kb, and 120 kb window sizes (with 25,\n100, and 1000 bp binning) on DNase-seq and histone mark ChIP-seq data.</p>\n<p>You can find the autoencoder at <a href=\"https://zenodo.org/record/2609763\">zenodo.org/record/2609763</a>.</p>\n<h2 id=\"preprint\">Preprint</h2>\n<p>Lekschas et al., <a href=\"https://www.biorxiv.org/content/10.1101/597518v1\">Peax: Interactive Visual Pattern Search in Sequential Data Using Unsupervised Deep Representation Learning</a>,\n<em>bioRxiv</em>, 2019, doi: <a href=\"10.1101/597518\">10.1101/597518</a>.</p>\n<h2 id=\"quick-start\">Quick start</h2>\n<p>Peax comes with <a href=\"#autoencoders\">6 autoencoders</a> for DNase-seq and histone mark\nChIP-seq data and several example configurations for which we provide\nconvenience scripts to get you started as quickly as possible.</p>\n<p>For instance, run one of the following commands to start Peax with a DNase-seq\ntrack for 3 kb, 12 kb, and 120 kb genomic windows.</p>\n<table>\n<thead>\n<tr>\n<th>Command</th>\n<th>Window Size</th>\n<th>Step Freq.</th>\n<th>Chromosomes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>make example-3kb</code></td>\n<td>3 kb</td>\n<td>2</td>\n<td>21</td>\n</tr>\n<tr>\n<td><code>make example-12kb</code></td>\n<td>12 kb</td>\n<td>3</td>\n<td>20-21</td>\n</tr>\n<tr>\n<td><code>make example-120kb</code></td>\n<td>120 kb</td>\n<td>6</td>\n<td>17-21</td>\n</tr>\n</tbody></table>\n<p><strong>Note:</strong> The first time Peax is started it will precompute the datasets for\nexploration. This can take a few minutes depending on your hardware. Also, these demos\nwill only prepare the above mentioned chromosomes, so don&#39;t try to search for patterns\non another chromosome. It won&#39;t work! For your own data you can freely configure this\nof course.</p>\n<p>The scripts will download test ENCODE tracks and use the matching\nconfiguration to start the server. More examples are described in <a href=\"https://github.com/Novartis/peax/tree/develop/examples\"><code>/examples</code></a>.</p>\n<h2 id=\"get-started\">Get Started</h2>\n<p>In the following we describe how you can configure Peax for your own data.</p>\n<h4 id=\"configure-peax-with-your-data\">Configure Peax with your data</h4>\n<p>Next, you need to configure Peax with your data to tell it which tracks you want to visualize in HiGlass and which of those tracks are encodable using an (auto)encoder.</p>\n<p>The fastest way to get started is to copy the example config:</p>\n<pre><code>cp config.json.sample config.json</code></pre><p>The config file has 6 top level properties:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Description</th>\n<th>Dtype</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>encoders</td>\n<td>List of encoders.</td>\n<td>list</td>\n</tr>\n<tr>\n<td>datasets</td>\n<td>List of tracks.</td>\n<td>list</td>\n</tr>\n<tr>\n<td>coords</td>\n<td>Genome coordinates. Peax currently supports hg19, hg28, mm9, and mm10</td>\n<td>str</td>\n</tr>\n<tr>\n<td>chroms</td>\n<td>Chromosomes to to be searched. If omitted all chromosomes will be prepared for searching.</td>\n<td>list</td>\n</tr>\n<tr>\n<td>step_freq</td>\n<td>Step frequency of the sliding window approach. E.g., given an encoder with window size 12 kb, a step frequency of 6 means that every 2 kb a 12 kb window will be extracted from the bigWig.</td>\n<td>int</td>\n</tr>\n<tr>\n<td>db_path</td>\n<td>Relative path to the sqlite db for storing searches.</td>\n<td>str</td>\n</tr>\n</tbody></table>\n<p>The main parts to adjust are <code>encoders</code> and <code>datasets</code>. <code>encoders</code> is a list of\n(auto)encoder definitions for different datatypes.T here are two ways to\nconfigure an (auto)encoder: (a) point to a pre-defined autoencoder or (b)\nconfigure from scratch.</p>\n<p>Assuming you want to use a predefined autoencoder all you have to do is</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Description</th>\n<th>Dtype</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>content_type</td>\n<td>Unique string identifying the autoencoder in the configuration file</td>\n<td>str</td>\n</tr>\n<tr>\n<td>from_file</td>\n<td>Relative path to the encoder configuration file.</td>\n<td>str</td>\n</tr>\n</tbody></table>\n<p><strong>Example:</strong></p>\n<pre><code class=\"language-json\">{\n  &quot;content_type&quot;: &quot;histone-mark-chip-seq-3kb&quot;,\n  &quot;from_file&quot;: &quot;examples/encoders.json&quot;\n}</code></pre>\n<p>The encoder configuration file is a dictionary with the top level keys acting\nas the identifier and need to match <code>content_type</code> above. Given the example\nfrom above the file could look like this:</p>\n<pre><code class=\"language-json\">{\n  &quot;histone-mark-chip-seq-3kb&quot;: {},\n  &quot;dnase-seq-3kb&quot;: {}\n}</code></pre>\n<p>See <code>[encoders.json](encoders.json)</code> for an example. The specific definition if an\nautoencoder is the same as described in the following.</p>\n<p>To configure an autoencoder from scratch you need to provide a dictionary with\nthe following required format:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Description</th>\n<th>Defaults</th>\n<th>Dtype</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>autoencoder</td>\n<td>Relative path to your pickled autoencoder model. (hdf5 file)</td>\n<td>&nbsp;</td>\n<td>str</td>\n</tr>\n<tr>\n<td>encoder</td>\n<td>Relative path to your pickled encoder model. (hdf5 file)</td>\n<td>&nbsp;</td>\n<td>str</td>\n</tr>\n<tr>\n<td>decoder</td>\n<td>Relative path to your pickled decoder model. (hdf5 file)</td>\n<td>&nbsp;</td>\n<td>str</td>\n</tr>\n<tr>\n<td>content_type</td>\n<td>Unique string describing the content this autoencoder can handle. Data tracks with the same content type will be encoded by this autoencoder.</td>\n<td>&nbsp;</td>\n<td>str</td>\n</tr>\n<tr>\n<td>window_size</td>\n<td>Window size in base pairs used for training the autoencoder.</td>\n<td>&nbsp;</td>\n<td>int</td>\n</tr>\n<tr>\n<td>resolution</td>\n<td>Resolution or bin size of the window in base pairs.</td>\n<td>&nbsp;</td>\n<td>int</td>\n</tr>\n<tr>\n<td>latent_dim</td>\n<td>Number of latent dimensions of the encoded windows.</td>\n<td>&nbsp;</td>\n<td>int</td>\n</tr>\n<tr>\n<td>input_dim</td>\n<td>Number of input dimensions for Keras. For 1D data these are 3: samples, data length (which is <code>window_size</code> / <code>resolution</code>), channels.</td>\n<td>3</td>\n<td>int</td>\n</tr>\n<tr>\n<td>channels</td>\n<td>Number of channels of the input data. This is normally 1.</td>\n<td>1</td>\n<td>int</td>\n</tr>\n</tbody></table>\n<p><em>Note that if you have specified an <code>autoencoder</code> you do not need to provide\nseparate <code>encoder</code> and <code>decoder</code> models.</em></p>\n<p><strong>Example:</strong></p>\n<pre><code class=\"language-json\">{\n  &quot;encoder&quot;: &quot;path/to/my-12kb-chip-seq-encoder.h5&quot;,\n  &quot;decoder&quot;: &quot;path/to/my-12kb-chip-seq-decoder.h5&quot;,\n  &quot;content_type&quot;: &quot;histone-mark-chip-seq&quot;,\n  &quot;window_size&quot;: 12000,\n  &quot;resolution&quot;: 100,\n  &quot;channels&quot;: 1,\n  &quot;input_dim&quot;: 3,\n  &quot;latent_dim&quot;: 12\n}</code></pre>\n<p>Datasets require the following format:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Description</th>\n<th>Dtype</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>filepath</td>\n<td>Relative path to the data file (bigWig or bigBed).</td>\n<td>str</td>\n</tr>\n<tr>\n<td>content_type</td>\n<td>Unique string describing the content this dataset. If you want to search for patterns in this track you need to have an autoencoder with a matching content type.</td>\n<td>str</td>\n</tr>\n<tr>\n<td>id</td>\n<td>A unique string identifying your track. (Optional)</td>\n<td>str</td>\n</tr>\n<tr>\n<td>name</td>\n<td>A human readable name to be shown in HiGlass. (Optional)</td>\n<td>str</td>\n</tr>\n</tbody></table>\n<p><strong>Example:</strong></p>\n<pre><code class=\"language-json\">{\n  &quot;filepath&quot;: &quot;data/chip-seq/my-fancy-gm12878-chip-seq-h3k27ac-fc-signal.bigWig&quot;,\n  &quot;content_type&quot;: &quot;histone-mark-chip-seq&quot;,\n  &quot;uuid&quot;: &quot;my-fancy-gm12878-chip-seq-h3k27c-track&quot;,\n  &quot;name&quot;: &quot;My Fancy GM12878 ChIP-Seq H3k27c Track&quot;\n}</code></pre>\n<h4 id=\"start-peax\">Start Peax</h4>\n<p>First, start the Peax server to serve your data.</p>\n<p><strong>Note:</strong> The first time you run Peax on a new dataset all the data will be prepared!\nDepending on your machine this can take some time. If you want to track the progress\nactivate the debugging mode using <code>-d</code>.</p>\n<pre><code class=\"language-bash\">python start.py</code></pre>\n<p>Now go to <a href=\"http://localhost:5000\">http://localhost:5000</a>.</p>\n<p>To <code>start.py</code> script supports the following options:</p>\n<pre><code class=\"language-bash\">usage: start.py [-h] [-c CONFIG] [--clear] [--clear-cache]\n                [--clear-cache-at-exit] [--clear-db] [-d] [--host HOST]\n                [--port PORT] [-v]\n\nPeak Explorer CLI\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -c CONFIG, --config CONFIG\n                        path to your JSON config file\n  --clear               clears the cache and database on startup\n  --clear-cache         clears the cache on startup\n  --clear-cache-at-exit\n                        clear the cache on shutdown\n  --clear-db            clears the database on startup\n  -d, --debug           turn on debug mode\n  --host HOST           customize the hostname\n  --port PORT           customize the port\n  -v, --verbose         turn verbose logging on</code></pre>\n<p>The <code>hostname</code> defaults to <code>localhost</code> and the <code>port</code> of the backend server defaults\nto <code>5000</code>.</p>\n<p>In order to speed up subsequend user interaction, Peax initially prepapres all\nthe data and caches that data under <code>/cache</code>. You can always remove this\ndirectory manually or clear the cache on startup or at exist using the <code>--clear</code>\nas specified above.</p>\n<hr>\n<h2 id=\"development\">Development</h2>\n<p>Handy commands to keep in mind:</p>\n<ul>\n<li><code>make install</code> installs the conda environment and npm packages and builds the UI</li>\n<li><code>make update</code> updates the conda environment and npm packages and rebuilds the UI</li>\n<li><code>make build</code> builds the UI</li>\n<li><code>./start.py</code> starts the Flask server application for serving data</li>\n<li>[/ui]: <code>npm install</code> installs and updates all the needed packages for the frontend</li>\n<li>[/ui]: <code>npm build</code> creates the production built of the frontend</li>\n<li>[/ui]: <code>npm start</code> starts a dev server with hot reloading for the frontend</li>\n</ul>\n<p>To start developing on the server and the ui in parallel, first start the backend server\napplication using <code>./start.py</code> and then start the frontend server application from\n<code>./ui</code> using <code>npm start</code>. Both server&#39;s watch the source code, so whenever you make\nchanges to the source code the servers will reload.</p>\n<h3 id=\"configuration\">Configuration</h3>\n<p>There are 2 types of configuration files. The <a href=\"#configure-peax-with-your-data\">backend server configuration</a>\ndefines the datasets to explore and is described in detail <a href=\"#configure-peax-with-your-data\">above</a>.</p>\n<p>Additionally, the frontend application can be configured to talk to a different backend\nserver and port if needed. Get started by copying the example configuration:</p>\n<pre><code class=\"language-bash\">cd ui &amp;&amp; cp config.json.sample config.json</code></pre>\n<p>By default the <code>server</code> is dynamically set to the hostname of the server running the\nfrontend application. I.e., it is assumed that the backend server application is\nrunning on the same host as the frontend application. The <code>port</code> of the server\ndefaults to <code>5000</code>.</p>\n<h3 id=\"start-the-backend-and-frontend-apps\">Start the backend and frontend apps</h3>\n<p>For development the backend and frontend applications run as seperate server\napplications.</p>\n<pre><code class=\"language-bash\"># Backend server\n./start.py --debug --config path/to/your/config.json\n\n# Frontend server\ncd ui &amp;&amp; npm start</code></pre>\n"},{"slug":"jenkins-lsci","metadata":{"name":"Jenkins-LSCI","abbreviation":"Jenkins-LSCI","projectName":"Jenkins-LSCI","launched":"2016-08-30T00:00:00.000Z","icon":"/projects/jenkins-lsci/Jenkins_LifeSci.png","summary":"Life Science Continuous Integration -- Workflows and data pipelines for life science research.","description":"<p>Jenkins-LSCI enables research scientists to build workflows and data pipelines on the same robust framework and plugin ecosystem as Jenkins-CI the widely used continuous integration server that supports building, deploying and automating any software project.</p>\n","dateString":"Tue Aug 30 2016","title":"Jenkins-LSCI","styles":[],"scripts":[]},"html":"<h2 id=\"what-can-jenkins-lsci-do-\">What can Jenkins-LSCI do?</h2>\n<p>Jenkins-LSCI enables research scientists to build workflows and data pipelines on the same robust framework and plugin ecosystem as <a href=\"https://jenkins.io/\">Jenkins-CI</a>, the widely used continuous integration server that supports building, deploying and automating any software project.</p>\n<h2 id=\"high-content-image-analysis-with-jenkins-lsci-and-cellprofiler\">High Content Image Analysis with Jenkins-LSCI and CellProfiler</h2>\n<p>To demonstrate the utility of Jenkins-LSCI as an integration platform for life-sciences research applications, we provide a set of Jenkins-LSCI jobs that can enhance the usability of <a href=\"http://cellprofiler.org\">CellProfiler</a> in a high performance workflow for high content screening image analysis. In addition, these jobs can form the basis for managing and sharing imaging pipelines and data in ways that can enhance scientific collaboration and reproducibility.</p>\n<p>These related Jenkins-LSCI jobs enable users to:</p>\n<ol>\n<li>Create a library of re-usable, annotated CellProfiler pipelines.</li>\n<li>Create a library of re-usable, annotated CellProfiler image lists</li>\n<li>Prepare task arrays for CellProfiler high performance image analysis (CellProfiler batch mode) on a Linux cluster</li>\n</ol>\n<h2 id=\"what-are-the-advantages-\">What are the advantages?</h2>\n<ul>\n<li><p>Image analysis pipelines, image lists, and generated measurements are all managed by the Jenkins-LSCI server and can be reviewed, shared and reused in multiple ways thus building a <strong>collaborative foundation for reproducible research</strong>.</p>\n</li>\n<li><p>CellProfiler can be parallelized to execute on a <strong>high performance</strong> compute cluster.</p>\n</li>\n<li><p>High performance image analysis becomes <strong>accesible to laboratory scientists</strong>.</p>\n</li>\n<li><p>All functionality is <strong>web-accessible</strong>. Users do not need to install CellProfiler locally, or run CellProfiler with the potentially limited compute resources of their local workstation/laptop.</p>\n</li>\n<li>This basic set of jobs can be <strong>easily extended</strong> with jobs for data and imaging utilities as the computational needs of the laboratory demand.</li>\n</ul>\n<h2 id=\"the-jenkins-lsci-cellprofiler-jobs\">The Jenkins-LSCI-CellProfiler Jobs</h2>\n<h3 id=\"os_contribute_pipeline\">OS_CONTRIBUTE_PIPELINE</h3>\n<p>The <code>OS_CONTRIBUTE_PIPELINE</code> job allows users to annotate and upload a CellProfiler pipeline to the Jenkins server. The build generates a pipeline summary report combining user and internal pipeline annotations. The pipeline is stored on the Jenkins server and can be used by CellProfiler running on a remote cluster or workstation, or on the user&#39;s desktop.</p>\n<h3 id=\"os_contribute_imagelist\">OS_CONTRIBUTE_IMAGELIST</h3>\n<p>The <code>OS_CONTRIBUTE_IMAGELIST</code> job facilitates the creation and management of correctly formatted image lists that can be used for processing large number of images through a CellProfiler pipeline. Image processing is highly dependent on image acquisition and experimental metadata (wavelengths, plates, wells, fields, time-points, Z-stacks etc.) We support the direct parsing of metadata generated during the image acquisition phase from the <strong>InCell</strong> and <strong>Yokogawa</strong> scientific imagers.</p>\n<p>In the absence of instrument metadata, image lists can be created using the CellProfiler desktop client. These image lists can then be easily adapted using the <code>OS_CONTRIBUTE_IMAGELIST</code> job to formats compatible with high performance cluster analysis.</p>\n<h3 id=\"os_cellprofiler_batch\">OS_CELLPROFILER_BATCH</h3>\n<p>The <code>OS_CELLPROFILER_BATCH</code> job allows users to generate correctly formatted CellProfiler task array scripts that can be submitted to a Linux cluster for parallel processing. The job builds take as input a reference to an <code>OS_CONTRIBUTE_PIPELINE</code> build and a reference to an <code>OS_CONTRIBUTE_IMAGELIST</code> build. Theses builds define the two inputs for a typical CellProfiler run, the image processing pipeline and the image list.</p>\n<p>The task array script is currently formatted for the <a href=\"http://www.univa.com/products/\">UNIVA grid engine</a> scheduler and it should require little effort to customize for other cluster schedulers. However, due to the non-standardized, local, Linux cluster environment and installed software dependencies, the task array submission process is left up to the user. In addition, CellProfiler and its library dependencies must be installed and correctly working on the destination Linux cluster.</p>\n<h3 id=\"os_cellprofiler_jclustbatch\">OS_CELLPROFILER_JCLUSTBATCH</h3>\n<p>The <code>OS_CELLPROFILER_JCLUSTBATCH</code> job is an example CellProfiler on cluster job that includes the steps for, creating task arrays, submitting them to the UNIVA grid engine, monitoring the task array execution, and finally merging the individual CellProfiler task output generated on the Linux cluster.</p>\n<p>Due to its high dependency on the Novartis cluster infrastructure this job is unlikely to run unmodified on other Linux clusters, but it provides a useful and complete exemplar on how Jenkins-LSCI is used to integrate a complex workflow and monitor an external job.</p>\n<h2 id=\"test-drive-jenkins\">Test Drive Jenkins</h2>\n<p>If you would like to quickly <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/Meet+Jenkins\">test drive Jenkins-CI</a> you can <a href=\"http://mirrors.jenkins-ci.org/war/latest/jenkins.war\">download jenkins.war</a> directly and launch it by executing <code>java -jar jenkins.war</code>. Once it launches, visit <code>http://localhost:8080/</code> in your browser to get to the dashboard. On Windows, you can even choose to install <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/Installing+Jenkins+as+a+Windows+service\">Jenkins as a service</a> afterwards.</p>\n<h2 id=\"jenkins-lsci-installation-configuration\">Jenkins-LSCI Installation &amp; Configuration</h2>\n<p>A fully functional Jenkins-LSCI server supporting CellProfiler image analysis requires the installation and configuraton of the following software components:</p>\n<ol>\n<li>A mirror of the Jenkins-LSCI project code (from git)</li>\n<li><a href=\"https://jenkins.io/\">Jenkins-CI</a> and required Jenkins plugins</li>\n<li><a href=\"http://cellprofiler.org\">CellProfiler</a></li>\n</ol>\n<p>Please, refer to the <a href=\"./userContent/docs/installation_and_use.html\">Jenkins-LSCI Installation and Usage</a> for more details</p>\n<h2 id=\"getting-help\">Getting Help</h2>\n<p>For general assistance with Jenkins-CI you can consult the <a href=\"https://groups.google.com/forum/#!forum/jenkinsci-users\">Jenkins Google user group</a> and the extensive, community-maintained <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/Use+Jenkins\">Jenkins-CI wiki</a></p>\n<p>For general assistance with CellProfiler you can consult the <a href=\"http://forum.cellprofiler.org/\">CellProfiler User Group</a></p>\n<p>The <a href=\"http://biouno.org\">BioUno</a> open source project and <a href=\"https://groups.google.com/forum/#!forum/biouno-users\">BioUno Google user group</a> can provide additional guidance and assistance in setting up Jenkins for bioinformatics and data science applications</p>\n<h2 id=\"license\">License</h2>\n<p>The Jenkins-LSCI code is provided under the <a href=\"http://www.apache.org/licenses/LICENSE-2.0.txt\">Apache 2.0</a> license</p>\n"},{"slug":"habitat","metadata":{"name":"Habitat","abbreviation":"Habitat","projectName":"habitat","launched":"2016-04-29T00:00:00.000Z","icon":"/projects/habitat/habitat-icon.png","summary":"Habitat - Where files live. A simple and yet powerful self-contained object storage management system.","description":"<p>Habitat is a simple and yet powerful self-contained object storage management system.\nBased on Amazon Web Services, it is capable of virtually unlimited storage.\nInstead of a large centralized management system, Habitat can be used as a local repository\nfor a single application or it can be shared and used with many clients.</p>\n<p>Habitat is best used for situations where the client producers and consumers of the files\ndo not require a file system protocol interface and can use http(s) to access the store.</p>\n","dateString":"Fri Apr 29 2016","title":"Habitat","styles":[],"scripts":[]},"html":"<h2 id=\"key-features\">Key features</h2>\n<ul>\n<li>Upload/download via http(s) from any client that can issue HTTP POST requests</li>\n<li>Upload/download via any tools that support writing/reading from S3. For example:<ul>\n<li>Web browser (via a Java applet <a href=\"http://docs.aws.amazon.com/AmazonS3/latest/UG/enhanced-uploader.htm\">http://docs.aws.amazon.com/AmazonS3/latest/UG/enhanced-uploader.htm</a>)</li>\n<li>Windows client: <a href=\"http://s3browser.com\">http://s3browser.com</a></li>\n<li>Windows client: <a href=\"http://www.cloudberrylab.com/free-amazon-s3-explorer-cloudfront-IAM.aspx#close\">http://www.cloudberrylab.com/free-amazon-s3-explorer-cloudfront-IAM.aspx#close</a></li>\n<li>Future: Web browser: we are building a web client that we will likely open source</li>\n</ul>\n</li>\n<li>Future: upload via file interface where a file is stored by dropping in a temp directory</li>\n<li>Future: checkout a collection of files into a local file system (setting user/group permissions as desired)</li>\n<li>Immutable reference key for each stored file</li>\n<li>Stores metadata about the object from a variety of sources:<ul>\n<li>Parsed from the file name (customized parsing via a regular expression)</li>\n<li>Extracted from the file contents (via a custom plugin)</li>\n<li>Exracted from a companion metadata file (uploaded either before or after the data file)</li>\n<li>Extracted from the write action event itself</li>\n<li>Extracted from the S3 metadata attribute on the object</li>\n</ul>\n</li>\n<li>Life cycle management to reduce to lower cost storage or to delete after defined time periods</li>\n<li>Get file or file list based on metadata search</li>\n<li>Search index using a discrete or shared Elastic Search instance</li>\n<li>Future: Supports object versioning</li>\n<li>Easily customized to tailor to unique requirements (config file driven, with custom plugins)</li>\n<li>Supports saving a shadow copy of metadata into the S3 object metadata in addition to the Elastic Search index</li>\n</ul>\n"},{"slug":"yada","metadata":{"name":"YADA","abbreviation":"yada","projectName":"yada","launched":"2015-10-25T00:00:00.000Z","summary":"A universal remote control for data.","description":"<p>Enable efficient, non-redundant development of data-dependent applications and utilities, data source querying, data analysis, processing pipelines, extract, transform, and load (ETL) processes. YADA does all this while preserving total decoupling between data access and other aspects of application architecture such as user interface.</p>\n","dateString":"Sun Oct 25 2015","title":"YADA","styles":[],"scripts":[]},"html":"<h2 id=\"why-yada-\">Why YADA?</h2>\n<img src=\"https://github.com/Novartis/YADA/raw/master/src/site/resources/images/blox250.png\"/>\n\n<p><strong>YADA</strong> is like a <a href=\"https://en.wikipedia.org/wiki/Universal_remote\"><em>Universal Remote Control</em></a> for data.</p>\n<p>For example, what if you could access</p>\n<ul>\n<li><em>any</em> data set</li>\n<li>at <em>any</em> data source</li>\n<li>in <em>any</em> format</li>\n<li>from <em>any</em> environment</li>\n<li>using <em>just</em> a URL</li>\n<li>with just <em>one-time</em> configuration?</li>\n</ul>\n<p>You can with <strong>YADA</strong>.</p>\n<p>Or, what if you could get data</p>\n<ul>\n<li>from <em>multiple</em> sources</li>\n<li>in <em>different</em> formats,</li>\n<li><em>merging</em> the results</li>\n<li>into a <em>single</em> set</li>\n<li><em>on-the-fly</em></li>\n<li>with <em>uniform</em> column names</li>\n<li>using <em>just one</em> URL?</li>\n</ul>\n<p>You can with <strong>YADA</strong>.</p>\n<h2 id=\"what-is-yada-\">What is YADA?</h2>\n<p><strong>YADA</strong> exists to simplify data access and eliminate work.</p>\n<p><strong>YADA</strong> is secure.</p>\n<p><strong>YADA</strong> is a lightweight framework for data retrieval, searching, storage, and manipulation.</p>\n<p><strong>YADA</strong> is an instant web service for your data.</p>\n<p><strong>YADA</strong> is a tool to enable efficient development of interfaces and data-processing pipelines.</p>\n<p><strong>YADA</strong> is as an implementation of <a href=\"http://bit.ly/1dhuiRY\">Thin Server Architecture</a>.</p>\n<p><strong>YADA</strong> is anti-middleware.</p>\n<p><strong>YADA</strong> is an acronym for &quot;Yet Another Data Abstraction.&quot;</p>\n<p><strong>YADA</strong> is an open source software framework distributed by <a href=\"http://opensource.nibr.com\">Novartis Institutes for BioMedical Research</a> under the <a href=\"http://www.apache.org/licenses/LICENSE-2.0.html\">Apache 2.0 license</a>.</p>\n<p>Its <a href=\"http://bit.ly/1SHuiAd\">raisons d&#39;être</a> are to enable efficient, non-redundent development of data-dependent applications and utilities, data source querying, data analysis, processing pipelines, extract, transform, and load (ETL) processes, etc. <strong>YADA</strong> does all this while preserving total decoupling between data access and other aspects of application architecture such as user interface.</p>\n<h2 id=\"still-like-huh-\">Still like &quot;Huh?&quot;</h2>\n<p><strong>YADA</strong> is a software framework, which means it is a collection of software tools forming a basic structure underlying a system, for developers and data analysts to use to create new tools and solutions in a new way.</p>\n<p>The novelty and utility of YADA lies in its centralization of management of data source access configuration. It simplifies these aspects of software development by eliminating many steps, thereby enabling rapid development, standardization of access methods, and the code in which these methods are implemented. Further it strongly encourages reuse of existing configurations (once configured.)</p>\n<p>As a result of these configuration facilities, YADA enables the aggregation or integration of data from multiple data sources using a standard method, agnostic with regard to any vendor or technology-specific details of disparate data source implementations.</p>\n<p>For example, the conventional method to access, or furthermore, combine data from say, an Oracle® database, and a web service, is to write code which connects to each database or service independently using different methods and libraries, write code to execute embedded queries independently, also using different methods and libraries, and write code to parse and aggregate the separately acquired data sets. Then the data is typically fed to an analysis tool.</p>\n<p>With YADA, the data source connections and application-specific queries are stored securely and centrally, the queries are executed using identical methods (despite the different sources,) and the data can be integrated or aggregated on-the-fly.</p>\n<p>For software developers and data analysts alike, these features offer potentially tremendous time savings, faster time-to-delivery, and a larger percentage of time focused not on the tedium of configuration, but on the specific context of a software solution or data analysis.</p>\n<h2 id=\"what-s-in-this-document-\">What&#39;s in this document?</h2>\n<p>This document contains an overview of the framework and features. Check out the <a href=\"src/site/markdown/deployment.md\">Quickstart/Deployment Guide</a> for details on getting started.</p>\n<h3 id=\"table-of-contents\">Table of Contents</h3>\n<ol>\n<li><a href=\"#other\">Other Documentation</a></li>\n<li><a href=\"#mindset\">Getting into the YADA Mindset</a></li>\n<li><a href=\"#features\">Features</a></li>\n<li><a href=\"#arch\">Architecture</a></li>\n<li><a href=\"#history\">History</a></li>\n<li><a href=\"#apps\">YADA Apps and Uses</a></li>\n<li><a href=\"#sources\">Data Sources</a></li>\n<li><a href=\"#plugins\">Plugins</a></li>\n<li><a href=\"src/site/markdown/deployment.md\">Installation</a> (links directly to the <a href=\"src/site/markdown/deployment.md\">Quickstart/Deployment Guide</a>)</li>\n<li><a href=\"#license\">License</a></li>\n<li><a href=\"#issues\">Known Issues</a></li>\n</ol>\n<p><a name=\"other\"></a></p>\n<h2 id=\"other-documentation\">Other Documentation</h2>\n<h3 id=\"getting-started\">Getting Started</h3>\n<ul>\n<li><a href=\"src/site/markdown/deployment.md\">Quickstart/Deployment Guide</a></li>\n</ul>\n<h3 id=\"advanced-documentation\">Advanced Documentation</h3>\n<ul>\n<li><a href=\"src/site/markdown/admin.md\">YADA Admin Guide</a></li>\n<li><a href=\"src/site/markdown/guide.md\">User Guide</a></li>\n<li><a href=\"src/site/markdown/security.md\">Security Guide</a></li>\n<li><a href=\"src/site/markdown/pluginguide.md\">Plugin Use and Development Guide</a></li>\n<li><a href=\"src/site/markdown/testing.md\">Testing Notes</a></li>\n</ul>\n<h3 id=\"specifications\">Specifications</h3>\n<ul>\n<li><a href=\"http://opensource.nibr.com/yada/yada-api/apidocs/index.html\">Javadoc</a></li>\n<li><a href=\"src/site/markdown/params.md\">YADA Parameter Specification</a></li>\n<li><a href=\"src/site/markdown/jsonparams.md\">JSONParams Specification</a></li>\n<li><a href=\"src/site/markdown/harmony.md\">Harmonizer Guide and Specification</a></li>\n<li><a href=\"src/site/markdown/uml.md\">Java® Visual Reference</a></li>\n<li><a href=\"src/site/markdown/filters.md\">Filters Specification</a></li>\n<li><a href=\"src/site/markdown/mail.md\">Mail Specification</a></li>\n</ul>\n<p><a name=\"mindset\"></a></p>\n<h2 id=\"getting-into-the-yada-mindset\">Getting into the YADA Mindset</h2>\n<p>YADA exists to simplify data access and eliminate work.</p>\n<p>YADA may be exactly what you&#39;ve been looking for, or it may be a solution to a problem you didn&#39;t know you had. YADA is the perfect tool for many use-cases. Here are a few examples. Suppose you are a</p>\n<h3 id=\"scientist-or-data-analyst-\">Scientist or Data Analyst...</h3>\n<p>The new numbers are in from the lab, or from last night&#39;s feed, and uploaded to your database or data warehouse. You want to create a new visualization in your favorite statistical analysis package, but you&#39;re not sure how to connect to the database. Your data gal helped you set it up in Python once, but since then, you just run the script to get the data. Now you need it in a different environment.</p>\n<p>If your data gal had set up a <strong>YADA</strong> query for your datasource, you could simply run the query in your web browser to download the data, or use any module that will retrieve data from a url. You can reuse the <em>very same query</em> that was already configured for your other tasks.</p>\n<h3 id=\"datasource-owner-curator-\">Datasource Owner/Curator...</h3>\n<p>Your constituents want their data, and they call you. Everyone wants basically the same set of columns but with a different &quot;WHERE&quot; clause, i.e., they each want a different subset of rows. Some can handle connection strings, but most can&#39;t.</p>\n<p>So you configure your datasource in the <strong>YADA</strong> server, store a query, and send the same url to every one, explaining to them where to plug in the values in the query parameter string so they get only the data they want. They might see some columns they don&#39;t want but they can easily ignore them. If someone complains, heck, you just store another similar query with a different name, and voilà.</p>\n<h3 id=\"software-user-interface-developer-\">Software User Interface Developer...</h3>\n<p>You hate middleware. Every time you want to extend the data model, you have to change your Resource layer, your DAO layer, your DAOImpls, your DTOs, your Model classes, your UI code, etc. You might have to touch 20 files to add one field.</p>\n<p>Not so with <strong>YADA</strong>.</p>\n<p>With <strong>YADA</strong>, you change your stored query, and you change the code that executes the stored query, whether it&#39;s a javascript-based ajax call, or a perl LWP request, or a curl call from a shell script. As long as your client speaks HTTP, <strong>YADA</strong> will deliver your data.</p>\n<h3 id=\"software-middleware-developer-\">Software Middleware Developer...</h3>\n<p>Even you, middleware guy, can benefit from <strong>YADA</strong>.</p>\n<p>Maybe you have to provide a RESTful interface to an existing application, and need to deliver in such a short window, or have only a handful of users, so a fully-specified REST service is not practical. Maybe you need to access an existing REST interface and can&#39;t use your own proxy script; or, you have to grant access to a unix filesystem without mapping it in Apache or changing privileges. Perhaps the business wants to integrate some existing perl-based pipeline processes into a user interface or your Javascript UI team is already using <strong>YADA</strong>, and needs a Java® plugin to post-process data it&#39;s retrieving from a third party.</p>\n<h2 id=\"features\">Features</h2>\n<ul>\n<li>Data vendor- and technology-agnostic</li>\n<li>Accesses any JDBC, SOAP, or REST, and some Filesystem datasources</li>\n<li>Delivers data as JSON (default), XML, or CSV, TSV, Pipe, or custom-delimited, natively, and in any other format via custom Response and Converter classes, or Plugins</li>\n<li>4-layer security model including url and token validation, query-execution authorization, and dynamic-predicate-based, pre-execution, row-level filtering</li>\n<li>Dynamic datasource configuration</li>\n<li>Executes multiple queries in a single HTTP request</li>\n<li>On-the-fly inner and outer joins across disparate data sources</li>\n<li>Ad hoc Harmonization (i.e., single http request to multiple data sources with harmonized results)</li>\n<li>Utilizes JDBC transactions (e.g., multiple inserts in a single HTTP request, with a single commit/rollback)</li>\n<li>Commits a single query or an entire request</li>\n<li>Processes file uploads</li>\n<li>Compatible with any client that speaks HTTP (e.g., web browser, python, curl, javascript, spotfire, curl, web service, mobile app, etc)</li>\n<li>Flexible Java® and Script plugin API to preprocess request parameters, post-process results, or override normal processing altogether</li>\n<li><a href=\"http://www.ehcache.org/\">EhCache</a> query-index caching</li>\n<li>Security (via Cookie forwarding and/or Default Plugins)</li>\n<li>Support for Oracle®, MySQL®, Vertica®, PostgreSQL®, HyperSQL®, SQLite®</li>\n<li>Tomcat 8 and JDK 1.8-compatible (YADA 8)</li>\n<li><em>Coming Later</em>: ElasticSearch® support</li>\n<li><em>Coming Later</em>: ElasticSearch®-based result and aggregate-result caching</li>\n<li><em>Coming Later</em>: Dynamic memory management and caching to facilitate large-scale request queuing and high volume result transformation in high frequency environments</li>\n<li><em>Coming Later</em>: SQL DDL</li>\n<li><em>Coming Later</em>: MongoDB® and other NoSQL support</li>\n<li><em>Coming Later</em>: SQL Server® support</li>\n<li><em>Coming Later</em>: Node.js® port (maybe?)</li>\n<li><em>Coming Later</em>: Scala?</li>\n<li><em>Coming Later</em>: Standalone java application</li>\n<li><em>Coming Later</em>: Spark-based result post-processor</li>\n</ul>\n<p><a name=\"arch\"></a></p>\n<h2 id=\"architecture\">Architecture</h2>\n<p>A quick overview of the architecture</p>\n<h3 id=\"generic\">Generic</h3>\n<p>About as basic as it can be...\n<img src=\"https://github.com/Novartis/YADA/raw/master/src/site/resources/images/generic-arch.png\" alt=\"generic architecture\" title=\"Generic Architecture\"></p>\n<h3 id=\"specific\">Specific</h3>\n<p>...and a little bit more specific:</p>\n<blockquote>\n<p>Note the image indicates Tomcat 6. It should be Tomcat <strong>7</strong></p>\n</blockquote>\n<p><img src=\"https://github.com/Novartis/YADA/raw/master/src/site/resources/images/specific-arch.png\" alt=\"specific architecture\" title=\"Specific Architecture\"></p>\n<p><a name=\"history\"></a></p>\n<h2 id=\"history\">History</h2>\n<p>YADA grew organically from a reverse-engineering effort.</p>\n<p>Over the course of a few years, a scientist had developed an array of Perl CGI applications with thousands of lines of embedded SQL and Javascript.</p>\n<p>Then he abruptly left the company.</p>\n<p>He was not a trained, nor experienced software developer, he made little use of third party libraries, and violated a lot of conventions.</p>\n<p>To gain an understanding of his code in order to maintain, extend, or replace it, SQL queries were extracted from the code, stored in a database, and given unique names.</p>\n<p>A &quot;finder&quot; function was written in Perl to retrieve the SQL by name.</p>\n<p>This &quot;finder&quot; was extended to support the passing of parameters.</p>\n<p>Soon thereafter, this perl utility was ported to Java®. The burgeoning framework was extended further to support multiple data types, INSERT, UPDATE, and DELETE statements in addition to SELECT statements, JDBC transactions, SOAP queries, plugins, I/O, and so on.</p>\n<p><a name=\"apps\"></a></p>\n<h2 id=\"yada-apps-and-uses\">YADA &quot;Apps&quot; and uses</h2>\n<p>Most YADA &quot;Apps&quot; are <a href=\"http://en.wikipedia.org/wiki/Single-page_application\">single-page Javascript applications</a> running in web browsers. YADA is also heavily utilized by data analysts and bioinformaticians who need parameterized, delimited data subsets imported into their analysis tools such as R and Spotfire, or to be used by Perl or Python-based data processing pipelines.</p>\n<p><a name=\"sources\"></a></p>\n<h2 id=\"data-sources\">Data Sources</h2>\n<p>YADA ships with scripts for using, as the YADA Index:</p>\n<ul>\n<li>MySQL®</li>\n<li>PostgreSQL®</li>\n<li>HyperSQL®</li>\n<li>SQLite®</li>\n<li>Oracle®</li>\n</ul>\n<p>Soon the index will be stored in ElasticSearch®, but ultimately, it is vendor-agnostic. Other supported data sources currently include</p>\n<ul>\n<li>Vertica®</li>\n<li>SOAP</li>\n<li>REST</li>\n</ul>\n<p>MongoDB®, SQL Server®, and other datasource compatibility will be added soon.</p>\n<p><a name=\"plugins\"></a></p>\n<h2 id=\"plugins\">Plugins</h2>\n<p>For detailed information about plugin use and development, see the <a href=\"src/site/markdown/pluginguide.md\">Plugin Use and Development Guide</a>.</p>\n<p>The plugin API is versatile. Plugins can be written in java, or in any scripting language supported on the YADA server. Plugins can be applied at the request level, affecting the entire request, or it&#39;s output, or at the query level, affecting just a single query in a request. The conceptual, or implementation hierarchy of the plugin API (not to be confused with the actual package hierarchy) is reflected in the diagram below, from two different perspectives.</p>\n<p><img src=\"https://github.com/Novartis/YADA/raw/master/src/site/resources/images/plugin-concept.png\" alt=\"Plugin Concept\" title=\"Plugin Concept\"></p>\n<h3 id=\"plugin-types\">Plugin Types</h3>\n<h4 id=\"pre-processors\">Pre-Processors</h4>\n<p>These are intended to manipulate URL parameters, either by removing, appending, or modifying them.</p>\n<h4 id=\"post-processors\">Post-Processors</h4>\n<p>These are intended to modify results returned by queries. For example, an XSL Post-Processor might accept XML-formatted results and transform them before returning the to the client. Uploaded file processors, i.e., batch handlers, are post-processors.</p>\n<h4 id=\"bypassers\">Bypassers</h4>\n<p>These circumvent conventional YADA query processing. Effectively, anything is possible in a Bypass. Bypass plugins are popular ETL tools and bulk data loaders.</p>\n<p><a name=\"license\"></a></p>\n<h2 id=\"license\">License</h2>\n<p>Copyright &copy; 2016 <a href=\"http://opensource.nibr.com\">Novartis Institutes for Biomedical Research</a></p>\n<p>Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at <a href=\"http://www.apache.org/licenses/LICENSE-2.0\">http://www.apache.org/licenses/LICENSE-2.0</a></p>\n<p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>\n<p><a name=\"issues\"></a></p>\n<h3 id=\"known-issues-last-updated-25-oct-2015-\">Known Issues (last updated 25-OCT-2015)</h3>\n<ul>\n<li>Date and time value syntax, just like in the real world, are database-vendor specific. Use vendor-specific literals and functions. Check the test queries for guidance.</li>\n<li>Speaking of dates and times, right now the TestNG tests which validate date and time values pass only on machines in the &quot;America/NewYork&quot; timezone. This is likely because the insert statements used to put the test data into the test table is not specific.</li>\n<li>There are two drivers for SQL Server®. The one I picked has problems, and I haven&#39;t made time to work with the other.</li>\n</ul>\n"},{"slug":"ontobrowser","metadata":{"name":"OntoBrowser","abbreviation":"OntoBrowser","projectName":"ontobrowser","launched":"2015-03-23T00:00:00.000Z","summary":"A web-based application for managing ontologies and assigning synonyms to ontology terms.\n","description":"<p>The OntoBrowser tool was developed to manage ontologies and code lists. The primary goal of the tool is to provide an online collaborative solution for expert curators to map code list terms (sourced from multiple systems/databases) to preferred ontology terms. Other key features include visualisation of ontologies in hierarchical/graph format, advanced search capabilities, peer review/approval workflow and web service access to data.</p>\n","icon":"/projects/ontobrowser/ontobrowser-logo.png","dateString":"Mon Mar 23 2015","title":"OntoBrowser","styles":[],"scripts":[]},"html":"\n<h2>Key Features</h2>\n<ul>\n  <li>Web based collaborative ontology curation</li>\n  <li>Interactive hierarchical/graph visualisation</li>\n  <li>Cross ontology searching</li>\n  <li>Synonym mapping</li>\n  <li>Automated mapping of similar matching synonyms</li>\n  <li>Central database for all ontologies</li>\n  <li>Version tracking</li>\n  <li>Review/Approve workflow</li>\n  <li>Email notification</li>\n  <li>Full curator audit trail/history</li>\n</ul>\n\n<h2>Documentation</h2>\n<ul>\n  <li>\n    <a href=\"https://github.com/Novartis/ontobrowser/blob/master/doc/INSTALL.md\"\n      >Build and Deploy</a\n    >\n  </li>\n  <li>\n    <a\n      href=\"https://github.com/Novartis/ontobrowser/blob/master/doc/approval_workflow.pdf\"\n      >Approval Workflow</a\n    >\n  </li>\n  <li>\n    <a\n      href=\"https://github.com/Novartis/ontobrowser/blob/master/doc/web_services.md\"\n      >Web Services</a\n    >\n  </li>\n  <li>\n    <a\n      href=\"https://github.com/Novartis/ontobrowser/blob/master/doc/database_design.pdf\"\n      >Database Design</a\n    >\n  </li>\n  <li>\n    <a\n      href=\"https://github.com/Novartis/ontobrowser/blob/master/doc/security_review.md\"\n      >Security Review</a\n    >\n  </li>\n</ul>\n\n<h2>Acknowledgements</h2>\n<p>\n  The OntoBrowser was initially developed in frame of the eTOX consortium. It\n  has received support from the Innovative Medicines Initiative Joint\n  Undertaking under grant agreement 115002, resources of which are composed of\n  financial contribution from the European Union's Seventh Framework Programme\n  (FP7/2007-2013) and EFPIA companies' in kind contribution. We would like to\n  formally acknowledge the contribution to the eTOX project of all scientists\n  and other staff involved.\n</p>\n<center>\n  <a href=\"http://www.etoxproject.eu\"\n    ><img\n      style=\"padding-right:20px;border-style:none\"\n      src=\"/projects/ontobrowser/etox-logo.png\"\n      class=\"scale-image\"\n  /></a>\n  <a href=\"http://www.imi.europa.eu\"\n    ><img\n      style=\"padding-left:20px;border-style:none\"\n      src=\"/projects/ontobrowser/imi-logo.png\"\n      class=\"scale-image\"\n  /></a>\n</center>\n"},{"slug":"railroadtracks","metadata":{"name":"Railroadtracks","abbreviation":"Railroadtracks","projectName":"railroadtracks","icon":"/projects/railroadtracks/railroadtracks-logo.png","launched":"2014-11-07T00:00:00.000Z","buttons":[{"link":"railroadtracks.pdf","icon":"file","text":"Documentation (PDF)"}],"summary":"A toolkit for DNA and RNA-Seq processing steps.\n","description":"<p><code>railroadtracks</code> is a Python toolkit to handle graphs of dependent tasks such as the ones found in bioinformatics pipelines.</p>\n<p>It was created for comparing RNA-Seq pipelines and found its use is other situations, such as writing a flexible system \nfor the QC of NGS data.</p>\n","dateString":"Fri Nov 07 2014","title":"Railroadtracks","styles":[],"scripts":[]},"html":"\n<p><code>railroadtracks</code> provides the following main features:</p>\n\n<ul>\n  <li>\n    <i>ad hoc</i> creation of pipelines, interactive use in mind and\n    <a href=\"http://ipython.org\">ipython</a>-specify display of objects\n  </li>\n\n  <li>separation of the declaration of tasks from their execution</li>\n\n  <li>\n    simple abstractions to perform parallel computing allowing computations to\n    be moved easily to different models for parallel and distributed computing\n  </li>\n\n  <li>\n    a fully-extendable and editable model layer unifying popular tools in DNA\n    and RNA-sequencing data processing under one common interface.\n  </li>\n\n  <li>\n    It can be installed as a regular Python package, for example using\n    <code>pip install</code>. A tutorial as an ipython notebook is avaible as\n    part of the documentation for the package.\n  </li>\n</ul>\n\n<div\n  id=\"carousel-example-generic\"\n  class=\"carousel slide\"\n  data-ride=\"carousel\"\n  data-interval=\"3000\"\n>\n  <div class=\"carousel-inner\">\n    <div class=\"item active\">\n      <div class=\"carousel-caption\" style=\"background: rgba(255, 255, 255, .6)\">\n        <h3>Use in the ipython notebook</h3>\n      </div>\n      <img\n        src=\"/projects/railroadtracks/ipython.png\"\n        alt=\"First slide\"\n        height=\"400px\"\n      />\n    </div>\n    <div class=\"item\">\n      <div class=\"carousel-caption\" style=\"background: rgba(255, 255, 255, .6)\">\n        <h3>Model for RNA-Seq</h3>\n      </div>\n      <img\n        src=\"/projects/railroadtracks/model.svg\"\n        alt=\"Second slide\"\n        height=\"400px\"\n      />\n    </div>\n  </div>\n  <!-- Controls -->\n  <a\n    class=\"left carousel-control\"\n    href=\"#carousel-example-generic\"\n    role=\"button\"\n    data-slide=\"prev\"\n  >\n    <span class=\"glyphicon glyphicon-chevron-left\"></span>\n  </a>\n  <a\n    class=\"right carousel-control\"\n    href=\"#carousel-example-generic\"\n    role=\"button\"\n    data-slide=\"next\"\n  >\n    <span class=\"glyphicon glyphicon-chevron-right\"></span>\n  </a>\n</div>\n"},{"slug":"yap","metadata":{"name":"Yet Another Pipeline","abbreviation":"YAP","projectName":"yap","launched":"2014-11-06T00:00:00.000Z","icon":"yap_logo.png","summary":"YAP allows researchers to quickly build high throughput big data pipelines without extensive\nknowledge of parallel programming.\n","description":"<p>YAP is an extensible parallel framework, written in Python using <a href=\"http://www.open-mpi.org/\">OpenMPI</a> libraries. It\nallows researchers to quickly build high throughput big data pipelines without extensive knowledge of parallel\nprogramming. The user interacts with the framework through simple configuration files to capture analysis parameters\nand user directed metadata, enabling reproducible research. Using YAP, analysts have been able to achieve a\nsignificant speed up of up to 36× in <a href=\"https://en.wikipedia.org/wiki/RNA-Seq\">RNASeq</a> workflow execution time.</p>\n<p>YAP has been designed to be scalable and flexible. We have implemented YAP with a focus on next-generation sequencing\n(NGS), to meet the large data processing challenges at NIBR. However, the framework can be easily adapted for any kind\nof analysis. It can be executed on your local Linux workstations or large HPC cluster systems. The framework achieves\nefficiency by implementing optimal data handling mechanisms such as, parallel data distribution, avoiding file I/O\nusing data streams and named pipes.</p>\n","dateString":"Thu Nov 06 2014","title":"Yet Another Pipeline","styles":[],"scripts":[]},"html":"\n<h2> YAP compared to analysts' scripts </h2>\n<table>\n  <thead>\n    <tr>\n      <th>Analysis</th>\n      <th>Data size</th>\n      <th>Number of cores</th>\n      <th>Analyst methods (hrs)</th>\n      <th>YAP (hrs)</th>\n      <th>Speed-up</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>RNASeq QC and Counts</th>\n      <td>3 billion reads (150 samples)</td>\n      <td>500</td>\n      <td>325.6</td>\n      <td>9</td>\n      <td>\n        36&times;\n      </td>\n    </tr>\n    <tr>\n      <th>Bacterial studies using Mothur</th>\n      <td>230,000 reads</td>\n      <td>72</td>\n      <td>90</td>\n      <td>12</td>\n      <td>8&times;</td>\n    </tr>\n    <tr>\n      <th>ChIPSeq Peak Calls</th>\n      <td>190 million reads (6 samples)</td>\n      <td>12</td>\n      <td>9.3</td>\n      <td>4.5</td>\n      <td>2&times;</td>\n    </tr>\n    <tr>\n      <th>EQP</th>\n      <td>400 million reads (5 samples)</td>\n      <td>60</td>\n      <td>45</td>\n      <td>12</td>\n      <td>4&times;</td>\n    </tr>\n  </tbody>\n</table>\n\n<div class=\"container\" style=\"text-align:center;\">\n  <div class=\"row\">\n    <div class=\"col-sm-3\"></div>\n    <div class=\"col-sm-6\">\n      <table class=\"table table-striped table-bordered\">\n        <thead>\n          <tr>\n            <th></th>\n            <th>Traditional method</th>\n            <th>YAP</th>\n          </tr>\n        </thead>\n        <tbody>\n          <tr>\n            <th>I/O steps</th>\n            <td>1400 file reads<br>1200 file writes</td>\n            <td>200 file reads<br>800 file writes</td>\n          </tr>\n          <tr>\n            <th>Jobs spawned</th>\n            <td>1500</td>\n            <td>1 MPI job</td>\n          </tr>\n          <tr>\n            <td colspan=3>\n              File-based reads reduced by <span class=\"label label-success\">70%</span>\n              <br>\n              File-based writes reduced by <span class=\"label label-success\">30%</span>\n            </td>\n          </tr>\n        </tbody>\n      </table>\n    </div>\n  </div>\n</div>\n\n<h2> Example Analysis Output </h2>\n<p>\n\n  The following images are the results of various applications run within the\n  YAP framework, such as FastQC, FastQScreen, PicardTools, etc.\n\n</p>\n<img src=\"/projects/yap/images/sample_output_images.png\" class=\"scale-image\">\n\n<p>\n\n  <strong>YAP consolidates results from across the samples for the various\n      packages, such as gene counts from HTSeq and normalized counts from Cufflinks.</strong>\n\n</p>\n\n<div class=\"row\">\n  <div class=\"col-md-6\">\n    <h5>HTSeq gene counts</h5>\n    <table class='table table-striped table-bordered'>\n        <thead>\n          <tr>\n            <th>\n              SAMPLE\n            </th>\n            <th>\n              CR560274_1\n            </th>\n            <th>\n              CR560457_1\n            </th>\n            <th>\n              CR560502_1\n            </th>\n            <th>\n              CR560562_1\n            </th>\n            <th>...</th>\n          </tr>\n        </thead>\n        <tbody>\n          <tr>\n            <td>\n              NM_000014\n            </td>\n            <td>\n              34\n            </td>\n            <td>\n              13\n            </td>\n            <td>\n              35\n            </td>\n            <td>\n              34\n            </td>\n            <td></td>\n          </tr>\n          <tr>\n            <td>\n              NM_000015\n            </td>\n            <td>\n              2\n            </td>\n            <td>\n              1\n            </td>\n            <td>\n              1\n            </td>\n            <td>\n              1\n            </td>\n            <td></td>\n          </tr>\n          <tr>\n            <td>\n              NM_000016\n            </td>\n            <td>\n              0\n            </td>\n            <td>\n              0\n            </td>\n            <td>\n              0\n            </td>\n            <td>\n              0\n            </td>\n            <td></td>\n          </tr>\n          <tr>\n            <td>\n              NM_000017\n            </td>\n            <td>\n              27\n            </td>\n            <td>\n              11\n            </td>\n            <td>\n              9\n            </td>\n            <td>\n              3\n            </td><td></td>\n          </tr>\n\n          <tr><td>NM_000018</td><td>0</td><td>0</td><td>0</td><td>0</td><td></td></tr>\n          <tr><td>NM_000019</td><td>18</td><td>48</td><td>17</td><td>14</td><td></td></tr>\n<tr><td>NM_000020</td><td>0</td><td>0</td><td>0</td><td>0</td><td></td></tr>\n<tr><td>NM_000021</td><td>0</td><td>0</td><td>0</td><td>0</td><td></td></tr>\n\n          <tr>\n            <td>&#x22ee;</td><td></td><td></td><td></td><td></td><td>&#x22f1;</td>\n          </tr>\n        </tbody>\n      </table>\n  </div>\n  <div class=\"col-md-6\">\n    <h5> Normalized counts from Cufflinks </h5>\n    <table class='table table-striped table-bordered'>\n      <tbody>\n          <tr style=\"font-weight: bold;\">\n            <td>\n              SAMPLE\n            </td>\n            <td colspan=2>\n              CR560274_1\n            </td>\n            <td colspan=2>\n              CR560457_1\n            </td>\n            <td colspan=2>\n              CR560502_1\n            </td>\n          <td>...</td></tr>\n          <tr style=\"font-weight: bold;\">\n            <td>\n              TRACKING_ID\n            </td>\n            <td>\n              FPKM\n            </td>\n            <td>\n              FPKM_Status\n            </td>\n            <td>\n              FPKM\n            </td>\n            <td>\n              FPKM_Status\n            </td>\n            <td>\n              FPKM\n            </td>\n            <td class=\"td10\">\n              FPKM_Status\n            </td>\n          <td></td></tr>\n          <tr>\n            <td>\n              NM_000014<br>|chr12:9220303-9268558|\n            </td>\n            <td>\n              0.187039\n            </td>\n            <td>\n              OK\n            </td>\n            <td>\n              0\n            </td>\n            <td>\n              OK\n            </td>\n            <td>\n              0.134608\n            </td>\n            <td class=\"td10\">\n              OK\n            </td>\n          <td></td></tr>\n          <tr>\n            <td>\n              NM_000015<br>|chr8:18248754-18258723|\n            </td>\n            <td>\n              0.218917\n            </td>\n            <td>\n              OK\n            </td>\n            <td>\n              0.152739\n            </td>\n            <td>\n              OK\n            </td>\n            <td>\n              0.13776\n            </td>\n            <td class=\"td10\">\n              OK\n            </td>\n          <td></td></tr>\n          <tr>\n            <td>\n              NM_000016<br>|chr1:76190042-76229355|\n            </td>\n            <td>\n              2.02618\n            </td>\n            <td>\n              OK\n            </td>\n            <td>\n              8.25528\n            </td>\n            <td>\n              OK\n            </td>\n            <td>\n              1.7346\n            </td>\n            <td class=\"td10\">\n              OK\n            </td>\n          <td></td></tr>\n          <tr><td>\n            NM_000017<br>|chr12:121163570-121177811|</td><td>1.40608</td><td>OK</td><td>0.980779</td><td>OK</td><td>0.708088</td><td>OK\n            </td><td></td></tr>\n              <tr>\n              <td>⋮</td><td></td><td></td><td></td><td></td><td></td><td></td><td>⋱</td>\n            </tr>\n            </tbody>\n          </table>\n\n  </div><!-- col-md-4 -->\n</div><!-- row -->\n\n<div class=\"container-fluid\"><div class=\"row\"><div class=\"col-xs-4\">\n  <h2> Reproducible research </h2>\n\n  Here's an example of the metadata automatically collected during a YAP run. By\n  storing the commands and parameters used to run the job, YAP allows\n  scientists to reproduce their analysis results at later points.\n\n</div>\n<div class=\"col-xs-8\" style=\"max-height: 600px; overflow-y: scroll;\">\n  <code><pre style=\"background-color: #111; color: #aaa;\">\n------------------------------ YAP ANALYSIS SUMMARY FOR WORKFLOW = yap2.3_test ------------------------------\n<span style=\"color:white;\">Operating System Information= Linux yourhostname 2.6.18-371.9.1.el5#1 SMP Tue May 13 06:52:49 EDT 2014x86_64\nUSER= my_user\nYAP SOURCE= /YAP/opensource/\nPython Source= 2.7.5 (default, Sep 10 2013, 17:21:36)  [GCC 4.1.2 20080704 (Red Hat 4.1.2-54)]\nAnalysis Start Time For Workflow : yap2.3_test 2014/09/04 15:55:44\nYAP analysis general metadata:\n1.comment:120 chars\n2.analyst_name:120 chars\n3.organisation_name:NIBR\nInstrument Type= Illumina\nSpecimen Information= [tissue type]\nWorkflow type= rnaseq\nNumber of input files= 2\nNumber of processors= 6\nInput files path for the workflow= /examples/sample_input\nInput file provided:\n1.RN0000108D_1 => /examples/sample_input/RN0000108D_1.fq\n\t\t  /examples/sample_input/RN0000108D_2.fq\n\nOutput file path for the workflow= /test_output/yap2.3_test\nSequence data type= paired end\nInput file format= fastq\nMaximum read length= 150\nFile chunk size (in megabytes)= 1024\nData distribution method=chunk_based\nOutput file path= /test_output/\n-------------------------------------------------------------------------------------------------------------\nAnalysis stages :\nPreprocess analysis= yes\nReference Sequence Alignment=yes\nPostprocess Analysis= yes\n-------------------------------------------------------------------------------------------------------------\nPreprocess Analysis commands:\nBarcodes information:\nno_barcode_specified :\n1. command name= fastq_screen,command line= /packages/FastQScreen/v0.4.1/fastq_screen --subset 500000 --paired --outdir output_directory --conf fastq_screen_v0.4.1.conf --aligner bowtie\n2. command name= fastqc,command line= /packages/fastqc/0.10.1/fastqc --outdir output_directory --extract --threads 12\n-------------------------------------------------------------------------------------------------------------\nAligner commands:\n1. command name= bowtie,command line= /packages/bowtie/1.0.0/bowtie  /accessory_files/indexes/bowtie/hg19 -q -v 2 -k 10 -m 10 --best -S -p 8 -1 pipe1 -2 pipe2  >output_file.sam\nAlignment output data sort order= both\n-------------------------------------------------------------------------------------------------------------\nSamples re-grouped in this workflow:\nNone.\n-------------------------------------------------------------------------------------------------------------\nPotprocess analysis commands:\n1. command type= :begin\n\tcommand input : ['input_file_type *junctions.bed*', 'input_directory aligner_output']\n\t1. command name= yap_junction_count,command line= yap_junction_count -exon_coordinates_file /accessory_files/human-ucsc-final_exon_coord.bed -exon_CoordToNumber_file /accessory_files/human-ucsc-final_exon_coord_number.bed -i - -o output_file\n2. command type= :begin\n\tcommand input : ['input_file_type *queryname*.sam', 'input_directory aligner_output']\n\t1. command name= htseq-count,command line= /packages/python/2.6.5_gnu/bin/htseq-count -s no -q  file_based_input  /accessory_files/human-ucsc-refGene.gtf  >output_file.out\n3. command type= :begin_tee\n\tcommand input : ['input_directory aligner_output', 'input_file_type *coordinate*']\n\t1. command name= yap_exon_count,command line= yap_exon_count -f 1.0 -exon_coordinates_file /accessory_files/human-ucsc-final_exon_coord.bed -exon_CoordToNumber_file /accessory_files/human-ucsc-final_exon_coord_number.bed -i - -o output_file\n\t2. command name= CollectAlignmentSummaryMetrics,command line= java -Xmx1g -jar /packages/picard-tools/1.89/CollectAlignmentSummaryMetrics.jar VALIDATION_STRINGENCY= SILENT I= /dev/stdin O= output_file.txt IS_BISULFITE_SEQUENCED= true ASSUME_SORTED= True REFERENCE_SEQUENCE= /db/nibrgenome/NG00006.0/fasta/hg19.fa QUIET= True\n\t3. command name= QualityScoreDistribution,command line= java -Xmx1g -jar /packages/picard-tools/1.89/QualityScoreDistribution.jar VALIDATION_STRINGENCY= SILENT I= /dev/stdin O= output_file.txt ASSUME_SORTED= true REFERENCE_SEQUENCE= /db/nibrgenome/NG00006.0/fasta/hg19.fa CHART= output_file.pdf ALIGNED_READS_ONLY= true\n\t4. command name= MeanQualityByCycle,command line= java -Xmx1g -jar /packages/picard-tools/1.89/MeanQualityByCycle.jar VALIDATION_STRINGENCY= SILENT ASSUME_SORTED= true REFERENCE_SEQUENCE= /db/nibrgenome/NG00006.0/fasta/hg19.fa I= /dev/stdin O= output_file.txt CHART= output_file.pdf ALIGNED_READS_ONLY= true\n\t5. command name= CollectGcBiasMetrics,command line= java -Xmx1g -jar /packages/picard-tools/1.89/CollectGcBiasMetrics.jar VALIDATION_STRINGENCY= SILENT I= /dev/stdin O= output_file.txt SUMMARY_OUTPUT= output_file_summary.txt CHART= output_file.pdf ASSUME_SORTED= true REFERENCE_SEQUENCE= /db/nibrgenome/NG00006.0/fasta/hg19.fa\n\t6. command name= CollectRnaSeqMetrics,command line= java -Xmx1g -jar  /packages/picard-tools/1.89/CollectRnaSeqMetrics.jar VALIDATION_STRINGENCY= SILENT ASSUME_SORTED= true REF_FLAT= /db/yap/ucsc/may_02_2013/gtf/hg19/human_refflat_for_picard.gff RIBOSOMAL_INTERVALS= /db/yap/ucsc/may_02_2013/gtf/hg19/Homo_sapiens_assembly19.rRNA.interval_list STRAND_SPECIFICITY= NONE I= /dev/stdin O= output_file.txt CHART_OUTPUT= output_file.pdf REFERENCE_SEQUENCE= /db/nibrgenome/NG00006.0/fasta/hg19.fa\n\t7. command name= CalculateHsMetrics,command line= /packages/picard-tools/1.89/CalculateHsMetrics.jar VALIDATION_STRINGENCY= SILENT BAIT_INTERVALS= /accessory_files/TruSeq_exome_targeted_regions_for_picard.bed TARGET_INTERVALS= /accessory_files/TruSeq_exome_targeted_regions_for_picard.bed INPUT= /dev/stdin OUTPUT= output_file.txt REFERENCE_SEQUENCE= /db/nibrgenome/NG00006.0/fasta/hg19.fa\n\t8. command name= CollectTargetedPcrMetrics,command line= java -Xmx1g -jar /packages/picard-tools/1.89/CollectTargetedPcrMetrics.jar VALIDATION_STRINGENCY= SILENT AMPLICON_INTERVALS= /accessory_files/TruSeq_exome_targeted_regions_for_picard.bed TARGET_INTERVALS= /accessory_files/TruSeq_exome_targeted_regions_for_picard.bed INPUT= /dev/stdin OUTPUT= output_file.txt REFERENCE_SEQUENCE= /db/nibrgenome/NG00006.0/fasta/hg19.fa\n4. command type= :begin\n\tcommand input : ['input_file_type *coordinate*', 'input_directory aligner_output']\n\t1. command name= CollectInsertSizeMetrics,command line= java -Xmx1g -jar  /packages/picard-tools/1.89/CollectInsertSizeMetrics.jar VALIDATION_STRINGENCY= SILENT ASSUME_SORTED= true I= file_based_input O= output_file.txt H= output_file.pdf TMP_DIR= /scratch/$USER HISTOGRAM_WIDTH= 500\n5. command type= :begin\n\tcommand input : ['input_file_type *coordinate*', 'input_directory aligner_output']\n\t1. command name= MarkDuplicates,command line= java -Xmx1g -jar  /packages/picard-tools/1.89/MarkDuplicates.jar VALIDATION_STRINGENCY= SILENT TMP_DIR= /scratch/$USER MAX_RECORDS_IN_RAM= 1000000 MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP= 2000 ASSUME_SORTED= true I= file_based_input O= output_file.bam METRICS_FILE= output_file.txt\n6. command type= :begin\n\tcommand input : ['input_file_type *coordinate*', 'input_directory aligner_output']\n\t1. command name= cufflinks,command line= /packages/cufflinks/2.1.1/cufflinks  file_based_input -o output_directory -p 12 -G /accessory_files/human-ucsc-refGene.gtf\n-------------------------------------------------------------------------------------------------------------\n\n******************* YAP CHECK SUMMARY *******************\n* --Syntax check          : Passed                      *\n* --Compatibility check   : Passed                      *\n* --File paths check      : Passed With Warnings        *\n*********************************************************\n* YAP Configuration overall check status: Passed With Warnings</span>\n\n--------------------------------------- YAP Check Error/Warning Info ---------------------------------------\n-------------------------------------------------------------------------------------------------------------\n--YAP Configuration File paths check status: Passed With Warnings\nWarning: At Line: 21 in file: bowtie_1.0.0_configuration.cfg. Files were found using basename in /db/nibrgenome/NG00006.0/indexes/bowtie/hg19. Please make sure that command: bowtie can work with basenames.\n\n\n-------------------------------------------------------------------------------------------------------------\n-------------------------- YAP configurations check end for Workflow = yap2.3_test --------------------------\n-------------------- PROVENANCE --------------------\n\nPREPROCESS:\n\n\t/packages/FastQScreen/v0.4.1/fastq_screen --subset 500000 --paired --outdir /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/preprocess_output --conf fastq_screen_v0.4.1.conf --aligner bowtie /db/yap/benchmark/robin_50_samples/RN0000108D_1.fq /db/yap/benchmark/robin_50_samples/RN0000108D_2.fq\n\n\t/packages/fastqc/0.10.1/fastqc --outdir /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/preprocess_output --extract --threads 12 /db/yap/benchmark/robin_50_samples/RN0000108D_1.fq /db/yap/benchmark/robin_50_samples/RN0000108D_2.fq\n\nALIGNMENT :\n\n\tINPUT: /db/yap/benchmark/robin_50_samples/RN0000108D_1.fq and /db/yap/benchmark/robin_50_samples/RN0000108D_2.fq chunk number= 0\n\n\t/packages/bowtie/1.0.0/bowtie  /db/nibrgenome/NG00006.0/indexes/bowtie/hg19 -q -v 2 -k 10 -m 10 --best -S -p 8 -1  /scratch/kulkatr1//kulkatr1/yap_temp/aligner_RN0000108D_1_0000003008281_0.797636572311_pipe_0_0_1  -2  /scratch/kulkatr1//kulkatr1/yap_temp/aligner_RN0000108D_1_0000003008281_0.797636572311_pipe_0_0_2   >/test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000000.sam\n\n\tsamtools view -bhS - samtools sort -on -m 100000000 - /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000000_queryname | samtools view -h - -o /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000000_queryname.sam\n\n\tsamtools view -bhS - samtools sort -o -m 100000000 - /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000000_coordinate | samtools view -h - -o /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000000_coordinate.sam\n\n\tINPUT: /db/yap/benchmark/robin_50_samples/RN0000108D_1.fq and /db/yap/benchmark/robin_50_samples/RN0000108D_2.fq chunk number= 1\n\n\t/packages/bowtie/1.0.0/bowtie  /db/nibrgenome/NG00006.0/indexes/bowtie/hg19 -q -v 2 -k 10 -m 10 --best -S -p 8 -1  /scratch/kulkatr1//kulkatr1/yap_temp/aligner_RN0000108D_1_0000013008281_0.797636572311_pipe_1_0_1  -2  /scratch/kulkatr1//kulkatr1/yap_temp/aligner_RN0000108D_1_0000013008281_0.797636572311_pipe_1_0_2   >/test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000001.sam\n\n\tsamtools view -bhS - samtools sort -on -m 100000000 - /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000001_queryname | samtools view -h - -o /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000001_queryname.sam\n\n\tsamtools view -bhS - samtools sort -o -m 100000000 - /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000001_coordinate | samtools view -h - -o /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000001_coordinate.sam\n\n\tINPUT: /db/yap/benchmark/robin_50_samples/RN0000108D_1.fq and /db/yap/benchmark/robin_50_samples/RN0000108D_2.fq chunk number= 2\n\n\t/packages/bowtie/1.0.0/bowtie  /db/nibrgenome/NG00006.0/indexes/bowtie/hg19 -q -v 2 -k 10 -m 10 --best -S -p 8 -1  /scratch/kulkatr1//kulkatr1/yap_temp/aligner_RN0000108D_1_0000023008281_0.797636572311_pipe_2_0_1  -2  /scratch/kulkatr1//kulkatr1/yap_temp/aligner_RN0000108D_1_0000023008281_0.797636572311_pipe_2_0_2   >/test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000002.sam\n\n\tsamtools view -bhS - samtools sort -on -m 100000000 - /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000002_queryname | samtools view -h - -o /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000002_queryname.sam\n\n\tsamtools view -bhS - samtools sort -o -m 100000000 - /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000002_coordinate | samtools view -h - -o /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000002_coordinate.sam\n\n\tINPUT: /db/yap/benchmark/robin_50_samples/RN0000108D_1.fq and /db/yap/benchmark/robin_50_samples/RN0000108D_2.fq chunk number= 3\n\n\t/packages/bowtie/1.0.0/bowtie  /db/nibrgenome/NG00006.0/indexes/bowtie/hg19 -q -v 2 -k 10 -m 10 --best -S -p 8 -1  /scratch/kulkatr1//kulkatr1/yap_temp/aligner_RN0000108D_1_0000033008281_0.797636572311_pipe_3_0_1  -2  /scratch/kulkatr1//kulkatr1/yap_temp/aligner_RN0000108D_1_0000033008281_0.797636572311_pipe_3_0_2   >/test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000003.sam\n\n\tsamtools view -bhS - samtools sort -on -m 100000000 - /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000003_queryname | samtools view -h - -o /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000003_queryname.sam\n\n\tsamtools view -bhS - samtools sort -o -m 100000000 - /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000003_coordinate | samtools view -h - -o /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000003_coordinate.sam\n\n\tINPUT: /db/yap/benchmark/robin_50_samples/RN0000108D_1.fq and /db/yap/benchmark/robin_50_samples/RN0000108D_2.fq chunk number= 4\n\n\t/packages/bowtie/1.0.0/bowtie  /db/nibrgenome/NG00006.0/indexes/bowtie/hg19 -q -v 2 -k 10 -m 10 --best -S -p 8 -1  /scratch/kulkatr1//kulkatr1/yap_temp/aligner_RN0000108D_1_0000043008281_0.797636572311_pipe_4_0_1  -2  /scratch/kulkatr1//kulkatr1/yap_temp/aligner_RN0000108D_1_0000043008281_0.797636572311_pipe_4_0_2   >/test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000004.sam\n\n\tsamtools view -bhS - samtools sort -on -m 100000000 - /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000004_queryname | samtools view -h - -o /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000004_queryname.sam\n\n\tsamtools view -bhS - samtools sort -o -m 100000000 - /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000004_coordinate | samtools view -h - -o /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000004_coordinate.sam\n\n\tINPUT: /db/yap/benchmark/robin_50_samples/RN0000108D_1.fq and /db/yap/benchmark/robin_50_samples/RN0000108D_2.fq chunk number= 5\n\n\t/packages/bowtie/1.0.0/bowtie  /db/nibrgenome/NG00006.0/indexes/bowtie/hg19 -q -v 2 -k 10 -m 10 --best -S -p 8 -1  /scratch/kulkatr1//kulkatr1/yap_temp/aligner_RN0000108D_1_0000053008281_0.797636572311_pipe_5_0_1  -2  /scratch/kulkatr1//kulkatr1/yap_temp/aligner_RN0000108D_1_0000053008281_0.797636572311_pipe_5_0_2   >/test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000005.sam\n\n\tsamtools view -bhS - samtools sort -on -m 100000000 - /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000005_queryname | samtools view -h - -o /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000005_queryname.sam\n\n\tsamtools view -bhS - samtools sort -o -m 100000000 - /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000005_coordinate | samtools view -h - -o /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000005_coordinate.sam\n\nMERGE ALIGNMENT OUTPUT :\n\n\tsamtools merge -n  -  /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000002_queryname.bam /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000003_queryname.bam /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000005_queryname.bam /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000000_queryname.bam /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000001_queryname.bam /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000004_queryname.bam  | samtools view -h - -o /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/RN0000108D_1_queryname.sam\n\n\tsamtools merge  -  /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000005_coordinate.bam /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000001_coordinate.bam /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000000_coordinate.bam /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000002_coordinate.bam /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000003_coordinate.bam /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/aligner_RN0000108D_1_000004_coordinate.bam  | samtools view -h - -o /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/RN0000108D_1_coordinate.sam\n\nPOSTPROCESS :\n\n\tINPUT: /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/RN0000108D_1_queryname.sam\n\n\t/packages/python/2.6.5_gnu/bin/htseq-count -s no -q  /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/RN0000108D_1_queryname.sam  /accessory_files/human-ucsc-refGene.gtf  >/test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_htseq-count.out\n\n\tINPUT: /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/RN0000108D_1_coordinate.sam\n\n\tyap_exon_count -f 1.0 -exon_coordinates_file /accessory_files/human-ucsc-final_exon_coord.bed -exon_CoordToNumber_file /accessory_files/human-ucsc-final_exon_coord_number.bed -i - -o /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_yap_exon_count\n\n\tjava -Xmx1g -jar /packages/picard-tools/1.89/CollectAlignmentSummaryMetrics.jar VALIDATION_STRINGENCY= SILENT I= /dev/stdin O= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_CollectAlignmentSummaryMetrics.txt IS_BISULFITE_SEQUENCED= true ASSUME_SORTED= True REFERENCE_SEQUENCE= /db/nibrgenome/NG00006.0/fasta/hg19.fa QUIET= True\n\n\tjava -Xmx1g -jar /packages/picard-tools/1.89/QualityScoreDistribution.jar VALIDATION_STRINGENCY= SILENT I= /dev/stdin O= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_QualityScoreDistribution.txt ASSUME_SORTED= true REFERENCE_SEQUENCE= /db/nibrgenome/NG00006.0/fasta/hg19.fa CHART= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_QualityScoreDistribution.pdf ALIGNED_READS_ONLY= true\n\n\tjava -Xmx1g -jar /packages/picard-tools/1.89/MeanQualityByCycle.jar VALIDATION_STRINGENCY= SILENT ASSUME_SORTED= true REFERENCE_SEQUENCE= /db/nibrgenome/NG00006.0/fasta/hg19.fa I= /dev/stdin O= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_MeanQualityByCycle.txt CHART= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_MeanQualityByCycle.pdf ALIGNED_READS_ONLY= true\n\n\tjava -Xmx1g -jar /packages/picard-tools/1.89/CollectGcBiasMetrics.jar VALIDATION_STRINGENCY= SILENT I= /dev/stdin O= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_CollectGcBiasMetrics.txt SUMMARY_OUTPUT= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_CollectGcBiasMetrics_summary.txt CHART= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_CollectGcBiasMetrics.pdf ASSUME_SORTED= true REFERENCE_SEQUENCE= /db/nibrgenome/NG00006.0/fasta/hg19.fa\n\n\tjava -Xmx1g -jar  /packages/picard-tools/1.89/CollectRnaSeqMetrics.jar VALIDATION_STRINGENCY= SILENT ASSUME_SORTED= true REF_FLAT= /db/yap/ucsc/may_02_2013/gtf/hg19/human_refflat_for_picard.gff RIBOSOMAL_INTERVALS= /db/yap/ucsc/may_02_2013/gtf/hg19/Homo_sapiens_assembly19.rRNA.interval_list STRAND_SPECIFICITY= NONE I= /dev/stdin O= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_CollectRnaSeqMetrics.txt CHART_OUTPUT= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_CollectRnaSeqMetrics.pdf REFERENCE_SEQUENCE= /db/nibrgenome/NG00006.0/fasta/hg19.fa\n\n\t/packages/picard-tools/1.89/CalculateHsMetrics.jar VALIDATION_STRINGENCY= SILENT BAIT_INTERVALS= /accessory_files/TruSeq_exome_targeted_regions_for_picard.bed TARGET_INTERVALS= /accessory_files/TruSeq_exome_targeted_regions_for_picard.bed INPUT= /dev/stdin OUTPUT= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_CalculateHsMetrics.txt REFERENCE_SEQUENCE= /db/nibrgenome/NG00006.0/fasta/hg19.fa\n\n\tjava -Xmx1g -jar /packages/picard-tools/1.89/CollectTargetedPcrMetrics.jar VALIDATION_STRINGENCY= SILENT AMPLICON_INTERVALS= /accessory_files/TruSeq_exome_targeted_regions_for_picard.bed TARGET_INTERVALS= /accessory_files/TruSeq_exome_targeted_regions_for_picard.bed INPUT= /dev/stdin OUTPUT= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_CollectTargetedPcrMetrics.txt REFERENCE_SEQUENCE= /db/nibrgenome/NG00006.0/fasta/hg19.fa\n\n\tINPUT: /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/RN0000108D_1_coordinate.sam\n\n\tjava -Xmx1g -jar  /packages/picard-tools/1.89/CollectInsertSizeMetrics.jar VALIDATION_STRINGENCY= SILENT ASSUME_SORTED= true I= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/RN0000108D_1_coordinate.sam O= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_CollectInsertSizeMetrics.txt H= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_CollectInsertSizeMetrics.pdf TMP_DIR= /scratch/$USER HISTOGRAM_WIDTH= 500\n\n\tINPUT: /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/RN0000108D_1_coordinate.sam\n\n\tjava -Xmx1g -jar  /packages/picard-tools/1.89/MarkDuplicates.jar VALIDATION_STRINGENCY= SILENT TMP_DIR= /scratch/$USER MAX_RECORDS_IN_RAM= 1000000 MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP= 2000 ASSUME_SORTED= true I= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/RN0000108D_1_coordinate.sam O= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_MarkDuplicates.bam METRICS_FILE= /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output/RN0000108D_1_MarkDuplicates.txt\n\n\tINPUT: /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/RN0000108D_1_coordinate.sam\n\n\t/packages/cufflinks/2.1.1/cufflinks  /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/aligner_output/RN0000108D_1_coordinate.sam -o /test_output/yap2.3_test/RN0000108D_1/no_barcode_specified/postprocess_output -p 12 -G /accessory_files/human-ucsc-refGene.gtf\n\n--------------------Analysis End Time For Workflow : yap2.3_test 2014/09/04 19:43:14--------------------</pre>\n      </code>\n    </div>\n  </div>\n</div>\n\n<h2>\n<a name=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"><span class=\"octicon octicon-link\"></span></a>Requirements</h2>\n\n<p><strong>YAP only runs on Linux systems!</strong></p>\n\n<h3>Dependencies: </h3>\n\n<p>The following dependencies have to be first installed in your environment. Once installed, make sure these dependencies are added to your path.</p>\n\n<ul class=\"task-list\">\n<li>Recent versions of gcc (gcc 4.8.x is well tested)</li>\n<li>Python 2.7.7</li>\n<li>Openmpi 1.6.5</li>\n<li>Python modules:\n\n<ul class=\"task-list\">\n<li>MPI4py - 1.3</li>\n<li>PyPdf - 1.13</li>\n<li>Numpy - 1.7.1</li>\n<li>netsa-utils - 1.4.3</li>\n</ul>\n</li>\n<li>bedtools - 2.15.0</li>\n<li>samtools - 0.1.18</li>\n</ul>\n\n<h3>System Configuration:</h3>\n\n<p>   YAP provides a framework to run external tools and data, so the tools used\nin the workflows drive the system requirements. It can be installed on multicore\nlinux workstation with a decent amount of memory for small data, or on large\ncluster systems to scale optimally for large data processing. The framework has\nbeen tested extensively for <abbr title=\"Next-Generation Sequencing\">NGS</abbr>\ndata on clusters with minimum system configuration of 8-12 cores and 24-48 GB\nmemory.   </p>\n\n<h2>YAP Setup</h2>\n\n<ul class=\"task-list\">\n<li>Download the yap source from <a href=\"https://github.com/Novartis/yap/archive/master.zip\">here</a>\n</li>\n<li>\n<p>Uncompress the source directory </p>\n\n<p>for example: uncompress the directory as <code>/home/packages/YAP</code></p>\n</li>\n<li>\n<p>Set <code>YAP_HOME</code> environment variable to the source directory.</p>\n\n<p><pre><code>$ export YAP_HOME=/home/packages/YAP</code></pre></p>\n</li>\n<li>\n<p>Add bin directory to path</p>\n\n<pre><code>$ export PATH=$PATH:$YAP_HOME/bin/</code></pre>\n</li>\n<li>\n<p>Set <code>YAP_LOCAL_TEMPDIR</code>\nenvironment variable for temporary computation. For optimum performance point\nthis directory to a location which is  local to the machine. </p>\n\n<pre><code>$ export YAP_LOCAL_TEMPDIR=/scratch/username/yap_temp</code></pre>\n</li>\n</ul><p><strong>Verification</strong></p>\n\n<pre><code>$ echo $YAP_HOME\n    /home/packages/YAP\n\n$ echo $YAP_LOCAL_TEMPDIR\n    /scratch/username/yap_temp\n\n$ which yap\n    /home/packages/YAP/bin/yap\n</code></pre>\n\n<h1> <a name=\"user-content-running-a-yap-job\" class=\"anchor\"\nhref=\"#running-a-yap-job\" aria-hidden=\"true\"><span class=\"octicon\nocticon-link\"></span></a>Running a YAP job</h1>\n\n<p>Once you've set your environment, it is best to run a quick demo job to get\nthe feel of running YAP. The following section is meant to be interactive and\nhence you would need Linux account access and access to the cluster.</p>\n\n<p>After downloading the project, please see the demo configuration files in\n<code>yap/cfg</code>.</p>\n\n<p>There are 3 stages in YAP - Preprocess, Alignment and Postprocess. You can\nhave command level control of these three stages in the namesake configuration\nfiles and a workflow level control in the\n<code>workflow_configuration.</code></p>\n<table class=\"table table-striped\">\n<thead><tr>\n<th>Configuration</th>\n<th>Purpose</th>\n</tr></thead>\n<tbody>\n<tr>\n<td>aligner_configuration</td>\n<td>bwa, bowtie, bowtie2, tophat or insert your own aligner</td>\n</tr>\n<tr>\n<td>postprocess_configuration</td>\n<td>postalignment packages, generate counts or metrics</td>\n</tr>\n<tr>\n<td>preprocess_configuration</td>\n<td>pre-alignment packages to massage your seqdata</td>\n</tr>\n<tr>\n<td>workflow_configuration</td>\n<td>manage metadata, specify input files, paths and output directories</td>\n</tr>\n<tr>\n<td>yap_sge</td>\n<td>submitting your job to the cluster</td>\n</tr>\n</tbody>\n</table><p>The demo runs a RNASeq QC and counts workflow consisting of:</p>\n\n<ul class=\"task-list\">\n<li>Preprocess: FastQC, Fastqscreen</li>\n<li>Alignment: Bowtie, both queryname and coordinate sorted</li>\n<li>Postprocess: yap junction and exon counts, Picard tools (PostQC), HTSeq (Raw counts) and Cufflinks (normalized counts)</li>\n</ul><p>We run this workflow on 2 nodes on the UGE cluster.</p>\n\n<p>To run the yap_demo job, we next need to check to see if our configuration files are correct using the command.</p>\n\n<pre><code>cd &lt;your_working directory&gt;\nyap --check workflow_configuration.cfg\n</code></pre>\n\n<p>The <code>yap --check</code>  command checks to see</p>\n\n<ul class=\"task-list\">\n<li>If all paths specified are valid</li>\n<li>If YAP finds the appropriate input files</li>\n<li>Checks for syntax errors</li>\n<li>Lists commands to be executed.</li>\n<li>Gives section-wise error/warning report.</li>\n</ul>\n\n<h2>Running the YAP job</h2>\n\n<pre><code>mpirun -n &lt;number_of_cores&gt; yap workflow_configuration.cfg\n</code></pre>\n\n<p>If you have a SGE environment, pass the number of slots into the <code>$NSLOTS</code> variable.</p>\n<script>\n$('abbr').tooltip();\n</script>\n"},{"slug":"gridvar","metadata":{"name":"GridVar","abbreviation":"GridVar","projectName":"gridvar","launched":"2014-02-21T00:00:00.000Z","icon":"/projects/gridvar/gridvar-gallery-01.png","summary":"GridVar is a jQuery plugin that visualizes multi-dimensional datasets as layers organized in a row-column format.\n","description":"<p>GridVar is a jQuery plugin that visualizes multi-dimensional datasets as layers organized in a row-column format. At each cell (i.e., rectangle at the intersection of a row and column), GridVar displays your data as a background color (like a color/heat map) and/or a glyph (shape). This enables different characteristics of your dataset to be layered on top of each other. For more information on usage, required libraries, and other developer information, please see <a href=\"https://github.com/Novartis/gridvar\">our documentation on GitHub</a>.</p>\n","scripts":["https://code.jquery.com/jquery-1.10.1.min.js","https://cdn.jsdelivr.net/qtip2/2.2.0/jquery.qtip.min.js","https://code.jquery.com/ui/1.10.3/jquery-ui.js","https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.5.2/underscore-min.js","https://cdnjs.cloudflare.com/ajax/libs/d3/3.3.11/d3.min.js","/projects/gridvar/jquery.nibrGridVar.min.js","/projects/gridvar/gridvar-example.js"],"styles":["https://cdn.jsdelivr.net/qtip2/2.2.0/jquery.qtip.min.css","/projects/gridvar/gridvar.css"],"onMount":"$(document).ready(function() {\n  onMainReady();\n});\n","onDestroy":"$('.qtip').remove();\n","dateString":"Fri Feb 21 2014","title":"GridVar"},"html":"\n<h3><a id=\"genomicsExample\"></a>Genomics Example</h3>\n\n<p>\n  Our inspiration for developing GridVar was a figure from\n  <a\n    target=\"_blank\"\n    href=\"http://www.nature.com/nature/journal/v486/n7403/full/nature11154.html\"\n    >Sequence analysis of mutations and translocations across breast cancer\n    subtypes</a\n  >\n  published in <i>Nature</i>. Users can interact with the visualization by\n  hovering over a cell to view a popup with more details, clicking on labels,\n  and by reordering the rows and columns by selecting actions in the dropdowns.\n</p>\n\n<div id=\"tissueSampleOrder\" class=\"btn-group\">\n  <label>Order Tissue Samples (Columns) By...</label>\n  <ul class=\"dropdown-menu\" role=\"menu\">\n    <li>\n      <a role=\"button\" class=\"nibr-pointer\" id=\"tissueTopSort\">\n        Top gene set mutation frequency\n      </a>\n    </li>\n    <li>\n      <a role=\"button\" class=\"nibr-pointer\" id=\"tissueGlobalSort\">\n        Global gene set mutation frequency\n      </a>\n    </li>\n  </ul>\n</div>\n\n<div class=\"btn-group\">\n  <label>Order Genes (Rows) By...</label>\n  <ul class=\"dropdown-menu\" role=\"menu\">\n    <li>\n      <a role=\"button\" class=\"nibr-pointer\" id=\"geneMissenseMutationSort\">\n        Missense mutations only\n      </a>\n    </li>\n    <li>\n      <a role=\"button\" class=\"nibr-pointer\" id=\"geneNonSynonymousMutationSort\">\n        Other Non-synonymous mutations only\n      </a>\n    </li>\n    <li>\n      <a role=\"button\" class=\"nibr-pointer\" id=\"geneAllMutationSort\">\n        All mutations\n      </a>\n    </li>\n  </ul>\n</div>\n\n<div id=\"gridVarGenomics\"></div>\n\n<p>\n  We've taken data (<a\n    target=\"_blank\"\n    href=\"http://www.nature.com/nature/journal/v486/n7403/extref/nature11154-s2.xls\"\n    >Supplementary Table 4</a\n  >\n  from the aforementioned Nature paper) to create the above example. Tissue\n  samples (columns) are initially ordered by total mutations per sample in\n  descending order across the entire set of genes. Similarly, genes (rows) are\n  ordered by the number of mutations across samples in descending order.\n</p>\n\n<p>\n  Alternate orderings for both tissue samples and genes enable visual\n  exploration of the view. By restricting the tissue samples mutation counts to\n  just the top 6 mutated genes, tissue samples can be reordered. This enables\n  our users to see the impact of mutations across the global set or restricted\n  to a subset. When selecting this option, users will see the samples with\n  mutations shift to the left.\n</p>\n\n<p>\n  To understand gain and/or loss of function, our users can adjust the gene\n  mutation ordering by switching the gene order based on the frequency of\n  mutation types: missense, other non-synonymous, or both. The gene rows will\n  reorder and the histogram will update based on this selection.\n</p>\n\n<h3>Example code for initializing GridVar</h3>\n\n<pre><code class=\"javascript\">$('#gridVarGenomics').gridVar({\n    cellHeight: 30,\n    cellWidth: 9,\n    rowOrder: ['TP53', 'PIK3CA', 'AKT1',\n        'CBFB', 'GATA3', 'MAP3K1'],\n        columnOrder: ['BR-M-191','BR-M-037',\n        'BR-V-043','BR-V-027','BR-V-067',\n        ...],\n    dataMapping: {\n        data: [\n            ['AKT1', 'BR-V-016', ['Missense']],\n            ['AKT1', 'BR-V-017', ['Missense']],\n            ['TP53', 'BR-M-123', ['other non-synonymous']],\n            ...],\n        dataIndex: {\n            rowKey: 0,\n            columnKey: 1,\n            mutation: 2\n        }\n    },\n    dataDisplayMapping: [{\n        dataType: 'mutation',\n        mappings: {\n            Missense: '#bb88bb',\n            'other non-synonymous': '#777777'\n        }\n    }],\n    histogramMapping: {\n        data: histogramData,\n        label: 'Mut. Frequency'\n    },\n    rowLabelClicked: function(event, data) {\n        alert(data + ' clicked!');\n    }\n});</code></pre>\n\n<h3>Example code for reordering rows</h3>\n<pre><code class=\"javascript\">$('#gridVarGenomics').gridVar('option', 'rowOrder', ['TP53', 'PIK3CA', 'AKT1', 'CBFB', 'GATA3', 'MAP3K1']);</code></pre>\n\n<h3>Example code for reordering columns</h3>\n<pre><code class=\"javascript\">$('#gridVarGenomics').gridVar('option', 'columnOrder', ['BR-M-191','BR-M-037','BR-V-043','BR-V-027','BR-V-067']);</code></pre>\n\n<h2><a id=\"weatherExample\"></a>Weather Example</h2>\n\n<p>\n  Although GridVar was developed for a genomics use case, GridVar can visualize\n  all types of categorical data. In the example below, GridVar displays climate\n  data from the\n  <a\n    target=\"_blank\"\n    href=\"http://www1.ncdc.noaa.gov/pub/data/ccd-data/CCD-2012.pdf\"\n    >Comparative Climate Data for the U.S. through 2012</a\n  >\n  from the National Climate Data Center. Each row is one city per state,\n  including Washington D.C., Puerto Rico, and Pacific Islands (i.e. Guam), each\n  column is a month, and at the intersection the example displays the highest\n  recorded temperature per month and the days in those months that were or\n  weren't cloudy.\n</p>\n\n<p>Temperature - Highest of Record: Degrees F</p>\n\n<div id=\"gridVarWeather\"></div>\n\n<h3>Example code for initializing GridVar with climate data</h3>\n\n<pre><code class=\"javascript\">$('#gridVarWeather').gridVar({\n    rowOrder: ['BIRMINGHAM AP, AL', 'ANCHORAGE, AK',\n        'FLAGSTAFF, AZ', 'FORT SMITH, AR',\n        'BAKERSFIELD, CA', ...\n    ],\n    columnOrder: ['January', 'February', 'March', ...],\n    dataMapping: data: [\n            ['BIRMINGHAM AP, AL', 'January', ['Neutral'],\n            [81],\n            ['cloudy'],\n            [7, 6, 18]\n        ],\n            ['BIRMINGHAM AP, AL', 'February', ['Neutral'],\n            [83],\n            ['cloudy'],\n            [7, 6, 15]\n        ],\n            ['BIRMINGHAM AP, AL', 'March', ['Hot'],\n            [89],\n            ['cloudy'],\n            [7, 8, 16]\n        ],\n        ...\n    ],\n    dataIndex: {\n        rowKey: 0,\n        columnKey: 1,\n        temperature: 2,\n        cloudy: 4\n    }\n    ],\n    dataDisplayMapping: [{\n      dataType: 'temperature',\n      mappings: {\n          Hot: '#E44C16',\n           Neutral: '#E3E3E3',\n            Cool: '#01A2BF'\n        },\n        labelMapping: {\n           Hot: 'Hot (greater than 88)',\n           Neutral: 'Neutral',\n           Cool: 'Cool (less than 55)'\n        }\n    }, {\n       dataType: 'cloudy',\n           mappings: {\n           clear: 'circleRenderer',\n           cloudy: 'minusRenderer'\n        }\n    }],\n    rowLabelClicked: function (event, data) {\n         alert(data + ' clicked!');\n    }\n});</code></pre>\n"}]